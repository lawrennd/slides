<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2020-10-22">
  <title>Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="http://inverseprobability.com/talks/assets/js/figure-animate.js"></script>
</head>
<body>
\[\newcommand{\tk}[1]{}
%\newcommand{\tk}[1]{\textbf{TK}: #1}
\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Gaussian Processes</h1>
  <p class="date" style="text-align:center"><time>2020-10-22</time></p>
  <p class="venue" style="text-align:center">Virtual Data Science Nigeria</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gaussian-processes-for-machine-learning-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gp/rasmussen-williams-book.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A key reference for Gaussian process models remains the excellent book “Gaussian Processes for Machine Learning” (<span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span>). The book is also <a href="http://www.gaussianprocess.org/gpml/" target="_blank" >freely available online</a>.
</aside>
<p><span style="text-align:right"><span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span></span></p>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level2">
<h2>What is Machine Learning?</h2>
</section>
<section id="what-is-machine-learning-1" class="slide level2">
<h2>What is Machine Learning?</h2>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-2" class="slide level2">
<h2>What is Machine Learning?</h2>
<p><span class="math display">\[\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction (\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span class="math inline">\(\errorFunction (\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</section>
<section id="artificial-intelligence" class="slide level2">
<h2>Artificial Intelligence</h2>
<ul>
<li>Machine learning is a mainstay because of importance of prediction.</li>
</ul>
</section>
<section id="uncertainty" class="slide level2">
<h2>Uncertainty</h2>
<ul>
<li>Uncertainty in prediction arises from:</li>
<li>scarcity of training data and</li>
<li>mismatch between the set of prediction functions we choose and all possible prediction functions.</li>
<li>Also uncertainties in objective, leave those for another day.</li>
</ul>
</section>
<section id="neural-networks-and-prediction-functions" class="slide level2">
<h2>Neural Networks and Prediction Functions</h2>
<ul>
<li>adaptive non-linear function models inspired by simple neuron models <span class="citation" data-cites="McCulloch:neuron43">(McCulloch and Pitts, 1943)</span></li>
<li>have become popular because of their ability to model data.</li>
<li>can be composed to form highly complex functions</li>
<li>start by focussing on one hidden layer</li>
</ul>
</section>
<section id="prediction-function-of-one-hidden-layer" class="slide level2">
<h2>Prediction Function of One Hidden Layer</h2>
<p><span class="math display">\[
\mappingFunction(\inputVector) = \left.\mappingVector^{(2)}\right.^\top \activationVector(\mappingMatrix_{1}, \inputVector)
\]</span></p>
<p><span class="math inline">\(\mappingFunction(\cdot)\)</span> is a scalar function with vector inputs,</p>
<p><span class="math inline">\(\activationVector(\cdot)\)</span> is a vector function with vector inputs.</p>
<ul>
<li><p>dimensionality of the vector function is known as the number of hidden units, or the number of neurons.</p></li>
<li><p>elements of <span class="math inline">\(\activationVector(\cdot)\)</span> are the <em>activation</em> function of the neural network</p></li>
<li><p>elements of <span class="math inline">\(\mappingMatrix_{1}\)</span> are the parameters of the activation functions.</p></li>
</ul>
</section>
<section id="relations-with-classical-statistics" class="slide level2">
<h2>Relations with Classical Statistics</h2>
<ul>
<li><p>In statistics activation functions are known as <em>basis functions</em>.</p></li>
<li><p>would think of this as a <em>linear model</em>: not linear predictions, linear in the parameters</p></li>
<li><p><span class="math inline">\(\mappingVector_{1}\)</span> are <em>static</em> parameters.</p></li>
</ul>
</section>
<section id="adaptive-basis-functions" class="slide level2">
<h2>Adaptive Basis Functions</h2>
<ul>
<li>In machine learning we optimize <span class="math inline">\(\mappingMatrix_{1}\)</span> as well as <span class="math inline">\(\mappingMatrix_{2}\)</span> (which would normally be denoted in statistics by <span class="math inline">\(\boldsymbol{\beta}\)</span>).</li>
</ul>
</section>
<section id="integrated-basis-functions" class="slide level2">
<h2>Integrated Basis Functions</h2>
<ul>
<li><p>Revisit that decision: follow the path of <span class="citation" data-cites="Neal:bayesian94">Neal (1994)</span> and <span class="citation" data-cites="MacKay:bayesian92">MacKay (1992)</span>.</p></li>
<li><p>Consider the probabilistic approach.</p></li>
</ul>
</section>
<section id="probabilistic-modelling" class="slide level2">
<h2>Probabilistic Modelling</h2>
<ul>
<li>Probabilistically we want, <span class="math display">\[
p(\dataScalar_*|\dataVector, \inputMatrix, \inputVector_*),
\]</span> <span class="math inline">\(\dataScalar_*\)</span> is a test output <span class="math inline">\(\inputVector_*\)</span> is a test input <span class="math inline">\(\inputMatrix\)</span> is a training input matrix <span class="math inline">\(\dataVector\)</span> is training outputs</li>
</ul>
</section>
<section id="joint-model-of-world" class="slide level2">
<h2>Joint Model of World</h2>
<p><span class="math display">\[
p(\dataScalar_*|\dataVector, \inputMatrix, \inputVector_*) = \int p(\dataScalar_*|\inputVector_*, \mappingMatrix) p(\mappingMatrix | \dataVector, \inputMatrix) \text{d} \mappingMatrix
\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\mappingMatrix\)</span> contains <span class="math inline">\(\mappingMatrix_1\)</span> and <span class="math inline">\(\mappingMatrix_2\)</span></p>
<p><span class="math inline">\(p(\mappingMatrix | \dataVector, \inputMatrix)\)</span> is posterior density</p>
</div>
</section>
<section id="likelihood" class="slide level2">
<h2>Likelihood</h2>
<p><span class="math inline">\(p(\dataScalar|\inputVector, \mappingMatrix)\)</span> is the <em>likelihood</em> of data point</p>
<div class="fragment">
<p>Normally assume independence: <span class="math display">\[
p(\dataVector|\inputMatrix, \mappingMatrix) = \prod_{i=1}^\numData p(\dataScalar_i|\inputVector_i, \mappingMatrix),\]</span></p>
</div>
</section>
<section id="likelihood-and-prediction-function" class="slide level2">
<h2>Likelihood and Prediction Function</h2>
<p><span class="math display">\[
p(\dataScalar_i | \mappingFunction(\inputVector_i)) = \frac{1}{\sqrt{2\pi \dataStd^2}} \exp\left(-\frac{\left(\dataScalar_i - \mappingFunction(\inputVector_i)\right)^2}{2\dataStd^2}\right)
\]</span></p>
</section>
<section id="unsupervised-learning" class="slide level2">
<h2>Unsupervised Learning</h2>
<ul>
<li><p>Can also consider priors over latents <span class="math display">\[
p(\dataVector_*|\dataVector) = \int p(\dataVector_*|\inputMatrix_*, \mappingMatrix) p(\mappingMatrix | \dataVector, \inputMatrix) p(\inputMatrix) p(\inputMatrix_*) \text{d} \mappingMatrix \text{d} \inputMatrix \text{d}\inputMatrix_*
\]</span></p></li>
<li><p>This gives <em>unsupervised learning</em>.</p></li>
</ul>
</section>
<section id="probabilistic-inference" class="slide level2">
<h2>Probabilistic Inference</h2>
<ul>
<li><p>Data: <span class="math inline">\(\dataVector\)</span></p></li>
<li><p>Model: <span class="math inline">\(p(\dataVector, \dataVector^*)\)</span></p></li>
<li><p>Prediction: <span class="math inline">\(p(\dataVector^*| \dataVector)\)</span></p></li>
</ul>
</section>
<section id="graphical-models" class="slide level2">
<h2>Graphical Models</h2>
<ul>
<li>Represent joint distribution through <em>conditional dependencies</em>.</li>
<li>E.g. Markov chain</li>
</ul>
<p><span class="math display">\[p(\dataVector) = p(\dataScalar_\numData | \dataScalar_{\numData-1}) p(\dataScalar_{\numData-1}|\dataScalar_{\numData-2}) \dots p(\dataScalar_{2} | \dataScalar_{1})\]</span></p>
<div class="figure">
<div id="markov-chain-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/markov.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Markov chain is a simple form of probabilistic graphical model providing a particular decomposition of the joint density.
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p>Predict Perioperative Risk of Clostridium Difficile Infection Following Colon Surgery <span class="citation" data-cites="Steele:predictive12">(Steele et al., 2012)</span></p>
<div class="figure">
<div id="c-difficile-bayes-net-diagnosis-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/bayes-net-diagnosis.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A probabilistic directed graph used to predict the perioperative risk of <em>C Difficile</em> infection following colon surgery. When these models have good predictive performance they are often difficult to interpret. This may be due to the limited representation capability of the conditional densities in the model.
</aside>
</section>
<section id="performing-inference" class="slide level2">
<h2>Performing Inference</h2>
<ul>
<li><p>Easy to write in probabilities</p></li>
<li><p>But underlying this is a wealth of computational challenges.</p></li>
<li><p>High dimensional integrals typically require approximation.</p></li>
</ul>
</section>
<section id="linear-models" class="slide level2">
<h2>Linear Models</h2>
<ul>
<li><p>In statistics, focussed more on <em>linear</em> model implied by <span class="math display">\[
\mappingFunction(\inputVector) = \left.\mappingVector^{(2)}\right.^\top \activationVector(\mappingMatrix_1, \inputVector)
\]</span></p></li>
<li><p>Hold <span class="math inline">\(\mappingMatrix_1\)</span> fixed for given analysis.</p></li>
<li><p>Gaussian prior for <span class="math inline">\(\mappingMatrix\)</span>, <span class="math display">\[
\mappingVector^{(2)} \sim \gaussianSamp{\zerosVector}{\covarianceMatrix}.
\]</span> <span class="math display">\[
\dataScalar_i = \mappingFunction(\inputVector_i) + \noiseScalar_i,
\]</span> where <span class="math display">\[
\noiseScalar_i \sim \gaussianSamp{0}{\dataStd^2}
\]</span></p></li>
</ul>
</section>
<section id="linear-gaussian-models" class="slide level2">
<h2>Linear Gaussian Models</h2>
<ul>
<li>Normally integrals are complex but for this Gaussian linear case they are trivial.</li>
</ul>
</section>
<section id="multivariate-gaussian-properties" class="slide level2">
<h2>Multivariate Gaussian Properties</h2>
</section>
<section id="recall-univariate-gaussian-properties" class="slide level2">
<h2>Recall Univariate Gaussian Properties</h2>
<div class="fragment">
<ol type="1">
<li>Sum of Gaussian variables is also Gaussian.</li>
</ol>
<p><span class="math display">\[\dataScalar_i \sim \gaussianSamp{\meanScalar_i}{\dataStd_i^2}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\sum_{i=1}^{\numData} \dataScalar_i \sim \gaussianSamp{\sum_{i=1}^\numData \meanScalar_i}{\sum_{i=1}^\numData\dataStd_i^2}\]</span></p>
</div>
</section>
<section id="recall-univariate-gaussian-properties-1" class="slide level2">
<h2>Recall Univariate Gaussian Properties</h2>
<ol start="2" type="1">
<li>Scaling a Gaussian leads to a Gaussian.</li>
</ol>
<div class="fragment">
<p><span class="math display">\[\dataScalar \sim \gaussianSamp{\meanScalar}{\dataStd^2}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\mappingScalar\dataScalar\sim \gaussianSamp{\mappingScalar\meanScalar}{\mappingScalar^2 \dataStd^2}\]</span></p>
</div>
</section>
<section id="multivariate-consequence" class="slide level2">
<h2>Multivariate Consequence</h2>
<p><span style="text-align:left">If</span> <span class="math display">\[\inputVector \sim \gaussianSamp{\meanVector}{\covarianceMatrix}\]</span></p>
<div class="fragment">
<p><span style="text-align:left">And</span> <span class="math display">\[\dataVector= \mappingMatrix\inputVector\]</span></p>
</div>
<div class="fragment">
<p><span style="text-align:left">Then</span> <span class="math display">\[\dataVector \sim \gaussianSamp{\mappingMatrix\meanVector}{\mappingMatrix\covarianceMatrix\mappingMatrix^\top}\]</span></p>
</div>
</section>
<section id="linear-gaussian-models-1" class="slide level2">
<h2>Linear Gaussian Models</h2>
<ol type="1">
<li>linear Gaussian models are easier to deal with</li>
<li>Even the parameters <em>within</em> the process can be handled, by considering a particular limit.</li>
</ol>
</section>
<section id="multivariate-gaussian-properties-1" class="slide level2">
<h2>Multivariate Gaussian Properties</h2>
<ul>
<li><p>If <span class="math display">\[
\dataVector = \mappingMatrix \inputVector + \noiseVector,
\]</span></p></li>
<li><p>Assume <span class="math display">\[
\begin{align}
\inputVector &amp; \sim \gaussianSamp{\meanVector}{\covarianceMatrix}\\
\noiseVector &amp; \sim \gaussianSamp{\zerosVector}{\covarianceMatrixTwo}
\end{align}
\]</span></p></li>
<li><p>Then <span class="math display">\[
\dataVector \sim \gaussianSamp{\mappingMatrix\meanVector}{\mappingMatrix\covarianceMatrix\mappingMatrix^\top + \covarianceMatrixTwo}.
\]</span> If <span class="math inline">\(\covarianceMatrixTwo=\dataStd^2\eye\)</span>, this is Probabilistic Principal Component Analysis <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>, because we integrated out the inputs (or <em>latent</em> variables they would be called in that case).</p></li>
</ul>
</section>
<section id="non-linear-on-inputs" class="slide level2">
<h2>Non linear on Inputs</h2>
<ul>
<li>Set each activation function computed at each data point to be</li>
</ul>
<p><span class="math display">\[
\activationScalar_{i,j} = \activationScalar(\mappingVector^{(1)}_{j}, \inputVector_{i})
\]</span> Define <em>design matrix</em> <span class="math display">\[
\activationMatrix = 
\begin{bmatrix}
\activationScalar_{1, 1} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numHidden} \\
\activationScalar_{1, 2} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numData} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\activationScalar_{\numData, 1} &amp; \activationScalar_{\numData, 2} &amp; \dots &amp; \activationScalar_{\numData, \numHidden}
\end{bmatrix}.
\]</span></p>
</section>
<section id="matrix-representation-of-a-neural-network" class="slide level2">
<h2>Matrix Representation of a Neural Network</h2>
<p><span class="math display">\[\dataScalar\left(\inputVector\right) = \activationVector\left(\inputVector\right)^\top \mappingVector + \noiseScalar\]</span></p>
<div class="fragment">
<p><span class="math display">\[\dataVector = \activationMatrix\mappingVector + \noiseVector\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}\]</span></p>
</div>
</section>
<section id="prior-density" class="slide level2">
<h2>Prior Density</h2>
<ul>
<li>Define</li>
</ul>
<p><span class="math display">\[
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha\eye},
\]</span></p>
<ul>
<li>Rules of multivariate Gaussians to see that,</li>
</ul>
<p><span class="math display">\[
\dataVector \sim \gaussianSamp{\zerosVector}{\alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye}.
\]</span></p>
<p><span class="math display">\[
\kernelMatrix = \alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye.
\]</span></p>
</section>
<section id="joint-gaussian-density" class="slide level2">
<h2>Joint Gaussian Density</h2>
<ul>
<li>Elements are a function <span class="math inline">\(\kernel_{i,j} = \kernel\left(\inputVector_i, \inputVector_j\right)\)</span></li>
</ul>
<p><span class="math display">\[
\kernelMatrix = \alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye.
\]</span></p>
</section>
<section id="covariance-function" class="slide level2">
<h2>Covariance Function</h2>
<p><span class="math display">\[
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)
\]</span></p>
<ul>
<li>formed by inner products of the rows of the <em>design matrix</em>.</li>
</ul>
</section>
<section id="gaussian-process" class="slide level2">
<h2>Gaussian Process</h2>
<ul>
<li><p>Instead of making assumptions about our density over each data point, <span class="math inline">\(\dataScalar_i\)</span> as i.i.d.</p></li>
<li><p>make a joint Gaussian assumption over our data.</p></li>
<li><p>covariance matrix is now a function of both the parameters of the activation function, <span class="math inline">\(\mappingMatrix_1\)</span>, and the input variables, <span class="math inline">\(\inputMatrix\)</span>.</p></li>
<li><p>Arises from integrating out <span class="math inline">\(\mappingVector^{(2)}\)</span>.</p></li>
</ul>
</section>
<section id="basis-functions" class="slide level2">
<h2>Basis Functions</h2>
<ul>
<li>Can be very complex, such as deep kernels, <span class="citation" data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or could even put a convolutional neural network inside.</li>
<li>Viewing a neural network in this way is also what allows us to beform sensible <em>batch</em> normalizations <span class="citation" data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</li>
</ul>
</section>
<section id="non-degenerate-gaussian-processes" class="slide level2">
<h2>Non-degenerate Gaussian Processes</h2>
<ul>
<li>This process is <em>degenerate</em>.</li>
<li>Covariance function is of rank at most <span class="math inline">\(\numHidden\)</span>.</li>
<li>As <span class="math inline">\(\numData \rightarrow \infty\)</span>, covariance matrix is not full rank.</li>
<li>Leading to <span class="math inline">\(\det{\kernelMatrix} = 0\)</span></li>
</ul>
</section>
<section id="infinite-networks" class="slide level2">
<h2>Infinite Networks</h2>
<ul>
<li>In ML Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> asked “what would happen if you took <span class="math inline">\(\numHidden \rightarrow \infty\)</span>?”</li>
</ul>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Page 37 of <a href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s 1994 thesis</a>
</aside>
</section>
<section id="roughly-speaking" class="slide level2">
<h2>Roughly Speaking</h2>
<ul>
<li><p>Instead of</p>
<p><span class="math display">\[
\begin{align*}
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) &amp; = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)\\
&amp; = \alpha \sum_k \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_j\right)
\end{align*}
\]</span></p></li>
<li><p>Sample infinitely many from a prior density, <span class="math inline">\(p(\mappingVector^{(1)})\)</span>,</p></li>
</ul>
<p><span class="math display">\[
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \int \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}, \inputVector_j\right) p(\mappingVector^{(1)}) \text{d}\mappingVector^{(1)}
\]</span></p>
<ul>
<li>Also applies for non-Gaussian <span class="math inline">\(p(\mappingVector^{(1)})\)</span> because of the <em>central limit theorem</em>.</li>
</ul>
</section>
<section id="simple-probabilistic-program" class="slide level2">
<h2>Simple Probabilistic Program</h2>
<ul>
<li><p>If <span class="math display">\[
\begin{align*} 
\mappingVector^{(1)} &amp; \sim p(\cdot)\\ \phi_i &amp; = \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right), 
\end{align*}
\]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a Gaussian process.</p></li>
</ul>
</section>
<section id="further-reading" class="slide level2">
<h2>Further Reading</h2>
<ul>
<li><p>Chapter 2 of Neal’s thesis <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>Rest of Neal’s thesis. <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>David MacKay’s PhD thesis <span class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span></p></li>
</ul>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<!-- ### Two Dimensional Gaussian Distribution -->
<!-- include{_ml/includes/two-d-gaussian.md} -->
</section>
<section id="distributions-over-functions" class="slide level2">
<h2>Distributions over Functions</h2>
</section>
<section id="sampling-a-function" class="slide level2">
<h2>Sampling a Function</h2>
<p><strong>Multi-variate Gaussians</strong></p>
<ul>
<li>We will consider a Gaussian with a particular structure of covariance matrix.</li>
<li>Generate a single sample from this 25 dimensional Gaussian density, <span class="math display">\[
\mappingFunctionVector=\left[\mappingFunction_{1},\mappingFunction_{2}\dots \mappingFunction_{25}\right].
\]</span></li>
<li>We will plot these points against their index.</li>
</ul>
</section>
<section id="gaussian-distribution-sample" class="slide level2">
<h2>Gaussian Distribution Sample</h2>
<script>
showDivs(0, 'two_point_sample');
</script>
<p><small></small> <input id="range-two_point_sample" type="range" min="0" max="8" value="0" onchange="setDivs('two_point_sample')" oninput="setDivs('two_point_sample')"> <button onclick="plusDivs(-1, 'two_point_sample')">❮</button> <button onclick="plusDivs(1, 'two_point_sample')">❯</button></p>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample000.svg" width style=" ">
</object>
</div>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample001.svg" width style=" ">
</object>
</div>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample002.svg" width style=" ">
</object>
</div>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample003.svg" width style=" ">
</object>
</div>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample004.svg" width style=" ">
</object>
</div>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample005.svg" width style=" ">
</object>
</div>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample006.svg" width style=" ">
</object>
</div>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample007.svg" width style=" ">
</object>
</div>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample008.svg" width style=" ">
</object>
</div>
</section>
<section id="prediction-of-mappingfunction_2-from-mappingfunction_1" class="slide level2">
<h2>Prediction of <span class="math inline">\(\mappingFunction_{2}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h2>
<script>
showDivs(9, 'two_point_sample2');
</script>
<p><small></small> <input id="range-two_point_sample2" type="range" min="9" max="12" value="9" onchange="setDivs('two_point_sample2')" oninput="setDivs('two_point_sample2')"> <button onclick="plusDivs(-1, 'two_point_sample2')">❮</button> <button onclick="plusDivs(1, 'two_point_sample2')">❯</button></p>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample009.svg" width style=" ">
</object>
</div>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample010.svg" width style=" ">
</object>
</div>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample011.svg" width style=" ">
</object>
</div>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample012.svg" width style=" ">
</object>
</div>
</section>
<section id="uluru" class="slide level2">
<h2>Uluru</h2>
<div class="figure">
<div id="uluru-as-probability-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gp/799px-Uluru_Panorama.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Uluru, the sacred rock in Australia. If we think of it as a probability density, viewing it from this side gives us one <em>marginal</em> from the density. Figuratively speaking, slicing through the rock would give a conditional density.
</aside>
</section>
<section id="prediction-with-correlated-gaussians" class="slide level2">
<h2>Prediction with Correlated Gaussians</h2>
<ul>
<li>Prediction of <span class="math inline">\(\mappingFunction_2\)</span> from <span class="math inline">\(\mappingFunction_1\)</span> requires <em>conditional density</em>.</li>
<li>Conditional density is <em>also</em> Gaussian. <span class="math display">\[
p(\mappingFunction_2|\mappingFunction_1) = \gaussianDist{\mappingFunction_2}{\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1}{ \kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}}
\]</span> where covariance of joint density is given by <span class="math display">\[
\kernelMatrix = \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}.\end{bmatrix}
\]</span></li>
</ul>
</section>
<section id="prediction-of-mappingfunction_8-from-mappingfunction_1" class="slide level2">
<h2>Prediction of <span class="math inline">\(\mappingFunction_{8}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h2>
<script>
showDivs(13, 'two_point_sample3');
</script>
<p><small></small> <input id="range-two_point_sample3" type="range" min="13" max="17" value="13" onchange="setDivs('two_point_sample3')" oninput="setDivs('two_point_sample3')"> <button onclick="plusDivs(-1, 'two_point_sample3')">❮</button> <button onclick="plusDivs(1, 'two_point_sample3')">❯</button></p>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample013.svg" width style=" ">
</object>
</div>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample014.svg" width style=" ">
</object>
</div>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample015.svg" width style=" ">
</object>
</div>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample016.svg" width style=" ">
</object>
</div>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample017.svg" width style=" ">
</object>
</div>
</section>
<section id="key-object" class="slide level2">
<h2>Key Object</h2>
<ul>
<li>Covariance function, <span class="math inline">\(\kernelMatrix\)</span></li>
<li>Determines properties of samples.</li>
<li>Function of <span class="math inline">\(\inputMatrix\)</span>, <span class="math display">\[\kernelScalar_{i,j} = \kernelScalar(\inputVector_i, \inputVector_j)\]</span></li>
</ul>
</section>
<section id="linear-algebra" class="slide level2">
<h2>Linear Algebra</h2>
<ul>
<li><p>Posterior mean <span class="math display">\[\mappingFunction_D(\inputVector_*) = \kernelVector(\inputVector_*, \inputMatrix) \kernelMatrix^{-1}
\dataVector\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \kernelMatrix_{*,*} - \kernelMatrix_{*,\mappingFunctionVector}
\kernelMatrix^{-1} \kernelMatrix_{\mappingFunctionVector, *}\]</span></p></li>
</ul>
</section>
<section id="linear-algebra-1" class="slide level2">
<h2>Linear Algebra</h2>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[\mappingFunction_D(\inputVector_*) = \kernelVector(\inputVector_*, \inputMatrix) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\covarianceMatrix_* = \kernelMatrix_{*,*} - \kernelMatrix_{*,\mappingFunctionVector}
\kernelMatrix^{-1} \kernelMatrix_{\mappingFunctionVector, *}\]</span></p></li>
</ul>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level2">
<h2>Exponentiated Quadratic Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The exponentiated quadratic covariance function.
</aside>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1892.
</aside>
</section>
<section id="alan-turing" class="slide level2">
<h2>Alan Turing</h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank" >Alan Turing Internet Scrapbook</a>.
</aside>
</section>
<section id="probability-winning-olympics" class="slide level2">
<h2>Probability Winning Olympics?</h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
</section>
<section id="olympic-marathon-data-gp" class="slide level2">
<h2>Olympic Marathon Data GP</h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-marathon-gp.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Marathon data. The error bars are too large, perhaps due to the outlier from 1904.
</aside>
</section>
<section id="learning-covariance-parameters" class="slide level2">
<h2>Learning Covariance Parameters</h2>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\]</span></p>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\color{yellow} \det{\kernelMatrix}^{\frac{1}{2}}}}{\color{cyan}\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-27" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;{\color{yellow}-\frac{1}{2}\log\det{\kernelMatrix}}{\color{cyan}-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\errorFunction(\parameterVector) = {\color{yellow} \frac{1}{2}\log\det{\kernelMatrix}} + {\color{cyan} \frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
\]</span></p>
</section>
<section id="section-28" class="slide level2">
<h2></h2>
<p>The parameters are <em>inside</em> the covariance function (matrix).  <span class="math display">\[\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)\]</span></p>
</section>
<section id="eigendecomposition-of-covariance" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<p><span> <span class="math display">\[\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\eigenvalueMatrix\)</span> represents distance on axes. <span class="math inline">\(\rotationMatrix\)</span> gives rotation.
</td>
</tr>
</table>
</section>
<section id="eigendecomposition-of-covariance-1" class="slide level2">
<h2>Eigendecomposition of Covariance</h2>
<ul>
<li><span class="math inline">\(\eigenvalueMatrix\)</span> is <em>diagonal</em>, <span class="math inline">\(\rotationMatrix^\top\rotationMatrix = \eye\)</span>.</li>
<li>Useful representation since <span class="math inline">\(\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2\)</span>.</li>
</ul>
</section>
<section id="capacity-control-coloryellow-log-detkernelmatrix" class="slide level2">
<h2>Capacity control: <span class="math inline">\({\color{yellow} \log \det{\kernelMatrix}}\)</span></h2>
<!--
\only<1>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant1.svg}}\only<2>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant2.svg}}\only<3>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant3.svg}}\only<4>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant4.svg}}\only<5>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant5.svg}}\only<6>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant6.svg}}\only<7>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant7.svg}}\only<8>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant8.svg}}\only<9>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant9.svg}}\only<10>{\input{../../../gp/tex/diagrams/gpOptimiseDeterminant10.svg}}-->
</section>
<section id="data-fit-colorcyan-fracdatavectortopkernelmatrix-1datavector2" class="slide level2">
<h2>Data Fit: <span class="math inline">\({\color{cyan} \frac{\dataVector^\top\kernelMatrix^{-1}\dataVector}{2}}\)</span></h2>
<!--

\only<1>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic1.svg}}\only<2>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic2.svg}}\only<3>{\input{../../../gp/tex/diagrams/gpOptimiseQuadratic3.svg}}-->
</section>
<section id="errorfunctionparametervector-coloryellowfrac12logdetkernelmatrixcolorcyanfracdatavectortopkernelmatrix-1datavector2" class="slide level2">
<h2><span class="math display">\[\errorFunction(\parameterVector) = {\color{yellow}\frac{1}{2}\log\det{\kernelMatrix}}+{\color{cyan}\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}\]</span></h2>
<script>
showDivs(0, 'gp-optimise');
</script>
<p><small></small> <input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')"> <button onclick="plusDivs(-1, 'gp-optimise')">❮</button> <button onclick="plusDivs(1, 'gp-optimise')">❯</button></p>
</section>
<section id="section-29" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise000.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise001.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-30" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise002.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise003.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-31" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise004.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise005.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-32" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise007.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-33" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise008.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise009.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-34" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise011.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-35" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise012.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise013.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-36" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise014.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise015.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-37" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise017.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-38" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise018.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise019.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-39" class="slide level2">
<h2></h2>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise020.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="della-gatta-gene-data" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gene expression levels over time for a gene from data provided by <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>. We would like to understand whethere there is signal in the data, or we are only observing noise.
</aside>
</section>
<section id="gene-expression-example" class="slide level2">
<h2>Gene Expression Example</h2>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-40" class="slide level2">
<h2></h2>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The example is taken from the paper “A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through Gaussian Process Regression.” <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.
</aside>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-41" class="slide level2">
<h2></h2>
</section>
<section id="tp53-gene-data-gp" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale parameter initialized to 50 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-1" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale parameter initialized to 2000 minutes.
</aside>
</section>
<section id="tp53-gene-data-gp-2" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the noise initialized low (standard deviation 0.1) and the time scale parameter initialized to 20 minutes.
</aside>
</section>
<section id="multiple-optima" class="slide level2">
<h2>Multiple Optima</h2>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<!--
## Multiple Optima  {}



<object class="svgplot " data="../slides/diagrams/gp/multiple-optima001.svg" width="" style=" "></object>-->
</section>
<section id="example-prediction-of-malaria-incidence-in-uganda" class="slide level2">
<h2>Example: Prediction of Malaria Incidence in Uganda</h2>
<span style="text-align:right"><svg viewBox="2662 600 1110-2662 1780-600" style="width:1.5cm"> <defs> <clipPath id="clip">
<style>
  rect {
      fill: black;
         }
  </style>
<p><rect x="2662" y="600" width="1110-2662" height="1780-600"/> </clipPath> </defs> <image preserveAspectRatio="xMinYMin slice"  xlink:href="../slides/diagrams/people/2013_03_28_180606.JPG" clip-path="url(#clip)" /> </svg></span></p>
<ul>
<li>Work with Ricardo Andrade Pacheco, John Quinn and Martin Mubaganzi (Makerere University, Uganda)</li>
<li>See <a href="http://air.ug/research.html">AI-DEV Group</a>.</li>
</ul>
</section>
<section id="malaria-prediction-in-uganda" class="slide level2">
<h2>Malaria Prediction in Uganda</h2>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Ugandan districs. Data SRTM/NASA from <a href="https://dds.cr.usgs.gov/srtm/version2_1" class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.
</aside>
<p><span style="text-align:right"><span class="citation" data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco et al., 2014; Mubangizi et al., 2014)</span></span></p>
</section>
<section id="kapchorwa-district" class="slide level2">
<h2>Kapchorwa District</h2>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Kapchorwa_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kapchorwa District, home district of Stephen Kiprotich.
</aside>
</section>
<section id="tororo-district" class="slide level2">
<h2>Tororo District</h2>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Tororo district, where the sentinel site, Nagongera, is located.
</aside>
</section>
<section id="malaria-prediction-in-nagongera-sentinel-site" class="slide level2">
<h2>Malaria Prediction in Nagongera (Sentinel Site)</h2>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Sentinel and HMIS data along with rainfall and temperature for the Nagongera sentinel station in the Tororo district.
</aside>
</section>
<section id="mubende-district" class="slide level2">
<h2>Mubende District</h2>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Mubende District.
</aside>
</section>
<section id="malaria-prediction-in-uganda-1" class="slide level2">
<h2>Malaria Prediction in Uganda</h2>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Prediction of malaria incidence in Mubende.
</aside>
</section>
<section id="gp-school-at-makerere" class="slide level2">
<h2>GP School at Makerere</h2>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The project arose out of the Gaussian process summer school held at Makerere in Kampala in 2013. The school led, in turn, to the Data Science Africa initiative.
</aside>
</section>
<section id="kabarole-district" class="slide level2">
<h2>Kabarole District</h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="../slides/diagrams/health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kabarole district in Uganda.
</aside>
</section>
<section id="early-warning-system" class="slide level2">
<h2>Early Warning System</h2>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Estimate of the current disease situation in the Kabarole district over time. Estimate is constructed with a Gaussian process with an additive covariance funciton.
</aside>
</section>
<section id="early-warning-systems" class="slide level2">
<h2>Early Warning Systems</h2>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The map of Ugandan districts with an overview of the Malaria situation in each district.
</aside>
</section>
<section id="additive-covariance" class="slide level2">
<h2>Additive Covariance</h2>
<center>
<span class="math display">\[\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/add_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
An additive covariance function formed by combining two exponentiated quadratic covariance functions.
</aside>
</section>
<section id="section-42" class="slide level2">
<h2></h2>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
This is a retrospective analysis of US births by Aki Vehtari. The challenges of forecasting. Even with seasonal and weekly effects removed there are significant effects on holidays, weekends, etc.
</aside>
</section>
<section id="gelman-book" class="slide level2">
<h2>Gelman Book</h2>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Two different editions of Bayesian Data Analysis <span class="citation" data-cites="Gelman:bayesian13">(Gelman et al., 2013)</span>.
</aside>
<p><span style="text-align:right"><span class="citation" data-cites="Gelman:bayesian13">Gelman et al. (2013)</span></span></p>
</section>
<section id="basis-function-covariance" class="slide level2">
<h2>Basis Function Covariance</h2>
<center>
<span class="math display">\[\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/basis_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
A covariance function based on a non-linear basis given by <span class="math inline">\(\basisVector(\inputVector)\)</span>.
</aside>
</section>
<section id="brownian-covariance" class="slide level2">
<h2>Brownian Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)\]</span>
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/brownian_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Brownian motion covariance function.
</aside>
</section>
<section id="mlp-covariance" class="slide level2">
<h2>MLP Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/mlp_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The multi-layer perceptron covariance function. This is derived by considering the infinite limit of a neural network with probit activation functions.
</aside>
</section>
<section id="relu-covariance" class="slide level2">
<h2>RELU Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = 
\alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}
{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)
\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="relu-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/relu_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/relu_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Rectified linear unit covariance function.
</aside>
</section>
<section id="sinc-covariance" class="slide level2">
<h2>Sinc Covariance</h2>

</section>
<section id="polynomial-covariance" class="slide level2">
<h2>Polynomial Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha(w \inputVector^\top\inputVector^\prime + b)^d\]</span>
</center>
<div class="figure">
<div id="polynomial-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/polynomial_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Polynomial covariance function.
</aside>
</section>
<section id="periodic-covariance" class="slide level2">
<h2>Periodic Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha\exp\left(\frac{-2\sin(\pi rw)^2}{\lengthScale^2}\right)\]</span>
</center>
<div class="figure">
<div id="periodic-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/periodic_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/periodic_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Periodic covariance function.
</aside>
</section>
<section id="linear-model-of-coregionalization-covariance" class="slide level2">
<h2>Linear Model of Coregionalization Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(i, j, \inputVector, \inputVector^\prime) = b_{i,j} \kernelScalar(\inputVector, \inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="lmc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/lmc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/lmc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Linear model of coregionalization covariance function.
</aside>
</section>
<section id="intrinsic-coregionalization-model-covariance" class="slide level2">
<h2>Intrinsic Coregionalization Model Covariance</h2>
<center>
<span class="math display">\[\kernelScalar(i, j, \inputVector, \inputVector^\prime) = b_{i,j} \kernelScalar(\inputVector, \inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="icm-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/icm_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/icm_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Intrinsic coregionalization model covariance function.
</aside>
</section>
<section id="gpss-gaussian-process-summer-school" class="slide level2">
<h2>GPSS: Gaussian Process Summer School</h2>
<table>
<tr>
<td width="60%">
<ul>
<li><a href="http://gpss.cc" class="uri">http://gpss.cc</a></li>
<li>Next one is in Sheffield in <em>September 2019</em>.</li>
<li>Many lectures from past meetings available online</li>
</ul>
</td>
<td width="40%">
<div style="width:1.5cm;text-align:center">

</div>
</td>
</tr>
</table>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="other-software" class="slide level2">
<h2>Other Software</h2>
<ul>
<li><a href="https://github.com/GPflow/GPflow">GPflow</a></li>
<li><a href="https://github.com/cornellius-gp/gpytorch">GPyTorch</a></li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></p></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>blog posts:</p>
<p><a href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What is Machine Learning?</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Andrade:consistent14">
<p>Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014. Consistent mapping of government malaria records across a changing territory delimitation. Malaria Journal 13. <a href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a></p>
</div>
<div id="ref-Cho:deep09">
<p>Cho, Y., Saul, L.K., 2009. Kernel methods for deep learning, in: Bengio, Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.</p>
</div>
<div id="ref-DellaGatta:direct08">
<p>Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D., Missero, C., Bernardo, D. di, 2008. Direct targets of the trp63 transcription factor revealed by a combination of gene expression profiling and reverse engineering. Genome Research 18, 939–948. <a href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a></p>
</div>
<div id="ref-Gelman:bayesian13">
<p>Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.</p>
</div>
<div id="ref-Ioffe:batch15">
<p>Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: Bach, F., Blei, D. (Eds.), Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, Lille, France, pp. 448–456.</p>
</div>
<div id="ref-Kalaitzis:simple11">
<p>Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking differentially expressed gene expression time courses through Gaussian process regression. BMC Bioinformatics 12. <a href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a></p>
</div>
<div id="ref-MacKay:bayesian92">
<p>MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis). California Institute of Technology.</p>
</div>
<div id="ref-McCulloch:neuron43">
<p>McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5, 115–133.</p>
</div>
<div id="ref-Mubangizi:malaria14">
<p>Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence, N.D., 2014. Malaria surveillance with multiple data sources using Gaussian process models, in: 1st International Conference on the Use of Mobile ICT in Africa.</p>
</div>
<div id="ref-Neal:bayesian94">
<p>Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis). Dept. of Computer Science, University of Toronto.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine learning. mit, Cambridge, MA.</p>
</div>
<div id="ref-Steele:predictive12">
<p>Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson, E., Avital, I., Stojadinovic, A., 2012. Using machine-learned Bayesian belief networks to predict perioperative risk of clostridium difficile infection following colon surgery. Interact J Med Res 1, e6. <a href="https://doi.org/10.2196/ijmr.2131">https://doi.org/10.2196/ijmr.2131</a></p>
</div>
<div id="ref-Tipping:probpca99">
<p>Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
