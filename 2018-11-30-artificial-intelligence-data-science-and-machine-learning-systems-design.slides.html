<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2018-11-30">
  <title>Artificial Intelligence, Data Science and Machine Learning Systems Design</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["color.js"]
          }
        });
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
    document.setElementById("range-".concat(group)) = slideIndex
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
$$\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
$$
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Artificial Intelligence, Data Science and Machine Learning Systems Design</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2018-11-30</time></p>
  <p class="venue" style="text-align:center">Fifth International Workshop on Sustainable Road Freight, Cambridge</p>
</section>

<section class="slide level3">

<p><!--% not ipynb--></p>
<p><!--% not notes--></p>
<!-- SECTION The Gartner Hype Cycle -->
</section>
<section id="gartner-hype-cycle" class="slide level3">
<h3>Gartner Hype Cycle</h3>
<p><img class="negate" src="../slides/diagrams/Gartner_Hype_Cycle.png" width="70%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/data-science/bd-ds-iot-ml-google-trends000.svg">
</object>
</section>
<section id="section-1" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/data-science/bd-ds-iot-ml-google-trends001.svg">
</object>
</section>
<section id="section-2" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/data-science/bd-ds-iot-ml-google-trends002.svg">
</object>
</section>
<section id="section-3" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/data-science/bd-ds-iot-ml-google-trends003.svg">
</object>
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>??</p>
</blockquote>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Benjamin Disraeli</p>
</blockquote>
</section>
<section id="section-6" class="slide level3">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Benjamin Disraeli 1804-1881</p>
</blockquote>
</section>
<section id="mathematical-statistics" class="slide level3">
<h3><em>Mathematical</em> Statistics</h3>
<ul>
<li>‘Founded’ by Karl Pearson (1857-1936)</li>
</ul>
<p><img class="" src="../slides/diagrams/Portrait_of_Karl_Pearson.jpg" width="30%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-7" class="slide level3">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and ‘big data’</p>
<p>Neil Lawrence 1972-?</p>
</blockquote>
</section>
<section id="mathematical-data-science" class="slide level3">
<h3>‘Mathematical Data Science’</h3>
<ul>
<li>‘Founded’ by ? (?-?)</li>
</ul>
<p><img class="" src="../slides/diagrams/Question_mark.png" width="30%" align="center" style="background:none; border:none; box-shadow:none;"></p>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<div class="fragment">
<ul>
<li>To combine data with a model need:</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>an objective function</strong> <span class="math inline">\(\errorFunction(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</div>
</section>
<section id="machine-learning" class="slide level3 slide:" data-transition="none">
<h3>Machine Learning</h3>
<ul>
<li>Driver of two different domains:
<ol type="1">
<li><em>Data Science</em>: arises from the fact that we now capture data by happenstance.</li>
<li><em>Artificial Intelligence</em>: emulation of human behaviour.</li>
</ol></li>
<li>Connection: Internet of Things</li>
</ul>
</section>
<section id="machine-learning-1" class="slide level3 slide:" data-transition="none">
<h3>Machine Learning</h3>
<ul>
<li>Driver of two different domains:
<ol type="1">
<li><em>Data Science</em>: arises from the fact that we now capture data by happenstance.</li>
<li><em>Artificial Intelligence</em>: emulation of human behaviour.</li>
</ol></li>
<li>Connection: Internet of <del>Things</del></li>
</ul>
</section>
<section id="machine-learning-2" class="slide level3 slide:" data-transition="none">
<h3>Machine Learning</h3>
<ul>
<li>Driver of two different domains:
<ol type="1">
<li><em>Data Science</em>: arises from the fact that we now capture data by happenstance.</li>
<li><em>Artificial Intelligence</em>: emulation of human behaviour.</li>
</ol></li>
<li>Connection: Internet of People</li>
</ul>
</section>
<section id="what-does-machine-learning-do" class="slide level3">
<h3>What does Machine Learning do?</h3>
<ul>
<li>ML Automates through Data
<ul>
<li><em>Strongly</em> related to statistics.</li>
<li>Field underpins revolution in <em>data science</em> and <em>AI</em></li>
</ul></li>
<li>With AI:
<ul>
<li><em>logic</em>, <em>robotics</em>, <em>computer vision</em>, <em>speech</em></li>
</ul></li>
<li>With Data Science:
<ul>
<li><em>databases</em>, <em>data mining</em>, <em>statistics</em>, <em>visualization</em></li>
</ul></li>
</ul>
<!-- SECTION Natural and Artificial Intelligence: Embodiment Factors -->
</section>
<section id="embodiment-factors" class="slide level3">
<h3>“Embodiment Factors”</h3>
<table>
<tr>
<td>
</td>
<td align="center">
<img class="" src="../slides/diagrams/IBM_Blue_Gene_P_supercomputer.jpg" width="40%" align="center" style="background:none; border:none; box-shadow:none;">
</td>
<td align="center">
<img class="" src="../slides/diagrams/ClaudeShannon_MFO3807.jpg" width="25%" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
<tr>
<td>
compute
</td>
<td align="center">
<span class="math display">\[\approx 100 \text{ gigaflops}\]</span>
</td>
<td align="center">
<span class="math display">\[\approx 16 \text{ petaflops}\]</span>
</td>
</tr>
<tr>
<td>
communicate
</td>
<td align="center">
<span class="math display">\[1 \text{ gigbit/s}\]</span>
</td>
<td align="center">
<span class="math display">\[100 \text{ bit/s}\]</span>
</td>
</tr>
<tr>
<td>
(compute/communicate)
</td>
<td align="center">
<span class="math display">\[10^{4}\]</span>
</td>
<td align="center">
<span class="math display">\[10^{14}\]</span>
</td>
</tr>
</table>
<p>See <a href="https://arxiv.org/abs/1705.07996">“Living Together: Mind and Machine Intelligence”</a></p>
<!-- SECTION Evolved Relationship with Information -->
</section>
<section id="evolved-relationship" class="slide level3 slide:" data-transition="none">
<h3>Evolved Relationship</h3>
<object class="svgplot " align data="../slides/diagrams/data-science/information-flow001.svg">
</object>
</section>
<section id="evolved-relationship-1" class="slide level3 slide:" data-transition="none">
<h3>Evolved Relationship</h3>
<object class="svgplot " align data="../slides/diagrams/data-science/information-flow002.svg">
</object>
</section>
<section id="evolved-relationship-2" class="slide level3 slide:" data-transition="none">
<h3>Evolved Relationship</h3>
<object class="svgplot " align data="../slides/diagrams/data-science/information-flow003.svg">
</object>
</section>
<section id="what-does-machine-learning-do-1" class="slide level3">
<h3>What does Machine Learning do?</h3>
<ul>
<li>Automation scales by codifying processes and automating them.</li>
<li>Need:
<ul>
<li>Interconnected components</li>
<li>Compatible components</li>
</ul></li>
<li>Early examples:
<ul>
<li>cf Colt 45, Ford Model T</li>
</ul></li>
</ul>
</section>
<section id="codify-through-mathematical-functions" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression <span class="math display">\[ \text{odds} = \frac{p(\text{bought})}{p(\text{not bought})} \]</span> <span class="math display">\[ \log \text{odds}  = \beta_0 + \beta_1 \text{age} + \beta_2 \text{latitude}\]</span></li>
</ul>
</section>
<section id="codify-through-mathematical-functions-1" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression <span class="math display">\[ p(\text{bought}) =  \sigmoid{\beta_0 + \beta_1 \text{age} + \beta_2 \text{latitude}}\]</span></li>
</ul>
</section>
<section id="codify-through-mathematical-functions-2" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression <span class="math display">\[ p(\text{bought}) =  \sigmoid{\boldsymbol{\beta}^\top \inputVector}\]</span></li>
</ul>
</section>
<section id="codify-through-mathematical-functions-3" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression <span class="math display">\[ \dataScalar =  \mappingFunction\left(\inputVector, \boldsymbol{\beta}\right)\]</span></li>
</ul>
<div class="fragment">
<p>We call <span class="math inline">\(\mappingFunction(\cdot)\)</span> the <em>prediction function</em></p>
</div>
</section>
<section id="fit-to-data" class="slide level3">
<h3>Fit to Data</h3>
<ul>
<li>Use an objective function <span class="math display">\[\errorFunction(\boldsymbol{\beta}, \dataMatrix, \inputMatrix)\]</span></li>
</ul>
<div class="fragment">
<ul>
<li>E.g. least squares <span class="math display">\[\errorFunction(\boldsymbol{\beta}, \dataMatrix, \inputMatrix) = \sum_{i=1}^\numData \left(\dataScalar_i - \mappingFunction(\inputVector_i, \boldsymbol{\beta})\right)^2.\]</span></li>
</ul>
</div>
</section>
<section id="two-components" class="slide level3">
<h3>Two Components</h3>
<ul>
<li>Prediction function, <span class="math inline">\(\mappingFunction(\cdot)\)</span></li>
<li>Objective function, <span class="math inline">\(\errorFunction(\cdot)\)</span></li>
</ul>
</section>
<section id="deep-learning" class="slide level3">
<h3>Deep Learning</h3>
<ul>
<li><p>These are interpretable models: vital for disease etc.</p></li>
<li><p>Modern machine learning methods are less interpretable</p></li>
<li><p>Example: face recognition</p></li>
</ul>
</section>
<section id="section-8" class="slide level3">
<h3></h3>
<p><span class="fragment fade-in"><small>Outline of the DeepFace architecture. A front-end of a single convolution-pooling-convolution filtering on the rectified input, followed by three locally-connected layers and two fully-connected layers. Color illustrates feature maps produced at each layer. The net includes more than 120 million parameters, where more than 95% come from the local and fully connected.</small></span></p>
<p><img class="" src="../slides/diagrams/deepface_neg.png" width="100%" align="center" style="background:none; border:none; box-shadow:none;"> <span align="right"><small>Source: DeepFace <span class="citation" data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small></span></p>
</section>
<section id="section-9" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/576px-Early_Pinball.jpg" width="50%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-10" class="slide level3">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/pinball001.svg">
</object>
</section>
<section id="section-11" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/pinball002.svg">
</object>
<p><a href="file:///Users/neil/lawrennd/talks_gp/includes/gp-intro-very-short.md">_gp/includes/gp-intro-very-short.md</a></p>

</section>
<section id="section-12" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample001.svg">
</object>
</section>
<section id="section-13" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample002.svg">
</object>
</section>
<section id="section-14" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample003.svg">
</object>
</section>
<section id="section-15" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample004.svg">
</object>
</section>
<section id="section-16" class="slide level3" data-transition="none">
<h3></h3>
<p>x <object class="svgplot " align="" data="../slides/diagrams/gp/gp_rejection_sample005.svg"></object></p>
</section>
<section id="olympic-marathon-data" class="slide level3">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td width="70%">
<ul>
<li><p>Gold medal times for Olympic Marathon since 1896.</p></li>
<li><p>Marathons before 1924 didn’t have a standardised distance.</p></li>
<li><p>Present results using pace per km.</p></li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<img src="../slides/diagrams/Stephen_Kiprotich.jpg" alt="image" /> <small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3">
<h3>Olympic Marathon Data</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/olympic-marathon.svg">
</object>
</section>
<section id="olympic-marathon-data-gp" class="slide level3">
<h3>Olympic Marathon Data GP</h3>
<object class="svgplot " align data="../slides/diagrams/gp/olympic-marathon-gp.svg">
</object>
</section>
<section id="section-17" class="slide level3">
<h3></h3>
<table>
<tr>
<td width="40%">
<img class="" src="../slides/diagrams/turing-run.jpg" width="40%" align="" style="background:none; border:none; box-shadow:none;">
</td>
<td width="50%">
<img class="" src="../slides/diagrams/turing-times.gif" width="50%" align="" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
<center>
<em>Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html">Alan Turing Internet Scrapbook</a> </em>
</center>
</section>
<section id="deep-gp-fit" class="slide level3">
<h3>Deep GP Fit</h3>
<ul>
<li><p>Can a Deep Gaussian process help?</p></li>
<li><p>Deep GP is one GP feeding into another.</p></li>
</ul>
</section>
<section id="olympic-marathon-data-deep-gp" class="slide level3">
<h3>Olympic Marathon Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp.svg">
</object>
</section>
<section id="olympic-marathon-data-deep-gp-1" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data Deep GP</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-samples.svg">
</object>
</section>
<section id="olympic-marathon-data-latent-1" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data Latent 1</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-0.svg">
</object>
</section>
<section id="olympic-marathon-data-latent-2" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data Latent 2</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-1.svg">
</object>
</section>
<section id="olympic-marathon-pinball-plot" class="slide level3">
<h3>Olympic Marathon Pinball Plot</h3>
<object class="svgplot " align data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-pinball.svg">
</object>
</section>
<section id="data-science" class="slide level3">
<h3>Data Science</h3>
<ul>
<li>Industrial Revolution 4.0?</li>
<li><em>Industrial Revolution</em> (1760-1840) term coined by Arnold Toynbee, late 19th century.</li>
<li>Maybe: But this one is dominated by <em>data</em> not <em>capital</em></li>
<li>That presents <em>challenges</em> and <em>opportunities</em></li>
</ul>
<p>compare <a href="https://www.theguardian.com/media-network/2015/mar/05/digital-oligarchy-algorithms-personal-data">digital oligarchy</a> vs <a href="https://www.theguardian.com/media-network/2015/aug/25/africa-benefit-data-science-information">how Africa can benefit from the data revolution</a></p>
<ul>
<li>Apple vs Nokia: How you handle disruption.</li>
</ul>
</section>
<section id="a-time-for-professionalisation" class="slide level3">
<h3>A Time for Professionalisation?</h3>
<ul>
<li>New technologies historically led to new professions:
<ul>
<li>Brunel (born 1806): Civil, mechanical, naval</li>
<li>Tesla (born 1856): Electrical and power</li>
<li>William Shockley (born 1910): Electronic</li>
<li>Watts S. Humphrey (born 1927): Software</li>
</ul></li>
</ul>
</section>
<section id="why" class="slide level3">
<h3>Why?</h3>
<ul>
<li>Codification of best practice.</li>
<li>Developing trust</li>
</ul>
</section>
<section id="where-are-we" class="slide level3">
<h3>Where are we?</h3>
<ul>
<li>Perhaps around the 1980s of programming.
<ul>
<li>We understand <code>if</code>, <code>for</code>, and procedures</li>
<li>But we don’t share best practice.</li>
</ul></li>
<li>Let’s <em>avoid</em> the over formalisation of software engineering.</li>
</ul>
</section>
<section id="the-software-crisis" class="slide level3">
<h3>The Software Crisis</h3>
<blockquote>
<p>The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.</p>
<p>Edsger Dijkstra (1930-2002), The Humble Programmer</p>
</blockquote>
</section>
<section id="the-data-crisis" class="slide level3">
<h3>The Data Crisis</h3>
<blockquote>
<p>The major cause of the data crisis is that machines have become more interconnected than ever before. Data access is therefore cheap, but data quality is often poor. What we need is cheap high quality data. That implies that we develop processes for improving and verifying data quality that are efficient.</p>
<p>There would seem to be two ways for improving efficiency. Firstly, we should not duplicate work. Secondly, where possible we should automate work.</p>
<p>Me</p>
</blockquote>
</section>
<section id="rest-of-this-talk-two-areas-of-focus" class="slide level3">
<h3>Rest of this Talk: Two Areas of Focus</h3>
<ul>
<li><p>Reusability of Data</p></li>
<li><p>Deployment of Machine Learning Systems</p></li>
</ul>
</section>
<section id="rest-of-this-talk-two-areas-of-focus-1" class="slide level3">
<h3>Rest of this Talk: Two Areas of Focus</h3>
<ul>
<li><p><s>Reusability of Data</s></p></li>
<li><p>Deployment of Machine Learning Systems</p></li>
</ul>
<!-- SECTION Data Readiness Levels -->
</section>
<section id="data-readiness-levels" class="slide level3">
<h3>Data Readiness Levels</h3>
<p><a href="https://arxiv.org/pdf/1705.02245.pdf"><img class="" src="../slides/diagrams/data-science/data-readiness-levels.png" width="" align="" style="background:none; border:none; box-shadow:none;"></a></p>
<p><a href="http://inverseprobability.com/2017/01/12/data-readiness-levels">Data Readiness Levels</a></p>
</section>
<section id="three-grades-of-data-readiness" class="slide level3">
<h3>Three Grades of Data Readiness:</h3>
<ul>
<li><p>Grade C - accessibility</p></li>
<li><p>Grade B - validity</p></li>
<li><p>Grade A - usability</p></li>
</ul>
</section>
<section id="accessibility-grade-c" class="slide level3">
<h3>Accessibility: Grade C</h3>
<ul>
<li><em>Hearsay</em> data.</li>
<li>Availability, is it actually being recorded?</li>
<li>privacy or legal constraints on the accessibility of the recorded data, have ethical constraints been alleviated?</li>
<li>Format: log books, PDF …</li>
<li>limitations on access due to topology (e.g. it’s distributed across a number of devices)</li>
<li>At the end of Grade C data is ready to be loaded into analysis software (R, SPSS, Matlab, Python, Mathematica)</li>
</ul>
</section>
<section id="validity-grade-b" class="slide level3">
<h3>Validity: Grade B</h3>
<ul>
<li>faithfulness and representation</li>
<li>visualisations.</li>
<li>exploratory data analysis</li>
<li>noise characterisation.</li>
<li>Missing values.</li>
<li>Schema alignment, record linkage, data fusion?</li>
<li>Example, was a column or columns accidentally perturbed (e.g. through a sort operation that missed one or more columns)? Or was a <a href="http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-5-80">gene name accidentally converted to a date</a>?</li>
<li>At the end of Grade B, ready to define a candidate question, the context, load into OpenML</li>
</ul>
</section>
<section id="usability-grade-a" class="slide level3">
<h3>Usability: Grade A</h3>
<ul>
<li>The usability of data</li>
<li>Grade A is about data in context.</li>
<li>Consider appropriateness of a given data set to answer a particular question or to be subject to a particular analysis.</li>
<li>Data integration?</li>
<li>At the end of Grade A it’s ready for data platforms such as RAMP, Kaggle, define a <em>task</em> in OpenML.</li>
</ul>
</section>
<section id="recursive-effects" class="slide level3">
<h3>Recursive Effects</h3>
<ul>
<li>Grade A may also require:
<ul>
<li>active collection of new data.</li>
<li>rebalancing of data to ensure fairness</li>
<li>annotation of data by human experts</li>
<li>revisiting the collection (and running through the appropriate stages again)</li>
</ul></li>
</ul>
</section>
<section id="contribute" class="slide level3">
<h3>Contribute!</h3>
<ul>
<li><a href="http://data-readiness.org" class="uri">http://data-readiness.org</a></li>
</ul>
</section>
<section id="also" class="slide level3">
<h3>Also …</h3>
<ul>
<li><p>Encourage greater interaction between application domains and data scientists</p></li>
<li><p>Encourage <em>visualization</em> of data</p></li>
<li><p>Incentivise the delivery of data.</p></li>
</ul>
</section>
<section id="artificial-intelligence" class="slide level3">
<h3>Artificial Intelligence</h3>
<ul>
<li><p>Challenges in deploying AI.</p></li>
<li><p>Currently this is in the form of “machine learning systems”</p></li>
</ul>
</section>
<section id="internet-of-people" class="slide level3">
<h3>Internet of People</h3>
<ul>
<li>Fog computing: barrier between cloud and device blurring.
<ul>
<li>Computing on the Edge</li>
</ul></li>
<li>Complex feedback between algorithm and implementation</li>
</ul>
</section>
<section id="deploying-ml-in-real-world-machine-learning-systems-design" class="slide level3">
<h3>Deploying ML in Real World: Machine Learning Systems Design</h3>
<ul>
<li><p>Major new challenge for systems designers.</p></li>
<li>Internet of Intelligence but currently:
<ul>
<li>AI systems are <em>fragile</em></li>
</ul></li>
</ul>
<!-- SECTION Machine Learning System Design -->
</section>
<section id="fragility-of-ai-systems" class="slide level3">
<h3>Fragility of AI Systems</h3>
<ul>
<li>They are componentwise built from ML Capabilities.</li>
<li>Each capability is independently constructed and verified.
<ul>
<li>Pedestrian detection</li>
<li>Road line detection</li>
</ul></li>
<li>Important for verification purposes.</li>
</ul>
</section>
<section id="pigeonholing" class="slide level3">
<h3>Pigeonholing</h3>
<p><img class="" src="../slides/diagrams/TooManyPigeons.jpg" width="60%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="rapid-reimplementation" class="slide level3">
<h3>Rapid Reimplementation</h3>
<ul>
<li>Whole systems are being deployed.</li>
<li>But they change their environment.</li>
<li>The experience evolved adversarial behaviour.</li>
</ul>
</section>
<section id="early-ai" class="slide level3">
<h3>Early AI</h3>
<img class="rotateimg90" src="../slides/diagrams/2017-10-12 16.47.34.jpg" width="40%" align="center" style="background:none; border:none; box-shadow:none;">
<center>
<em>Watt’s Governor as held by “Science” on Holborn Viaduct</em>
</center>
</section>
<section id="machine-learning-systems-design" class="slide level3">
<h3>Machine Learning Systems Design</h3>
<img class="center" src="../slides/diagrams/SteamEngine_Boulton&Watt_1784_neg.png" width="50%" align="" style="background:none; border:none; box-shadow:none;">
<center>
<em>Watt’s Steam Engine which made Steam Power Efficient and Practical</em>
</center>
</section>
<section id="adversaries" class="slide level3">
<h3>Adversaries</h3>
<ul>
<li>Stuxnet</li>
<li>Mischevious-Adversarial</li>
</ul>
</section>
<section id="turnaround-and-update" class="slide level3">
<h3>Turnaround And Update</h3>
<ul>
<li>There is a massive need for turn around and update</li>
<li>A redeploy of the entire system.
<ul>
<li>This involves changing the way we design and deploy.</li>
</ul></li>
<li>Interface between security engineering and machine learning.</li>
</ul>
</section>
<section id="peppercorns" class="slide level3">
<h3>Peppercorns</h3>
<ul>
<li>A new name for system failures which aren’t bugs.</li>
<li>Difference between finding a fly in your soup vs a peppercorn in your soup.</li>
</ul>
</section>
<section id="uncertainty-quantification" class="slide level3">
<h3>Uncertainty Quantification</h3>
<ul>
<li><p>Deep nets are powerful approach to images, speech, language.</p></li>
<li><p>Proposal: Deep GPs may also be a great approach, but better to deploy according to natural strengths.</p></li>
</ul>
</section>
<section id="uncertainty-quantification-1" class="slide level3">
<h3>Uncertainty Quantification</h3>
<ul>
<li><p>Probabilistic numerics, surrogate modelling, emulation, and UQ.</p></li>
<li><p>Not a fan of AI as a term.</p></li>
<li><p>But we are faced with increasing amounts of <em>algorithmic decision making</em>.</p></li>
</ul>
</section>
<section id="ml-and-decision-making" class="slide level3">
<h3>ML and Decision Making</h3>
<ul>
<li><p>When trading off decisions: compute or acquire data?</p></li>
<li><p>There is a critical need for uncertainty.</p></li>
</ul>
</section>
<section id="uncertainty-quantification-2" class="slide level3">
<h3>Uncertainty Quantification</h3>
<blockquote>
<p>Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known.</p>
</blockquote>
<ul>
<li>Interaction between physical and virtual worlds of major interest for Amazon.</li>
</ul>
</section>
<section id="example-formula-one-racing" class="slide level3">
<h3>Example: Formula One Racing</h3>
<ul>
<li><p>Designing an F1 Car requires CFD, Wind Tunnel, Track Testing etc.</p></li>
<li><p>How to combine them?</p></li>
</ul>
</section>
<section id="mountain-car-simulator" class="slide level3">
<h3>Mountain Car Simulator</h3>
<p><img class="" src="../slides/diagrams/uq/mountaincar.png" width="negate" align="" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="car-dynamics" class="slide level3">
<h3>Car Dynamics</h3>
<p><span class="math display">\[\inputVector_{t+1} = \mappingFunction(\inputVector_{t},\textbf{u}_{t})\]</span></p>
<p>where <span class="math inline">\(\textbf{u}_t\)</span> is the action force, <span class="math inline">\(\inputVector_t = (p_t, v_t)\)</span> is the vehicle state</p>
</section>
<section id="policy" class="slide level3">
<h3>Policy</h3>
<ul>
<li>Assume policy is linear with parameters <span class="math inline">\(\boldsymbol{\theta}\)</span></li>
</ul>
<p><span class="math display">\[\pi(\inputVector,\theta)= \theta_0 + \theta_p p + \theta_vv.\]</span></p>
</section>
<section id="emulate-the-mountain-car" class="slide level3">
<h3>Emulate the Mountain Car</h3>
<ul>
<li>Goal is find <span class="math inline">\(\theta\)</span> such that</li>
</ul>
<p><span class="math display">\[\theta^* = arg \max_{\theta} R_T(\theta).\]</span></p>
<ul>
<li>Reward is computed as 100 for target, minus squared sum of actions</li>
</ul>
</section>
<section id="random-linear-controller" class="slide level3">
<h3>Random Linear Controller</h3>
<iframe src="../slides/diagrams/uq/mountain_car_random.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="best-controller-after-50-iterations-of-bayesian-optimization" class="slide level3">
<h3>Best Controller after 50 Iterations of Bayesian Optimization</h3>
<iframe src="../slides/diagrams/uq/mountain_car_simulated.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="data-efficient-emulation" class="slide level3">
<h3>Data Efficient Emulation</h3>
<ul>
<li><p>For standard Bayesian Optimization ignored <em>dynamics</em> of the car.</p></li>
<li><p>For more data efficiency, first <em>emulate</em> the dynamics.</p></li>
<li><p>Then do Bayesian optimization of the <em>emulator</em>.</p></li>
<li><p>Use a Gaussian process to model <span class="math display">\[\Delta v_{t+1} = v_{t+1} - v_{t}\]</span> and <span class="math display">\[\Delta x_{t+1} = p_{t+1} - p_{t}\]</span></p></li>
<li><p>Two processes, one with mean <span class="math inline">\(v_{t}\)</span> one with mean <span class="math inline">\(p_{t}\)</span></p></li>
</ul>
</section>
<section id="emulator-training" class="slide level3">
<h3>Emulator Training</h3>
<ul>
<li><p>Used 500 randomly selected points to train emulators.</p></li>
<li><p>Can make proces smore efficient through <em>experimental design</em>.</p></li>
</ul>
<!--
### Emulator Accuracy-->
</section>
<section id="comparison-of-emulation-and-simulation" class="slide level3">
<h3>Comparison of Emulation and Simulation</h3>
<object class="svgplot " align data="../slides/diagrams/uq/emu_sim_comparison.svg">
</object>
</section>
<section id="data-efficiency" class="slide level3">
<h3>Data Efficiency</h3>
<ul>
<li><p>Our emulator used only 500 calls to the simulator.</p></li>
<li><p>Optimizing the simulator directly required 37,500 calls to the simulator.</p></li>
</ul>
</section>
<section id="best-controller-using-emulator-of-dynamics" class="slide level3">
<h3>Best Controller using Emulator of Dynamics</h3>
<iframe src="../slides/diagrams/uq/mountain_car_emulated.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
<p>500 calls to the simulator vs 37,500 calls to the simulator</p>
<p><span class="math display">\[\mappingFunction_i\left(\inputVector\right) = \rho\mappingFunction_{i-1}\left(\inputVector\right) + \delta_i\left(\inputVector \right)\]</span></p>
</section>
<section id="multi-fidelity-emulation" class="slide level3">
<h3>Multi-Fidelity Emulation</h3>
<p><span class="math display">\[\mappingFunction_i\left(\inputVector\right) = \mappingFunctionTwo_{i}\left(\mappingFunction_{i-1}\left(\inputVector\right)\right) + \delta_i\left(\inputVector \right),\]</span></p>
</section>
<section id="best-controller-with-multi-fidelity-emulator" class="slide level3">
<h3>Best Controller with Multi-Fidelity Emulator</h3>
<iframe src="../slides/diagrams/uq/mountain_car_multi_fidelity.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
<p>250 observations of high fidelity simulator and 250 of the low fidelity simulator</p>
</section>
<section id="conclusion" class="slide level3">
<h3>Conclusion</h3>
<ul>
<li><p>Artificial Intelligence and Data Science are fundamentally different.</p></li>
<li><p>In one you are dealing with data collected by happenstance.</p></li>
<li><p>In the other you are trying to build systems in the real world, often by actively collecting data.</p></li>
<li><p>Our approaches to systems design are building powerful machines that will be deployed in evolving environments.</p></li>
</ul>
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: @lawrennd</li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
<li><a href="http://inverseprobability.com/2018/02/06/natural-and-artificial-intelligence">Natural vs Artifical Intelligence</a></li>
<li><a href="https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7">Mike Jordan’s Medium Post</a></li>
</ul>
<div id="refs" class="references">
<div id="ref-Taigman:deepface14">
<p>Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. DeepFace: Closing the gap to human-level performance in face verification, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. <a href="https://doi.org/10.1109/CVPR.2014.220" class="uri">https://doi.org/10.1109/CVPR.2014.220</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
