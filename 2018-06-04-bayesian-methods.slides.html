<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2018-06-04">
  <title>Bayesian Methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
$$\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
$$
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Bayesian Methods</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2018-06-04</time></p>
  <p class="venue" style="text-align:center">Data Science Africa, Nyeri, Kenya</p>
</section>

<section class="slide level3">

<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<div class="fragment">
<ul>
<li>To combine data with a model need:</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>an objective function</strong> <span class="math inline">\(\errorFunction(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</div>
</section>
<section id="olympic-marathon-data" class="slide level3">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td width="70%">
<ul>
<li><p>Gold medal times for Olympic Marathon since 1896.</p></li>
<li><p>Marathons before 1924 didn’t have a standardised distance.</p></li>
<li><p>Present results using pace per km.</p></li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<img src="../slides/diagrams/Stephen_Kiprotich.jpg" alt="image" /> <small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3">
<h3>Olympic Marathon Data</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/olympic-marathon.svg">
</object>
</section>
<section id="regression-linear-releationship" class="slide level3">
<h3>Regression: Linear Releationship</h3>
<p><span class="math display">\[\dataScalar_i = m \inputScalar_i + c\]</span></p>
<ul>
<li><p><span class="math inline">\(\dataScalar_i\)</span> : winning pace.</p></li>
<li><p><span class="math inline">\(\inputScalar_i\)</span> : year of Olympics.</p></li>
<li><p><span class="math inline">\(m\)</span> : rate of improvement over time.</p></li>
<li><p><span class="math inline">\(c\)</span> : winning time at year 0.</p></li>
</ul>
<!-- SECTION Overdetermined System -->
</section>
<section id="section" class="slide level3">
<h3></h3>
<script>
showDivs(1, 'over_determined_system');
</script>
<button onclick="plusDivs(-1, 'over_determined_system')">
❮
</button>
<button onclick="plusDivs(1, 'over_determined_system')">
❯
</button>
<p><input id="range-over_determined_system" type="range" min="1" max="8" value="1" onchange="setDivs('over_determined_system')" oninput="setDivs('over_determined_system')"> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system001.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system002.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system003.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system004.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system005.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system006.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system007.svg"></object></p>
</section>
<section id="datascalar-minputscalar-c" class="slide level3">
<h3><span class="math inline">\(\dataScalar = m\inputScalar + c\)</span></h3>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span></p>
<p><span class="math display">\[2.5 = 2m + c\]</span></p>
</div>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/ml/Pierre-Simon_Laplace.png" width="30%" align="" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/laplacesDeterminismFrench.png" width="80%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/laplacesDeterminismEnglish.png" width="80%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/philosophicaless00lapliala.png" width="80%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="datascalar-minputscalar-c-noisescalar" class="slide level3">
<h3><span class="math inline">\(\dataScalar = m\inputScalar + c + \noiseScalar\)</span></h3>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c + \noiseScalar_1
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c + \noiseScalar_2
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span> <span class="math display">\[
2.5 = 2m + c + \noiseScalar_3
\]</span></p>
</div>
</section>
<section id="a-probabilistic-process" class="slide level3">
<h3>A Probabilistic Process</h3>
<div class="fragment">
<p>Set the mean of Gaussian to be a function. <span class="math display">\[p
\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp \left(-\frac{\left(\dataScalar_i-\mappingFunction\left(\inputScalar_i\right)\right)^{2}}{2\dataStd^2}\right).
\]</span></p>
</div>
<div class="fragment">
<p>This gives us a ‘noisy function’.</p>
</div>
<div class="fragment">
<p>This is known as a stochastic process.</p>
</div>
</section>
<section id="the-gaussian-density" class="slide level3">
<h3>The Gaussian Density</h3>
<ul>
<li>Perhaps the most common probability density.</li>
</ul>
<div class="fragment">
<p><span class="math display">\[\begin{align}
  p(\dataScalar| \meanScalar, \dataStd^2) &amp; = \frac{1}{\sqrt{2\pi\dataStd^2}}\exp\left(-\frac{(\dataScalar - \meanScalar)^2}{2\dataStd^2}\right)\\&amp; \buildrel\triangle\over = \gaussianDist{\dataScalar}{\meanScalar}{\dataStd^2}
  \end{align}\]</span></p>
</div>
</section>
<section id="gaussian-density" class="slide level3">
<h3>Gaussian Density</h3>
<object class="svgplot " align data="../slides/diagrams/ml/gaussian_of_height.svg">
</object>
<center>
<em>The Gaussian PDF with <span class="math inline">\({\meanScalar}=1.7\)</span> and variance <span class="math inline">\({\dataStd}^2=0.0225\)</span>. Mean shown as cyan line. It could represent the heights of a population of students. </em>
</center>
</section>
<section id="gaussian-density-1" class="slide level3">
<h3>Gaussian Density</h3>
<p><large><span class="math display">\[
\gaussianDist{\dataScalar}{\meanScalar}{\dataStd^2} = \frac{1}{\sqrt{2\pi\dataStd^2}} \exp\left(-\frac{(\dataScalar-\meanScalar)^2}{2\dataStd^2}\right)
\]</span></large></p>
<div class="fragment">
<center>
<span class="math inline">\(\dataStd^2\)</span> is the variance of the density and <span class="math inline">\(\meanScalar\)</span> is the mean.
</center>
</div>
</section>
<section id="two-important-gaussian-properties" class="slide level3">
<h3>Two Important Gaussian Properties</h3>
</section>
<section id="sum-of-gaussians" class="slide level3">
<h3>Sum of Gaussians</h3>
<div class="fragment">
<p><span align="left">Sum of Gaussian variables is also Gaussian.</span></p>
<p><span class="math display">\[\dataScalar_i \sim \gaussianSamp{\meanScalar_i}{\sigma_i^2}\]</span></p>
</div>
<div class="fragment">
<p><span align="left">And the sum is distributed as</span></p>
<p><span class="math display">\[\sum_{i=1}^{\numData} \dataScalar_i \sim \gaussianSamp{\sum_{i=1}^\numData \meanScalar_i}{\sum_{i=1}^\numData \sigma_i^2}\]</span></p>
</div>
<div class="fragment">
<p><small>(<em>Aside</em>: As sum increases, sum of non-Gaussian, finite variance variables is also Gaussian because of <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>.)</small></p>
</div>
</section>
<section id="scaling-a-gaussian" class="slide level3">
<h3>Scaling a Gaussian</h3>
<div class="fragment">
<p><span align="left">Scaling a Gaussian leads to a Gaussian.</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\dataScalar \sim \gaussianSamp{\meanScalar}{\sigma^2}\]</span></p>
</div>
<div class="fragment">
<p><span align="left">And the scaled variable is distributed as</span></p>
<p><span class="math display">\[\mappingScalar \dataScalar \sim \gaussianSamp{\mappingScalar\meanScalar}{\mappingScalar^2 \sigma^2}.\]</span></p>
<!-- SECTION Laplace's Idea -->
</div>
</section>
<section id="a-probabilistic-process-1" class="slide level3">
<h3>A Probabilistic Process</h3>
<p>Set the mean of Gaussian to be a function.</p>
<div class="fragment">
<p><span class="math display">\[p\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp\left(-\frac{\left(\dataScalar_i-f\left(\inputScalar_i\right)\right)^{2}}{2\dataStd^2}\right).\]</span></p>
</div>
<div class="fragment">
<p>This gives us a ‘noisy function’.</p>
</div>
<div class="fragment">
<p>This is known as a stochastic process.</p>
</div>
</section>
<section id="height-as-a-function-of-weight" class="slide level3">
<h3>Height as a Function of Weight</h3>
<p>In the standard Gaussian, parametized by mean and variance.</p>
<p>Make the mean a linear function of an <em>input</em>.</p>
<p>This leads to a regression model. <span class="math display">\[
\begin{align*}
  \dataScalar_i=&amp;\mappingFunction\left(\inputScalar_i\right)+\noiseScalar_i,\\
         \noiseScalar_i \sim &amp; \gaussianSamp{0}{\dataStd^2}.
  \end{align*}
\]</span></p>
<p>Assume <span class="math inline">\(\dataScalar_i\)</span> is height and <span class="math inline">\(\inputScalar_i\)</span> is weight.</p>
</section>
<section id="data-point-likelihood" class="slide level3">
<h3>Data Point Likelihood</h3>
<p>Likelihood of an individual data point <span class="math display">\[
p\left(\dataScalar_i|\inputScalar_i,m,c\right)=\frac{1}{\sqrt{2\pi \dataStd^2}}\exp\left(-\frac{\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}}{2\dataStd^2}\right).
\]</span> Parameters are gradient, <span class="math inline">\(m\)</span>, offset, <span class="math inline">\(c\)</span> of the function and noise variance <span class="math inline">\(\dataStd^2\)</span>.</p>
</section>
<section id="data-set-likelihood" class="slide level3">
<h3>Data Set Likelihood</h3>
<p>If the noise, <span class="math inline">\(\epsilon_i\)</span> is sampled independently for each data point. Each data point is independent (given <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>). For <em>independent</em> variables: <span class="math display">\[
p(\dataVector) = \prod_{i=1}^\numData p(\dataScalar_i)
\]</span> <span class="math display">\[
p(\dataVector|\inputVector, m, c) = \prod_{i=1}^\numData p(\dataScalar_i|\inputScalar_i, m, c)
\]</span></p>
</section>
<section id="for-gaussian" class="slide level3">
<h3>For Gaussian</h3>
<p>i.i.d. assumption <span class="math display">\[
p(\dataVector|\inputVector, m, c) = \prod_{i=1}^\numData \frac{1}{\sqrt{2\pi \dataStd^2}}\exp \left(-\frac{\left(\dataScalar_i- m\inputScalar_i-c\right)^{2}}{2\dataStd^2}\right).
\]</span> <span class="math display">\[
p(\dataVector|\inputVector, m, c) = \frac{1}{\left(2\pi \dataStd^2\right)^{\frac{\numData}{2}}}\exp\left(-\frac{\sum_{i=1}^\numData\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}}{2\dataStd^2}\right).
\]</span></p>
</section>
<section id="log-likelihood-function" class="slide level3">
<h3>Log Likelihood Function</h3>
<ul>
<li>Normally work with the log likelihood: <span class="math display">\[
L(m,c,\dataStd^{2})=-\frac{\numData}{2}\log 2\pi -\frac{\numData}{2}\log \dataStd^2 -\sum_{i=1}^{\numData}\frac{\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}}{2\dataStd^2}.
\]</span></li>
</ul>
</section>
<section id="consistency-of-maximum-likelihood" class="slide level3">
<h3>Consistency of Maximum Likelihood</h3>
<ul>
<li>If data was really generated according to probability we specified.</li>
<li>Correct parameters will be recovered in limit as <span class="math inline">\(\numData \rightarrow \infty\)</span>.</li>
<li>This can be proven through sample based approximations (law of large numbers) of “KL divergences”.</li>
<li>Mainstay of classical statistics.</li>
</ul>
</section>
<section id="probabilistic-interpretation-of-the-error-function" class="slide level3">
<h3>Probabilistic Interpretation of the Error Function</h3>
<ul>
<li>Probabilistic Interpretation for Error Function is Negative Log Likelihood.</li>
<li><em>Minimizing</em> error function is equivalent to <em>maximizing</em> log likelihood.</li>
<li>Maximizing <em>log likelihood</em> is equivalent to maximizing the <em>likelihood</em> because <span class="math inline">\(\log\)</span> is monotonic.</li>
<li>Probabilistic interpretation: Minimizing error function is equivalent to maximum likelihood with respect to parameters.</li>
</ul>
</section>
<section id="error-function" class="slide level3">
<h3>Error Function</h3>
<ul>
<li>Negative log likelihood is the error function leading to an error function <span class="math display">\[\errorFunction(m,c,\dataStd^{2})=\frac{\numData}{2}\log \dataStd^2+\frac{1}{2\dataStd^2}\sum _{i=1}^{\numData}\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}.\]</span></li>
<li>Learning proceeds by minimizing this error function for the data set provided.</li>
</ul>
</section>
<section id="connection-sum-of-squares-error" class="slide level3">
<h3>Connection: Sum of Squares Error</h3>
<ul>
<li>Ignoring terms which don’t depend on <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> gives <span class="math display">\[\errorFunction(m, c) \propto \sum_{i=1}^\numData (\dataScalar_i - \mappingFunction(\inputScalar_i))^2\]</span> where <span class="math inline">\(\mappingFunction(\inputScalar_i) = m\inputScalar_i + c\)</span>.</li>
<li>This is known as the <em>sum of squares</em> error function.</li>
<li>Commonly used and is closely associated with the Gaussian likelihood.</li>
</ul>
</section>
<section id="reminder" class="slide level3">
<h3>Reminder</h3>
<ul>
<li>Two functions involved:
<ul>
<li><em>Prediction function</em>: <span class="math inline">\(\mappingFunction(\inputScalar_i)\)</span></li>
<li>Error, or <em>Objective function</em>: <span class="math inline">\(\errorFunction(m, c)\)</span></li>
</ul></li>
<li>Error function depends on parameters through prediction function.</li>
</ul>
</section>
<section id="mathematical-interpretation" class="slide level3">
<h3>Mathematical Interpretation</h3>
<ul>
<li>What is the mathematical interpretation?</li>
<li>There is a cost function.
<ul>
<li>It expresses mismatch between your prediction and reality. <span class="math display">\[
  \errorFunction(m, c)=\sum_{i=1}^\numData \left(\dataScalar_i - m\inputScalar_i-c\right)^2
  \]</span></li>
<li>This is known as the sum of squares error.</li>
</ul></li>
</ul>
<!-- SECTION Sum of Squares Error -->
<!-- SECTION Linear Algebra -->
</section>
<section id="coordinate-descent" class="slide level3">
<h3>Coordinate Descent</h3>
</section>
<section id="learning-is-optimization" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Learning is minimization of the cost function.</li>
<li>At the minima the gradient is zero.</li>
<li>Coordinate ascent, find gradient in each coordinate and set to zero. <span class="math display">\[\frac{\text{d}\errorFunction(c)}{\text{d}c} = -2\sum_{i=1}^\numData \left(\dataScalar_i- m \inputScalar_i - c \right)\]</span> <span class="math display">\[0 = -2\sum_{i=1}^\numData\left(\dataScalar_i- m\inputScalar_i - c \right)\]</span></li>
</ul>
</section>
<section id="learning-is-optimization-1" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Fixed point equations <span class="math display">\[0 = -2\sum_{i=1}^\numData \dataScalar_i +2\sum_{i=1}^\numData m \inputScalar_i +2n c\]</span> <span class="math display">\[c = \frac{\sum_{i=1}^\numData \left(\dataScalar_i - m\inputScalar_i\right)}{\numData}\]</span></li>
</ul>
</section>
<section id="learning-is-optimization-2" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Learning is minimization of the cost function.</li>
<li>At the minima the gradient is zero.</li>
<li>Coordinate ascent, find gradient in each coordinate and set to zero. <span class="math display">\[\frac{\text{d}\errorFunction(m)}{\text{d}m} = -2\sum_{i=1}^\numData \inputScalar_i\left(\dataScalar_i- m \inputScalar_i - c \right)\]</span> <span class="math display">\[0 = -2\sum_{i=1}^\numData \inputScalar_i \left(\dataScalar_i-m \inputScalar_i - c \right)\]</span></li>
</ul>
</section>
<section id="learning-is-optimization-3" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Fixed point equations <span class="math display">\[0 = -2\sum_{i=1}^\numData \inputScalar_i\dataScalar_i+2\sum_{i=1}^\numData m \inputScalar_i^2+2\sum_{i=1}^\numData c\inputScalar_i\]</span> <span class="math display">\[m  =    \frac{\sum_{i=1}^\numData \left(\dataScalar_i -c\right)\inputScalar_i}{\sum_{i=1}^\numData\inputScalar_i^2}\]</span></li>
</ul>
<p><span class="math display">\[m^* = \frac{\sum_{i=1}^\numData (\dataScalar_i - c)\inputScalar_i}{\sum_{i=1}^\numData \inputScalar_i^2}\]</span></p>
</section>
<section id="fixed-point-updates" class="slide level3">
<h3>Fixed Point Updates</h3>
<p><span align="left">Worked example.</span> <span class="math display">\[
\begin{aligned}
    c^{*}=&amp;\frac{\sum
_{i=1}^{\numData}\left(\dataScalar_i-m^{*}\inputScalar_i\right)}{\numData},\\
    m^{*}=&amp;\frac{\sum
_{i=1}^{\numData}\inputScalar_i\left(\dataScalar_i-c^{*}\right)}{\sum _{i=1}^{\numData}\inputScalar_i^{2}},\\
\left.\dataStd^2\right.^{*}=&amp;\frac{\sum
_{i=1}^{\numData}\left(\dataScalar_i-m^{*}\inputScalar_i-c^{*}\right)^{2}}{\numData}
\end{aligned}
\]</span></p>
</section>
<section id="important-concepts-not-covered" class="slide level3">
<h3>Important Concepts Not Covered</h3>
<ul>
<li>Other optimization methods:
<ul>
<li>Second order methods, conjugate gradient, quasi-Newton and Newton.</li>
</ul></li>
<li>Effective heuristics such as momentum.</li>
<li>Local vs global solutions.</li>
</ul>
</section>
<section id="reading" class="slide level3">
<h3>Reading</h3>
<ul>
<li>Section 1.1-1.2 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span> for fitting linear models.</li>
<li>Section 1.2.5 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> up to equation 1.65.</li>
</ul>
</section>
<section id="multi-dimensional-inputs" class="slide level3">
<h3>Multi-dimensional Inputs</h3>
<ul>
<li>Multivariate functions involve more than one input.</li>
<li>Height might be a function of weight and gender.</li>
<li>There could be other contributory factors.</li>
<li>Place these factors in a feature vector <span class="math inline">\(\inputVector_i\)</span>.</li>
<li>Linear function is now defined as <span class="math display">\[\mappingFunction(\inputVector_i) = \sum_{j=1}^p w_j \inputScalar_{i, j} + c\]</span></li>
</ul>
</section>
<section id="vector-notation" class="slide level3">
<h3>Vector Notation</h3>
<ul>
<li>Write in vector notation, <span class="math display">\[\mappingFunction(\inputVector_i) = \mappingVector^\top \inputVector_i + c\]</span></li>
<li>Can absorb <span class="math inline">\(c\)</span> into <span class="math inline">\(\mappingVector\)</span> by assuming extra input <span class="math inline">\(\inputScalar_0\)</span> which is always 1. <span class="math display">\[\mappingFunction(\inputVector_i) = \mappingVector^\top \inputVector_i\]</span></li>
</ul>
</section>
<section id="log-likelihood-for-multivariate-regression" class="slide level3">
<h3>Log Likelihood for Multivariate Regression</h3>
<p>The likelihood of a single data point is</p>
<div class="fragment">
<p><span class="math display">\[p\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp\left(-\frac{\left(\dataScalar_i-\mappingVector^{\top}\inputVector_i\right)^{2}}{2\dataStd^2}\right).\]</span></p>
</div>
<div class="fragment">
<p>Leading to a log likelihood for the data set of</p>
</div>
<div class="fragment">
<p><span class="math display">\[L(\mappingVector,\dataStd^2)= -\frac{\numData}{2}\log \dataStd^2-\frac{\numData}{2}\log 2\pi -\frac{\sum_{i=1}^{\numData}\left(\dataScalar_i-\mappingVector^{\top}\inputVector_i\right)^{2}}{2\dataStd^2}.\]</span></p>
</div>
</section>
<section id="error-function-1" class="slide level3">
<h3>Error Function</h3>
<p>And a corresponding error function of <span class="math display">\[\errorFunction(\mappingVector,\dataStd^2)=\frac{\numData}{2}\log\dataStd^2 + \frac{\sum_{i=1}^{\numData}\left(\dataScalar_i-\mappingVector^{\top}\inputVector_i\right)^{2}}{2\dataStd^2}.\]</span></p>
</section>
<section id="expand-the-brackets" class="slide level3">
<h3>Expand the Brackets</h3>
<p><span class="math display">\[
\begin{align*}
  \errorFunction(\mappingVector,\dataStd^2)  = &amp;
\frac{\numData}{2}\log \dataStd^2 + \frac{1}{2\dataStd^2}\sum
_{i=1}^{\numData}\dataScalar_i^{2}-\frac{1}{\dataStd^2}\sum
_{i=1}^{\numData}\dataScalar_i\mappingVector^{\top}\inputVector_i\\&amp;+\frac{1}{2\dataStd^2}\sum
_{i=1}^{\numData}\mappingVector^{\top}\inputVector_i\inputVector_i^{\top}\mappingVector
+\text{const}.\\
    = &amp; \frac{\numData}{2}\log \dataStd^2 + \frac{1}{2\dataStd^2}\sum
_{i=1}^{\numData}\dataScalar_i^{2}-\frac{1}{\dataStd^2}
\mappingVector^\top\sum_{i=1}^{\numData}\inputVector_i\dataScalar_i\\&amp;+\frac{1}{2\dataStd^2}
\mappingVector^{\top}\left[\sum
_{i=1}^{\numData}\inputVector_i\inputVector_i^{\top}\right]\mappingVector +\text{const}.
\end{align*}
\]</span></p>
<!-- SECTION Multiple Input Solution with Linear Algebra -->
<!-- SECTION Design Matrix -->
<!-- SECTION Objective Optimisation -->
</section>
<section id="multivariate-derivatives" class="slide level3">
<h3>Multivariate Derivatives</h3>
<ul>
<li>We will need some multivariate calculus.</li>
<li>For now some simple multivariate differentiation: <span class="math display">\[\frac{\text{d}{\mathbf{a}^{\top}}{\mappingVector}}{\text{d}\mappingVector}=\mathbf{a}\]</span> and <span class="math display">\[\frac{\mappingVector^{\top}\mathbf{A}\mappingVector}{\text{d}\mappingVector}=\left(\mathbf{A}+\mathbf{A}^{\top}\right)\mappingVector\]</span> or if <span class="math inline">\(\mathbf{A}\)</span> is symmetric (<em>i.e.</em> <span class="math inline">\(\mathbf{A}=\mathbf{A}^{\top}\)</span>) <span class="math display">\[\frac{\text{d}\mappingVector^{\top}\mathbf{A}\mappingVector}{\text{d}\mappingVector}=2\mathbf{A}\mappingVector.\]</span></li>
</ul>
</section>
<section id="differentiate-the-objective" class="slide level3">
<h3>Differentiate the Objective</h3>
<p><span align="left">Differentiating with respect to the vector <span class="math inline">\(\mappingVector\)</span> we obtain</span> <span class="math display">\[
\frac{\partial L\left(\mappingVector,\dataStd^2 \right)}{\partial
\mappingVector}=\frac{1}{\dataStd^2} \sum _{i=1}^{\numData}\inputVector_i \dataScalar_i-\frac{1}{\dataStd^2}
\left[\sum _{i=1}^{\numData}\inputVector_i\inputVector_i^{\top}\right]\mappingVector
\]</span> Leading to <span class="math display">\[
\mappingVector^{*}=\left[\sum
_{i=1}^{\numData}\inputVector_i\inputVector_i^{\top}\right]^{-1}\sum
_{i=1}^{\numData}\inputVector_i\dataScalar_i,
\]</span></p>
</section>
<section id="differentiate-the-objective-1" class="slide level3">
<h3>Differentiate the Objective</h3>
<p>Rewrite in matrix notation: <span class="math display">\[
\sum_{i=1}^{\numData}\inputVector_i\inputVector_i^\top = \inputMatrix^\top \inputMatrix
\]</span> <span class="math display">\[
\sum_{i=1}^{\numData}\inputVector_i\dataScalar_i = \inputMatrix^\top \dataVector
\]</span></p>
<!-- SECTION Update Equation for Global Optimum -->
</section>
<section id="update-equations" class="slide level3">
<h3>Update Equations</h3>
<ul>
<li>Update for <span class="math inline">\(\mappingVector^{*}\)</span>. <span class="math display">\[\mappingVector^{*} = \left(\inputMatrix^\top \inputMatrix\right)^{-1} \inputMatrix^\top \dataVector\]</span></li>
<li>The equation for <span class="math inline">\(\left.\dataStd^2\right.^{*}\)</span> may also be found <span class="math display">\[\left.\dataStd^2\right.^{{*}}=\frac{\sum_{i=1}^{\numData}\left(\dataScalar_i-\left.\mappingVector^{*}\right.^{\top}\inputVector_i\right)^{2}}{\numData}.\]</span></li>
</ul>
</section>
<section id="solving-the-multivariate-system" class="slide level3">
<h3>Solving the Multivariate System</h3>
</section>
<section id="reading-1" class="slide level3">
<h3>Reading</h3>
<ul>
<li>Section 1.3 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span> for Matrix &amp; Vector Review.</li>
</ul>
<!-- SECTION Underdetermined System -->
</section>
<section id="underdetermined-system" class="slide level3">
<h3>Underdetermined System</h3>
<ul>
<li>What about two unknowns and <em>one</em> observation? <span class="math display">\[\dataScalar_1 =  m\inputScalar_1 + c\]</span></li>
</ul>
<div class="fragment">
<p>Can compute <span class="math inline">\(m\)</span> given <span class="math inline">\(c\)</span>. <span class="math display">\[m = \frac{\dataScalar_1 - c}{\inputScalar}\]</span></p>
</div>
</section>
<section id="underdetermined-system-1" class="slide level3">
<h3>Underdetermined System</h3>
<script>
showDivs(1, 'under_determined_system');
</script>
<button onclick="plusDivs(-1, 'under_determined_system')">
❮
</button>
<button onclick="plusDivs(1, 'under_determined_system')">
❯
</button>
<p><input id="range-under_determined_system" type="range" min="1" max="3" value="1" onchange="setDivs('under_determined_system')" oninput="setDivs('under_determined_system')"> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system000.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system001.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system002.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system003.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system004.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system005.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system006.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system007.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system008.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system009.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system010.svg"></object></p>
</section>
<section id="basis-functions" class="slide level3">
<h3>Basis Functions</h3>
</section>
<section id="quadratic-basis" class="slide level3">
<h3>Quadratic Basis</h3>
<ul>
<li>Basis functions can be global. E.g. quadratic basis: <span class="math display">\[
  \basisVector = [1, \inputScalar, \inputScalar^2]
  \]</span></li>
</ul>
<p><span class="math display">\[
\begin{align*}
\basisFunc_1(\inputScalar) = 1, \\
\basisFunc_2(\inputScalar) = x, \\
\basisFunc_3(\inputScalar) = \inputScalar^2.
\end{align*}
\]</span></p>
<p><span class="math display">\[
\basisVector(\inputScalar) = \begin{bmatrix} 1\\ x \\ \inputScalar^2\end{bmatrix}.
\]</span></p>
</section>
<section id="matrix-valued-function" class="slide level3">
<h3>Matrix Valued Function</h3>
<p><span class="math display">\[
\basisMatrix(\inputVector) = 
\begin{bmatrix} 1 &amp; \inputScalar_1 &amp;
\inputScalar_1^2 \\
1 &amp; \inputScalar_2 &amp; \inputScalar_2^2\\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; \inputScalar_n &amp; \inputScalar_n^2
\end{bmatrix}
\]</span></p>
</section>
<section id="functions-derived-from-quadratic-basis" class="slide level3">
<h3>Functions Derived from Quadratic Basis</h3>
<p><span class="math display">\[
\mappingFunction(\inputScalar) = {\color{cyan}\mappingScalar_0}   + {\color{green}\mappingScalar_1 \inputScalar} + {\color{yellow}\mappingScalar_2 \inputScalar^2}
\]</span></p>
<script>
showDivs(0, 'quadratic_basis');
</script>
<button onclick="plusDivs(-1, 'quadratic_basis')">
❮
</button>
<button onclick="plusDivs(1, 'quadratic_basis')">
❯
</button>
<p><input id="range-quadratic_basis" type="range" min="0" max="2" value="0" onchange="setDivs('quadratic_basis')" oninput="setDivs('quadratic_basis')"> <object class="svgplot quadratic_basis" align="" data="../slides/diagrams/ml/quadratic_basis000.svg"></object> <object class="svgplot quadratic_basis" align="" data="../slides/diagrams/ml/quadratic_basis001.svg"></object> <object class="svgplot quadratic_basis" align="" data="../slides/diagrams/ml/quadratic_basis002.svg"></object></p>
</section>
<section id="quadratic-functions" class="slide level3">
<h3>Quadratic Functions</h3>
<script>
showDivs(0, 'quadratic_function');
</script>
<button onclick="plusDivs(-1, 'quadratic_function')">
❮
</button>
<button onclick="plusDivs(1, 'quadratic_function')">
❯
</button>
<p><input id="range-quadratic_function" type="range" min="0" max="2" value="0" onchange="setDivs('quadratic_function')" oninput="setDivs('quadratic_function')"> <object class="svgplot quadratic_function" align="" data="../slides/diagrams/ml/quadratic_function000.svg"></object> <object class="svgplot quadratic_function" align="" data="../slides/diagrams/ml/quadratic_function001.svg"></object> <object class="svgplot quadratic_function" align="" data="../slides/diagrams/ml/quadratic_function002.svg"></object></p>
</section>
<section id="alan-turing" class="slide level3">
<h3>Alan Turing</h3>
<table>
<tr>
<td width="50%">
<img class="" src="../slides/diagrams/turing-times.gif" width="" align="center" style="background:none; border:none; box-shadow:none;">
</td>
<td width="50%">
<img class="" src="../slides/diagrams/turing-run.jpg" width="" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
<center>
<em>Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html">Alan Turing Internet Scrapbook</a>.</em>
</center>
</section>
<section id="probability-winning-olympics" class="slide level3">
<h3>Probability Winning Olympics?</h3>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
</section>
<section id="prior-distribution" class="slide level3">
<h3>Prior Distribution</h3>
<ul>
<li><p>Bayesian inference requires a prior on the parameters.</p></li>
<li><p>The prior represents your belief <em>before</em> you see the data of the likely value of the parameters.</p></li>
<li><p>For linear regression, consider a Gaussian prior on the intercept: <span class="math display">\[c \sim \gaussianSamp{0}{\alpha_1}\]</span></p></li>
</ul>
</section>
<section id="posterior-distribution" class="slide level3">
<h3>Posterior Distribution</h3>
<ul>
<li>Posterior distribution is found by combining the prior with the likelihood.</li>
<li>Posterior distribution is your belief <em>after</em> you see the data of the likely value of the parameters.</li>
<li>The posterior is found through <strong>Bayes’ Rule</strong> <span class="math display">\[
  p(c|\dataScalar) = \frac{p(\dataScalar|c)p(c)}{p(\dataScalar)}
  \]</span></li>
</ul>
</section>
<section id="bayes-update" class="slide level3">
<h3>Bayes Update</h3>
<script>
showDivs(1, 'dem_gaussian');
</script>
<button onclick="plusDivs(-1, 'dem_gaussian')">
❮
</button>
<button onclick="plusDivs(1, 'dem_gaussian')">
❯
</button>
<p><input id="range-dem_gaussian" type="range" min="1" max="3" value="1" onchange="setDivs('dem_gaussian')" oninput="setDivs('dem_gaussian')"> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian001.svg"></object> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian002.svg"></object> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian003.svg"></object></p>
</section>
<section id="stages-to-derivation-of-the-posterior" class="slide level3">
<h3>Stages to Derivation of the Posterior</h3>
<ul>
<li>Multiply likelihood by prior</li>
<li>they are “exponentiated quadratics”, the answer is always also an exponentiated quadratic because <span class="math inline">\(\exp(a^2)\exp(b^2) = \exp(a^2 + b^2)\)</span>.</li>
<li>Complete the square to get the resulting density in the form of a Gaussian.</li>
<li>Recognise the mean and (co)variance of the Gaussian. This is the estimate of the posterior.</li>
</ul>
</section>
<section id="main-trick" class="slide level3">
<h3>Main Trick</h3>
<p><span class="math display">\[p(c) = \frac{1}{\sqrt{2\pi\alpha_1}} \exp\left(-\frac{1}{2\alpha_1}c^2\right)\]</span> <span class="math display">\[p(\dataVector|\inputVector, c, m, \dataStd^2) = \frac{1}{\left(2\pi\dataStd^2\right)^{\frac{\numData}{2}}} \exp\left(-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i - m\inputScalar_i - c)^2\right)\]</span></p>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) = \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{p(\dataVector|\inputVector, m, \dataStd^2)}\]</span></p>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) =  \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{\int p(\dataVector|\inputVector, c, m, \dataStd^2)p(c) \text{d} c}\]</span></p>
</section>
<section id="section-6" class="slide level3">
<h3></h3>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) \propto  p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)\]</span></p>
<p><span class="math display">\[\begin{aligned}
    \log p(c | \dataVector, \inputVector, m, \dataStd^2) =&amp;-\frac{1}{2\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-c - m\inputScalar_i)^2-\frac{1}{2\alpha_1} c^2 + \text{const}\\
     = &amp;-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)^2 -\left(\frac{\numData}{2\dataStd^2} + \frac{1}{2\alpha_1}\right)c^2\\
    &amp; + c\frac{\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)}{\dataStd^2},
  \end{aligned}\]</span></p>
</section>
<section id="section-7" class="slide level3">
<h3></h3>
<p>complete the square of the quadratic form to obtain <span class="math display">\[\log p(c | \dataVector, \inputVector, m, \dataStd^2) = -\frac{1}{2\tau^2}(c - \mu)^2 +\text{const},\]</span> where <span class="math inline">\(\tau^2 = \left(\numData\dataStd^{-2} +\alpha_1^{-1}\right)^{-1}\)</span> and <span class="math inline">\(\mu = \frac{\tau^2}{\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)\)</span>.</p>
</section>
<section id="two-dimensional-gaussian" class="slide level3">
<h3>Two Dimensional Gaussian</h3>
<ul>
<li>Consider height, <span class="math inline">\(h/m\)</span> and weight, <span class="math inline">\(w/kg\)</span>.</li>
<li>Could sample height from a distribution: <span class="math display">\[
  p(h) \sim \gaussianSamp{1.7}{0.0225}
  \]</span></li>
<li>And similarly weight: <span class="math display">\[
  p(w) \sim \gaussianSamp{75}{36}
  \]</span></li>
</ul>
</section>
<section id="height-and-weight-models" class="slide level3">
<h3>Height and Weight Models</h3>
<object class="svgplot " align data="../slides/diagrams/ml/height_weight_gaussian.svg">
</object>
<p>Gaussian distributions for height and weight.</p>
</section>
<section id="sampling-two-dimensional-variables" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>
<script>
showDivs(0, 'independent_height_weight');
</script>
<button onclick="plusDivs(-1, 'independent_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'independent_height_weight')">
❯
</button>
<p><input id="range-independent_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('independent_height_weight')" oninput="setDivs('independent_height_weight')"> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight000.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight001.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight002.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight003.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight004.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight005.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight006.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight007.svg"></object></p>
</section>
<section id="independence-assumption" class="slide level3">
<h3>Independence Assumption</h3>
<ul>
<li><p>This assumes height and weight are independent. <span class="math display">\[p(h, w) = p(h)p(w)\]</span></p></li>
<li><p>In reality they are dependent (body mass index) <span class="math inline">\(= \frac{w}{h^2}\)</span>.</p></li>
</ul>
</section>
<section id="sampling-two-dimensional-variables-1" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>
<script>
showDivs(0, 'correlated_height_weight');
</script>
<button onclick="plusDivs(-1, 'correlated_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'correlated_height_weight')">
❯
</button>
<p><input id="range-correlated_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('correlated_height_weight')" oninput="setDivs('correlated_height_weight')"> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight000.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight001.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight002.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight003.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight004.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight005.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight006.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight007.svg"></object></p>
</section>
<section id="independent-gaussians" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = p(w)p(h)
\]</span></p>
</section>
<section id="independent-gaussians-1" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi \dataStd_1^2}\sqrt{2\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\frac{(w-\meanScalar_1)^2}{\dataStd_1^2} + \frac{(h-\meanScalar_2)^2}{\dataStd_2^2}\right)\right)
\]</span></p>
</section>
<section id="independent-gaussians-2" class="slide level3">
<h3>Independent Gaussians</h3>
<p><small> <span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi\dataStd_1^22\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)^\top\begin{bmatrix}\dataStd_1^2&amp; 0\\0&amp;\dataStd_2^2\end{bmatrix}^{-1}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)\right)
\]</span> </small></p>
</section>
<section id="independent-gaussians-3" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi \mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-1" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)^\top\mathbf{D}^{-1}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-2" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\rotationMatrix\mathbf{D}^{-1}\rotationMatrix^\top(\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix^{-1} = \rotationMatrix \mathbf{D}^{-1} \rotationMatrix^\top
\]</span></p>
</section>
<section id="correlated-gaussian-3" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\covarianceMatrix}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\covarianceMatrix^{-1} (\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix = \rotationMatrix \mathbf{D} \rotationMatrix^\top
\]</span></p>
</section>
<section id="sampling-the-prior" class="slide level3">
<h3>Sampling the Prior</h3>
<ul>
<li>Always useful to perform a ‘sanity check’ and sample from the prior before observing the data.</li>
<li>Since <span class="math inline">\(\dataVector = \basisMatrix \mappingVector + \noiseVector\)</span> just need to sample <span class="math display">\[
  \mappingVector \sim \gaussianSamp{0}{\alpha\eye}
  \]</span> <span class="math display">\[
  \noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2}
  \]</span> with <span class="math inline">\(\alpha=1\)</span> and <span class="math inline">\(\dataStd^2 = 0.01\)</span>.</li>
</ul>
</section>
<section id="computing-the-posterior" class="slide level3">
<h3>Computing the Posterior</h3>
<p><span class="math display">\[
p(\mappingVector | \dataVector, \inputVector, \dataStd^2) = \gaussianDist{\mappingVector}{\meanVector_\mappingScalar}{\covarianceMatrix_\mappingScalar}
\]</span> with <span class="math display">\[
\covarianceMatrix_\mappingScalar = \left(\dataStd^{-2}\basisMatrix^\top \basisMatrix + \alpha^{-1}\eye\right)^{-1}
\]</span> and <span class="math display">\[
\meanVector_\mappingScalar = \covarianceMatrix_\mappingScalar \dataStd^{-2}\basisMatrix^\top \dataVector
\]</span></p>
</section>
<section id="olympic-data-with-bayesian-polynomials" class="slide level3">
<h3>Olympic Data with Bayesian Polynomials</h3>
<script>
showDivs(1, 'olympic_BLM_polynomial_number');
</script>
<button onclick="plusDivs(-1, 'olympic_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_BLM_polynomial_number')">
❯
</button>
<p><input id="range-olympic_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_BLM_polynomial_number')" oninput="setDivs('olympic_BLM_polynomial_number')"> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number001.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number002.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number003.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number004.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number005.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number006.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number007.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number008.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number009.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number010.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number011.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number012.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number013.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number014.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number015.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number016.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number017.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number018.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number019.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number020.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number021.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number022.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number023.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number024.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number025.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number026.svg"></object></p>
</section>
<section id="hold-out-validation" class="slide level3">
<h3>Hold Out Validation</h3>
<script>
showDivs(1, 'olympic_val_BLM_polynomial_number');
</script>
<button onclick="plusDivs(-1, 'olympic_val_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_val_BLM_polynomial_number')">
❯
</button>
<p><input id="range-olympic_val_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_val_BLM_polynomial_number')" oninput="setDivs('olympic_val_BLM_polynomial_number')"> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number001.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number002.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number003.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number004.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number005.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number006.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number007.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number008.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number009.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number010.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number011.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number012.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number013.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number014.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number015.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number016.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number017.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number018.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number019.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number020.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number021.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number022.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number023.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number024.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number025.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number026.svg"></object></p>
</section>
<section id="fold-cross-validation" class="slide level3">
<h3>5-fold Cross Validation</h3>
<script>
showDivs(1, 'olympic_5cv05_BLM_polynomial_number');
</script>
<button onclick="plusDivs(-1, 'olympic_5cv05_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_5cv05_BLM_polynomial_number')">
❯
</button>
<p><input id="range-olympic_5cv05_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_5cv05_BLM_polynomial_number')" oninput="setDivs('olympic_5cv05_BLM_polynomial_number')"> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number001.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number002.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number003.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number004.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number005.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number006.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number007.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number008.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number009.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number010.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number011.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number012.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number013.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number014.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number015.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number016.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number017.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number018.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number019.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number020.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number021.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number022.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number023.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number024.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number025.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number026.svg"></object></p>
</section>
<section id="marginal-likelihood" class="slide level3">
<h3>Marginal Likelihood</h3>
<ul>
<li><p>The marginal likelihood can also be computed, it has the form: <span class="math display">\[
  p(\dataVector|\inputMatrix, \dataStd^2, \alpha) = \frac{1}{(2\pi)^\frac{n}{2}\left|\kernelMatrix\right|^\frac{1}{2}} \exp\left(-\frac{1}{2} \dataVector^\top \kernelMatrix^{-1} \dataVector\right)
  \]</span> where <span class="math inline">\(\kernelMatrix = \alpha \basisMatrix\basisMatrix^\top + \dataStd^2 \eye\)</span>.</p></li>
<li><p>So it is a zero mean <span class="math inline">\(\numData\)</span>-dimensional Gaussian with covariance matrix <span class="math inline">\(\kernelMatrix\)</span>.</p></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bishop:book06">
<p>Bishop, C.M., 2006. Pattern recognition and machine learning. springer.</p>
</div>
<div id="ref-Rogers:book11">
<p>Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
