<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Neil D. Lawrence">
  <title>DRAFT SLIDES: Deep Probabilistic Modelling with with Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
div.sourceLine, a.sourceLine { display: inline-block; min-height: 1.25em; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; }
@media print {
code.sourceCode { white-space: pre-wrap; }
div.sourceLine, a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource div.sourceLine, .numberSource a.sourceLine
  { position: relative; }
pre.numberSource div.sourceLine::before, .numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em; }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; color: #aaaaaa;  padding-left: 4px; }
@media screen {
a.sourceLine::before { text-decoration: underline; color: initial; }
}
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.bn { color: #40a070; } /* BaseN */
code span.fl { color: #40a070; } /* Float */
code span.ch { color: #4070a0; } /* Char */
code span.st { color: #4070a0; } /* String */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.ot { color: #007020; } /* Other */
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.fu { color: #06287e; } /* Function */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code span.cn { color: #880000; } /* Constant */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.ss { color: #bb6688; } /* SpecialString */
code span.im { } /* Import */
code span.va { color: #19177c; } /* Variable */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.op { color: #666666; } /* Operator */
code span.bu { } /* BuiltIn */
code span.ex { } /* Extension */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.at { color: #7d9029; } /* Attribute */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">DRAFT SLIDES: Deep Probabilistic Modelling with with Gaussian Processes</h1>
  <p class="author">Neil D. Lawrence</p>
</section>

<section id="draft-slides" class="slide level3">
<h3>DRAFT SLIDES</h3>

<!-- Introduction to GPs -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \rightarrow \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{data}\)</span> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{model}\)</span> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{prediction}\)</span> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
</section>
<section id="artificial-intelligence" class="slide level3">
<h3>Artificial Intelligence</h3>
<ul>
<li>Machine learning is a mainstay because of importance of prediction.</li>
</ul>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<ul>
<li><p>Simple description</p>
<p><span class="math display">\[\text{data} + \text{model} \rightarrow \text{prediction}\]</span></p></li>
<li><p>To combine data with a model need:</p>
<ul>
<li><p><strong>a prediction function</strong> <span class="math inline">\(f(\cdot)\)</span> includes our beliefs about the regularities of the universe</p></li>
<li><p><strong>an objective function</strong> <span class="math inline">\(E(\cdot)\)</span> defines the cost of misprediction.</p></li>
</ul></li>
</ul>
</section>
<section id="uncertainty" class="slide level3">
<h3>Uncertainty</h3>
<ul>
<li>Uncertainty in prediction arises from:</li>
</ul>
<ol type="1">
<li>scarcity of training data and</li>
<li>mismatch between the set of prediction functions we choose and all possible prediction functions.</li>
</ol>
<ul>
<li>Also uncertainties in objective, leave those for another day.</li>
</ul>
</section>
<section id="neural-networks-and-prediction-functions" class="slide level3">
<h3>Neural Networks and Prediction Functions</h3>
<ul>
<li><p>adaptive non-linear function models inspired by simple neuron models [McCulloch:neuron43]</p></li>
<li><p>have become popular because of their ability to model data.</p></li>
<li><p>can be composed to form highly complex functions</p></li>
<li><p>start by focussing on one hidden layer</p></li>
</ul>
</section>
<section id="prediction-function-of-one-hidden-layer" class="slide level3">
<h3>Prediction Function of One Hidden Layer</h3>
<p><span class="math display">\[
f({\bf x}) = \mathbf{W}^\top \boldsymbol{\phi}(\mathbf{V}, {\bf x})
\]</span></p>
<p><span class="math inline">\(f(\cdot)\)</span> is a scalar function with vector inputs,</p>
<p><span class="math inline">\(\boldsymbol{\phi}(\cdot)\)</span> is a vector function with vector inputs.</p>
<ul>
<li><p>dimensionality of the vector function is known as the number of hidden units, or the number of neurons.</p></li>
<li><p>elements of <span class="math inline">\(\boldsymbol{\phi}(\cdot)\)</span> are the <em>activation</em> function of the neural network</p></li>
<li><p>elements of <span class="math inline">\(\mathbf{V}\)</span> are the parameters of the activation functions.</p></li>
</ul>
</section>
<section id="relations-with-classical-statistics" class="slide level3">
<h3>Relations with Classical Statistics</h3>
<ul>
<li><p>In statistics activation functions are known as <em>basis functions</em>.</p></li>
<li><p>would think of this as a <em>linear model</em>: not linear predictions, linear in the parameters</p></li>
<li><p><span class="math inline">\(\mathbf{V}\)</span> are <em>static</em> parameters.</p></li>
</ul>
</section>
<section id="adaptive-basis-functions" class="slide level3">
<h3>Adaptive Basis Functions</h3>
<ul>
<li><p>In machine learning we optimize <span class="math inline">\(\mathbf{V}\)</span> as well as <span class="math inline">\(\mathbf{W}\)</span> (which would normally be denoted in statistics by <span class="math inline">\(\boldsymbol{\beta}\)</span>).</p></li>
<li><p>This tutorial: revisit that decision: follow the path of Radford Neal [Neal:bayesian94] and David MacKay [MacKay:bayesian92].</p></li>
<li><p>Consider the probabilistic approach.</p></li>
</ul>
</section>
<section id="probabilistic-modelling" class="slide level3" data-transition="None">
<h3>Probabilistic Modelling</h3>
<ul>
<li>Probabilistically we want, <span class="math display">\[
p(y_*|\mathbf{y}, {\bf X}, {\bf x}_*),
\]</span> <span class="math inline">\(y_*\)</span> is a test output <span class="math inline">\({\bf x}_*\)</span> is a test input <span class="math inline">\({\bf X}\)</span> is a training input matrix <span class="math inline">\(\mathbf{y}\)</span> is training outputs</li>
</ul>
</section>
<section id="joint-model-of-world" class="slide level3" data-transition="None">
<h3>Joint Model of World</h3>
<p><span class="math display">\[
p(y_*|\mathbf{y}, {\bf X}, {\bf x}_*) = \int p(y_*|{\bf x}_*, \boldsymbol{\theta}) p(\boldsymbol{\theta}| \mathbf{y}, {\bf X}) \text{d} \boldsymbol{\theta}
\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\boldsymbol{\theta}\)</span> contains <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{V}\)</span></p>
<p><span class="math inline">\(p(\boldsymbol{\theta}| \mathbf{y}, {\bf X})\)</span> is posterior density</p>
</div>
</section>
<section id="likelihood" class="slide level3">
<h3>Likelihood</h3>
<p><span class="math inline">\(p(y|{\bf x}, \boldsymbol{\theta})\)</span> is the <em>likelihood</em> of data point</p>
<div class="fragment">
<p>Normally assume independence: <span class="math display">\[
p(\mathbf{y}|{\bf X}, \boldsymbol{\theta}) \prod_{i=1}^np(y_i|{\bf x}_i, \boldsymbol{\theta}),\]</span></p>
</div>
</section>
<section id="likelihood-and-prediction-function" class="slide level3" data-transition="None">
<h3>Likelihood and Prediction Function</h3>
<p><span class="math display">\[
p(y_i | f({\bf x}_i)) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{\left(y_i - f({\bf x}_i)\right)^2}{2\sigma^2}\right)
\]</span></p>
</section>
<section id="unsupervised-learning" class="slide level3" data-transition="None">
<h3>Unsupervised Learning</h3>
<ul>
<li><p>Can also consider priors over latents <span class="math display">\[
p(\mathbf{y}_*|\mathbf{y}) = \int p(\mathbf{y}_*|{\bf X}_*, \boldsymbol{\theta}) p(\boldsymbol{\theta}| \mathbf{y}, {\bf X}) p({\bf X}) p({\bf X}_*) \text{d} \boldsymbol{\theta}\text{d} {\bf X}\text{d}{\bf X}_*
\]</span></p></li>
<li><p>This gives <em>unsupervised learning</em>.</p></li>
</ul>
</section>
<section id="probabilistic-inference" class="slide level3" data-transition="None">
<h3>Probabilistic Inference</h3>
<ul>
<li><p>Data: <span class="math inline">\(\mathbf{y}\)</span></p></li>
<li><p>Model: <span class="math inline">\(p(\mathbf{y}, \mathbf{y}^*)\)</span></p></li>
<li><p>Prediction: <span class="math inline">\(p(\mathbf{y}^*| \mathbf{y})\)</span></p></li>
</ul>
</section>
<section id="performing-inference" class="slide level3" data-transition="None">
<h3>Performing Inference</h3>
<ul>
<li><p>Easy to write in probabilities</p></li>
<li><p>But underlying this is a wealth of computational challenges.</p></li>
<li><p>High dimensional integrals typically require approximation.</p></li>
</ul>
</section>
<section id="linear-models" class="slide level3" data-transition="None">
<h3>Linear Models</h3>
<ul>
<li><p>In statistics, focussed more on <em>linear</em> model implied by <span class="math display">\[
  f({\bf x}) = \mathbf{W}^\top \boldsymbol{\phi}(\boldsymbol{\theta}, {\bf x})
  \]</span></p></li>
<li><p>Hold <span class="math inline">\(\mathbf{V}\)</span> fixed for given analysis.</p></li>
<li><p>Gaussian prior for <span class="math inline">\(\mathbf{W}\)</span>, <span class="math display">\[
  \mathbf{W}\sim \mathcal{N}\left(\mathbf{0},\mathbf{C}\right).
  \]</span> <span class="math display">\[
  y_i = f({\bf x}_i) + \epsilon_i,
  \]</span> where <span class="math display">\[
  \epsilon_i \sim \mathcal{N}\left(0,\sigma^2\right)
  \]</span></p></li>
</ul>
</section>
<section id="linear-gaussian-models" class="slide level3" data-transition="None">
<h3>Linear Gaussian Models</h3>
<ul>
<li>Normally integrals are complex but for this Gaussian linear case they are trivial.</li>
</ul>
</section>
<section id="linear-gaussian-models-1" class="slide level3" data-transition="None">
<h3>Linear Gaussian Models</h3>
<ol type="1">
<li>linear Gaussian models are easier to deal with</li>
<li>Even the parameters <em>within</em> the process can be handled, by considering a particular limit.</li>
</ol>
</section>
<section id="multivariate-gaussian-properties" class="slide level3" data-transition="None">
<h3>Multivariate Gaussian Properties</h3>
<ul>
<li>If <span class="math display">\[
\mathbf{y}= \mathbf{W}{\bf x}+ \boldsymbol{\epsilon},
\]</span></li>
<li>Assume <span class="math display">\[\begin{align}
{\bf x}&amp; \sim \mathcal{N}\left(\boldsymbol{\mu},\mathbf{C}\right)\\
\boldsymbol{\epsilon}&amp; \sim \mathcal{N}\left(\mathbf{0},\boldsymbol{\Sigma}\right)
\end{align}\]</span></li>
<li>Then <span class="math display">\[
\mathbf{y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{\Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{\Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic Principal Component Analysis [Tipping:probpca99], because we integrated out the inputs (or <em>latent</em> variables they would be called in that case).</li>
</ul>
</section>
<section id="non-linear-on-inputs" class="slide level3" data-transition="None">
<h3>Non linear on Inputs</h3>
<ul>
<li>Set each activation function computed at each data point to be <span class="math display">\[
\phi_{i,j} = \phi(\mathbf{v}_{j}, {\bf x}_{i})
\]</span> Define <em>design matrix</em> <span class="math display">\[
\boldsymbol{\Phi}= 
\begin{bmatrix}
\phi_{1, 1} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, h} \\
\phi_{1, 2} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi_{n, 1} &amp; \phi_{n, 2} &amp; \dots &amp; \phi_{n, h}
\end{bmatrix}.
\]</span></li>
</ul>
</section>
<section id="prior-density" class="slide level3" data-transition="None">
<h3>Prior Density</h3>
<ul>
<li><p>Define <span class="math display">\[
\mathbf{w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right),
\]</span></p></li>
<li><p>Rules of multivariate Gaussians to see that, <span class="math display">\[
\mathbf{y}\sim \mathcal{N}\left(\mathbf{0},\alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top + \sigma^2 \mathbf{I}\right).
\]</span></p></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top + \sigma^2 \mathbf{I}.
\]</span></p>
</section>
<section id="joint-gaussian-density" class="slide level3" data-transition="None">
<h3>Joint Gaussian Density</h3>
<ul>
<li>Elements are a function <span class="math inline">\(k_{i,j} = k\left({\bf x}_i, {\bf x}_j\right)\)</span></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top + \sigma^2 \mathbf{I}.
\]</span></p>
</section>
<section id="covariance-function" class="slide level3" data-transition="None">
<h3>Covariance Function</h3>
<p><span class="math display">\[
k_f\left({\bf x}_i, {\bf x}_j\right) = \alpha \boldsymbol{\phi}\left(\mathbf{V}, {\bf x}_i\right)^\top \boldsymbol{\phi}\left(\mathbf{V}, {\bf x}_j\right)
\]</span></p>
<ul>
<li>formed by inner products of the rows of the <em>design matrix</em>.</li>
</ul>
</section>
<section id="gaussian-process" class="slide level3" data-transition="None">
<h3>Gaussian Process</h3>
<ul>
<li><p>Instead of making assumptions about our density over each data point, <span class="math inline">\(y_i\)</span> as i.i.d.</p></li>
<li><p>make a joint Gaussian assumption over our data.</p></li>
<li><p>covariance matrix is now a function of both the parameters of the activation function, <span class="math inline">\(\mathbf{V}\)</span>, and the input variables, <span class="math inline">\({\bf X}\)</span>.</p></li>
<li><p>Arises from integrating out, <span class="math inline">\(\mathbf{w}\)</span>.</p></li>
</ul>
</section>
<section id="basis-functions" class="slide level3" data-transition="None">
<h3>Basis Functions</h3>
<ul>
<li><p>Can be very complex, such as deep kernels, [Cho:deep09] or could even put a convolutional neural network inside.</p></li>
<li><p>Viewing a neural network in this way is also what allows us to beform sensible <em>batch</em> normalizations [Ioffe:batch15].</p></li>
</ul>
</section>
<section id="non-degenerate-gaussian-processes" class="slide level3" data-transition="None">
<h3>Non-degenerate Gaussian Processes</h3>
<ul>
<li><p>This process is <em>degenerate</em>.</p></li>
<li><p>Covariance function is of rank at most <span class="math inline">\(h\)</span>.</p></li>
<li><p>As <span class="math inline">\(n\rightarrow \infty\)</span>, covariance matrix is not full rank.</p></li>
<li><p>Leading to <span class="math inline">\(\left|\mathbf{K}\right| = 0\)</span></p></li>
</ul>
</section>
<section id="infinite-networks" class="slide level3" data-transition="None">
<h3>Infinite Networks</h3>
<ul>
<li>In ML Radford Neal [Neal:bayesian94] asked “what would happen if you took <span class="math inline">\(h\rightarrow \infty\)</span>?”</li>
</ul>
<p><a href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf"><img src="./diagrams/neal-infinite-priors.png" width="80%" class="negate"  style="background:none; border:none; box-shadow:none;" align="center"></a></p>
<p><em>Page 37 of Radford Neal’s 1994 thesis</em></p>
</section>
<section id="roughly-speaking" class="slide level3" data-transition="None">
<h3>Roughly Speaking</h3>
<ul>
<li><p>Instead of <span class="math display">\[\begin{align*}
k_f\left({\bf x}_i, {\bf x}_j\right) &amp; = \alpha \boldsymbol{\phi}\left(\mathbf{V}, {\bf x}_i\right)^\top \boldsymbol{\phi}\left(\mathbf{V}, {\bf x}_j\right)\\
&amp; = \sum_k \phi\left(\mathbf{v}_k, {\bf x}_i\right) \phi\left(\mathbf{v}_k, {\bf x}_j\right)
\end{align*}\]</span></p></li>
<li><p>Sample infinitely many from a prior density, <span class="math inline">\(p(\mathbf{v})\)</span>, <span class="math display">\[
k_f\left({\bf x}_i, {\bf x}_j\right) = \int \phi\left(\mathbf{v}, {\bf x}_i\right) \phi\left(\mathbf{v}, {\bf x}_j\right) p(\mathbf{v}) \text{d}\mathbf{v}
\]</span></p></li>
<li><p>Also applies for non-Gaussian <span class="math inline">\(p(\mathbf{v})\)</span> because of the <em>central limit theorem</em>.</p></li>
</ul>
</section>
<section id="simple-probabilistic-program" class="slide level3" data-transition="None">
<h3>Simple Probabilistic Program</h3>
<ul>
<li><p>If <span class="math display">\[\begin{align*}
\mathbf{v}&amp; \sim p(\cdot)\\
\phi_i &amp; = \phi\left(\mathbf{v}, {\bf x}_i\right), 
\end{align*}\]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a Gaussian process.</p></li>
</ul>
</section>
<section id="further-reading" class="slide level3">
<h3>Further Reading</h3>
<ul>
<li><p>Chapter 2 of Neal’s thesis</p></li>
<li><p>Rest of Neal’s thesis.</p></li>
<li><p>David MacKay’s PhD thesis [MacKay:bayesian92]</p></li>
</ul>
</section>
<section id="section" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_few.svg">
</object>
</section>
<section id="section-1" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples.svg">
</object>
</section>
<section id="section-2" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-3" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_rejection_samples.svg">
</object>
</section>
<section id="key-object" class="slide level3" data-transition="none">
<h3>Key Object</h3>
<ul>
<li><p>Covariance function, <span class="math inline">\(\mathbf{K}\)</span></p></li>
<li><p>Determines properties of samples.</p></li>
<li><p>Function of <span class="math inline">\({\bf X}\)</span>, <span class="math display">\[k_{i,j} = k({\bf x}_i, {\bf x}_j)\]</span></p></li>
</ul>
</section>
<section id="linear-algebra" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D({\bf x}_*) = \mathbf{k}({\bf x}_*, {\bf X}) \mathbf{K}^{-1}
\mathbf{y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{f}, *}\]</span></p></li>
</ul>
</section>
<section id="linear-algebra-1" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D({\bf x}_*) = \mathbf{k}({\bf x}_*, {\bf X}) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{f}, *}\]</span></p></li>
</ul>
</section>
<section id="section-4" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-5" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_rejection_samples.svg">
</object>
</section>
<section id="section-6" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prediction.svg">
</object>
</section>
<section id="approximate-gaussian-processes" class="slide level3">
<h3>Approximate Gaussian Processes</h3>
</section>
<section id="low-rank-motivation" class="slide level3" data-transition="None">
<h3>Low Rank Motivation</h3>
<ul>
<li><p>Inference in a GP has the following demands:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(n^3)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(n^2)\)</span></td>
</tr>
</tbody>
</table></li>
<li><p>Inference in a low rank GP has the following demands:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(nm^2)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\mathcal{O}(nm)\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(m\)</span> is a user chosen parameter.</p></li>
</ul>
<p>[Snelson:pseudo05,Quinonero:unifying05,Lawrence:larger07,Titsias:variational09]</p>
</section>
<section id="variational-compression" class="slide level3" data-transition="None">
<h3>Variational Compression</h3>
<ul>
<li><p>Inducing variables are a compression of the real observations.</p></li>
<li><p>They are like pseudo-data. They can be in space of <span class="math inline">\(\mathbf{f}\)</span> or a space that is related through a linear operator [Alvarez:efficient10] — e.g. a gradient or convolution.</p></li>
</ul>
</section>
<section id="variational-compression-ii" class="slide level3">
<h3>Variational Compression II</h3>
<ul>
<li><p>Introduce <em>inducing</em> variables.</p></li>
<li><p>Compress information into the inducing variables and avoid the need to store all the data.</p></li>
<li><p>Allow for scaling e.g. stochastic variational Hensman:bigdata13 or parallelization Gal:Distributed14,Dai:gpu14, Seeger:auto17</p></li>
</ul>
</section>
<section id="nonparametric-gaussian-processes" class="slide level3" data-transition="None">
<h3>Nonparametric Gaussian Processes</h3>
<ul>
<li><p>We’ve seen how we go from parametric to non-parametric.</p></li>
<li><p>The limit implies infinite dimensional <span class="math inline">\(\mathbf{w}\)</span>.</p></li>
<li><p>Gaussian processes are generally non-parametric: combine data with covariance function to get model.</p></li>
<li><p>This representation <em>cannot</em> be summarized by a parameter vector of a fixed size.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck" class="slide level3" data-transition="None">
<h3>The Parametric Bottleneck</h3>
<ul>
<li><p>Parametric models have a representation that does not respond to increasing training set size.</p></li>
<li><p>Bayesian posterior distributions over parameters contain the information about the training data.</p>
<ul>
<li><p>Use Bayes’ rule from training data, <span class="math inline">\(p\left(\mathbf{w}|\mathbf{y}, {\bf X}\right)\)</span>,</p></li>
<li><p>Make predictions on test data <span class="math display">\[p\left(y_*|{\bf X}_*, \mathbf{y}, {\bf X}\right) = \int
      p\left(y_*|\mathbf{w},{\bf X}_*\right)p\left(\mathbf{w}|\mathbf{y},
        {\bf X})\text{d}\mathbf{w}\right).\]</span></p></li>
</ul></li>
<li><p><span class="math inline">\(\mathbf{w}\)</span> becomes a bottleneck for information about the training set to pass to the test set.</p></li>
<li><p>Solution: increase <span class="math inline">\(m\)</span> so that the bottleneck is so large that it no longer presents a problem.</p></li>
<li><p>How big is big enough for <span class="math inline">\(m\)</span>? Non-parametrics says <span class="math inline">\(m\rightarrow \infty\)</span>.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck" class="slide level3" data-transition="None">
<h3>The Parametric Bottleneck</h3>
<ul>
<li>Now no longer possible to manipulate the model through the standard parametric form.</li>
</ul>
<div class="fragment">
<ul>
<li>However, it <em>is</em> possible to express <em>parametric</em> as GPs: <span class="math display">\[k\left({\bf x}_i,{\bf x}_j\right)=\phi_:\left({\bf x}_i\right)^\top\phi_:\left({\bf x}_j\right).\]</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>These are known as degenerate covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Their rank is at most <span class="math inline">\(m\)</span>, non-parametric models have full rank covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Most well known is the “linear kernel”, <span class="math inline">\(k({\bf x}_i, {\bf x}_j) = {\bf x}_i^\top{\bf x}_j\)</span>.</li>
</ul>
</div>
</section>
<section id="making-predictions" class="slide level3" data-transition="None">
<h3>Making Predictions</h3>
<ul>
<li>For non-parametrics prediction at new points <span class="math inline">\(\mathbf{f}_*\)</span> is made by conditioning on <span class="math inline">\(\mathbf{f}\)</span> in the joint distribution.</li>
</ul>
<div class="fragment">
<ul>
<li>In GPs this involves combining the training data with the covariance function and the mean function.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Parametric is a special case when conditional prediction can be summarized in a <em>fixed</em> number of parameters.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Complexity of parametric model remains fixed regardless of the size of our training data set.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>For a non-parametric model the required number of parameters grows with the size of the training data.</li>
</ul>
</div>
</section>
<section id="information-capture" class="slide level3" data-transition="None">
<h3>Information capture</h3>
<ul>
<li><p>Everything we want to do with a GP involves marginalising <span class="math inline">\(\mathbf{f}\)</span></p>
<ul>
<li><p>Predictions</p></li>
<li><p>Marginal likelihood</p></li>
<li><p>Estimating covariance parameters</p></li>
</ul></li>
<li><p>The posterior of <span class="math inline">\(\mathbf{f}\)</span> is the central object. This means inverting <span class="math inline">\(\mathbf{K}_{\mathbf{f}\mathbf{f}}\)</span>.</p></li>
</ul>
</section>
<section id="computational-savings" class="slide level3" data-transition="None">
<h3>Computational Savings</h3>
<p><img src="./diagrams/cov_approx.png" width="90.00000%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;" ></p>
<p><span class="math display">\[\mathbf{K}_{\mathbf{f}\mathbf{f}}\approx {\bf Q}_{\mathbf{f}\mathbf{f}}= \mathbf{K}_{\mathbf{f}\mathbf{u}}\mathbf{K}_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{K}_{\mathbf{u}\mathbf{f}}\]</span></p>
<p>Instead of inverting <span class="math inline">\(\mathbf{K}_{\mathbf{f}\mathbf{f}}\)</span>, we make a low rank (or Nyström) approximation, and invert <span class="math inline">\(\mathbf{K}_{\mathbf{u}\mathbf{u}}\)</span> instead.</p>
<p><em>Figure originally from presentation by Ed Snelson at NIPS</em></p>
<p>Smola:sparsegp00,Csato:sparse00,Csato:sparse02,Csato:thesis02,Seeger:fast03</p>
</section>
<section id="section-7" class="slide level3" data-transition="None">
<h3></h3>
<table>
<tr>
<td>
<span class="math display">\[{\bf X},\,\mathbf{y}\]</span>
</td>
<td>
<img src="./diagrams/nomenclature1.png" width="90%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="section-8" class="slide level3" data-transition="None">
<h3></h3>
<table>
<tr>
<td>
<span class="math display">\[{\bf X},\,\mathbf{y}\]</span> <span class="math display">\[{\color{\blueColor} f({\bf x})} \sim {\mathcal GP}\]</span>
</td>
<td>
<img src="./diagrams/nomenclature2.png" width="90%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
<table>
</section>
<section id="section-9" class="slide level3" data-transition="None">
<h3></h3>
<table>
<tr>
<td>
<span class="math display">\[{\bf X},\,\mathbf{y}\]</span> <span class="math display">\[f({\bf x}) \sim {\mathcal GP}\]</span> <span class="math display">\[p({\color{\blueColor} \mathbf{f}}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{f}\mathbf{f}}\right)\]</span>
</td>
<td>
<img src="./diagrams/nomenclature3.png" width="90%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="section-10" class="slide level3" data-transition="None">
<h3></h3>
<table>
<tr>
<td>
<span class="math display">\[{\bf X},\,\mathbf{y}\]</span> <span class="math display">\[f({\bf x}) \sim {\mathcal GP}\]</span> <span class="math display">\[p(\mathbf{f}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{f}\mathbf{f}}\right)\]</span> <span class="math display">\[p( \mathbf{f}|\mathbf{y},{\bf X})\]</span>
</td>
<td>
<img src="./diagrams/nomenclature3a.png" width="90%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="introducing-mathbfu" class="slide level3" data-transition="None">
<h3>Introducing <span class="math inline">\(\mathbf{u}\)</span></h3>
<p>Take an extra <span class="math inline">\(m\)</span> points on the function, <span class="math inline">\(\mathbf{u}= f(\mathbf{Z})\)</span>. <span class="math display">\[p(\mathbf{y},\mathbf{f},\mathbf{u}) = p(\mathbf{y}|\mathbf{f}) p(\mathbf{f}|\mathbf{u}) p(\mathbf{u})\]</span></p>
</section>
<section id="introducing-inducingvector" class="slide level3" data-transition="None">
<h3>Introducing <span class="math inline">\(\mathbf{u}\)</span></h3>
<p><img src="./diagrams/cov_inducing_withX.png" height="60%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="introducing-inducingvector" class="slide level3" data-transition="None">
<h3>Introducing <span class="math inline">\(\mathbf{u}\)</span></h3>
<p>Take and extra <span class="math inline">\(M\)</span> points on the function, <span class="math inline">\(\mathbf{u}= f(\mathbf{Z})\)</span>. <span class="math display">\[p(\mathbf{y},\mathbf{f},\mathbf{u}) = p(\mathbf{y}|\mathbf{f}) p(\mathbf{f}|\mathbf{u}) p(\mathbf{u})\]</span> <span class="math display">\[\begin{aligned}
    p(\mathbf{y}|\mathbf{f}) &amp;= \mathcal{N}\left(\mathbf{y}|\mathbf{f},\sigma^2 \mathbf{I}\right)\\
    p(\mathbf{f}|\mathbf{u}) &amp;= \mathcal{N}\left(\mathbf{f}| \mathbf{K}_{\mathbf{f}\mathbf{u}}\mathbf{K}_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{u}, \tilde{\mathbf{K}}\right)\\
    p(\mathbf{u}) &amp;= \mathcal{N}\left(\mathbf{u}| \mathbf{0},\mathbf{K}_{\mathbf{u}\mathbf{u}}\right)
  \end{aligned}\]</span></p>
</section>
<section id="section-11" class="slide level3" data-transition="None">
<h3></h3>
<table>
<tr>
<td>
<span class="math display">\[{\bf X},\,\mathbf{y}\]</span> <span class="math display">\[f({\bf x}) \sim {\mathcal GP}\]</span> <span class="math display">\[p(\mathbf{f}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{f}\mathbf{f}}\right)\]</span> <span class="math display">\[p(\mathbf{f}|\mathbf{y},{\bf X})\]</span>
</td>
<td>
<img src="./diagrams/nomenclature4" width="90%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
<span class="math display">\[\begin{align}
                                                                                                           &amp;\qquad\inducingInputMatrix, \inducingVector\\                                                                                                                                                                                                    
                                                                                               &amp;p({\color{red} \inducingVector})  = \gaussianSamp{\zerosVector}{\Kuu}                                                                                                                                                                                        
                                                                                                                          \end{align}\]</span>
<p>$$</p>
</section>
<section id="section-12" class="slide level3" data-transition="None">
<h3></h3>
<table>
<tr>
<td>
<span class="math display">\[{\bf X},\,\mathbf{y}\]</span> <span class="math display">\[f({\bf x}) \sim {\mathcal GP}\]</span> <span class="math display">\[p(\mathbf{f}) = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{f}\mathbf{f}}\right)\]</span> <span class="math display">\[p(\mathbf{f}|\mathbf{y},{\bf X})\]</span> <span class="math display">\[p(\mathbf{u})  = \mathcal{N}\left(\mathbf{0},\mathbf{K}_{\mathbf{u}\mathbf{u}}\right)\]</span> <span class="math display">\[\widetilde p({\color{red}\mathbf{u}}|\mathbf{y},{\bf X})\]</span>
</td>
<td>
<img src="./diagrams/nomenclature5.png" width="90%"  class="negate" align="cener" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="the-alternative-posterior" class="slide level3">
<h3>The alternative posterior</h3>
<p><span>Instead of doing</span> <span class="math display">\[p(\mathbf{f}|\mathbf{y},{\bf X}) = \frac{p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|{\bf X})}{\int p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|{\bf X}){\text{d}\mathbf{f}}}\]</span> <span>We’ll do</span> <span class="math display">\[p(\mathbf{u}|\mathbf{y},\mathbf{Z}) = \frac{p(\mathbf{y}|\mathbf{u})p(\mathbf{u}|\mathbf{Z})}{\int p(\mathbf{y}|\mathbf{u})p(\mathbf{u}|\mathbf{Z}){\text{d}\mathbf{u}}}\]</span>  </p>
<!--Flexible Parametric Approximation-->
<!--frame start-->
</section>
<section id="parametric-but-non-parametric" class="slide level3">
<h3>Parametric but Non-parametric</h3>
<ul>
<li><p>Augment with a vector of <em>inducing</em> variables, <span class="math inline">\(\mathbf{u}\)</span>.</p></li>
<li><p>Form a variational lower bound on true likelihood.</p></li>
<li><p>Bound <em>factorizes</em> given inducing variables.</p></li>
<li><p>Inducing variables appear in bound similar to parameters in a parametric model.</p></li>
<li><p><em>But</em> number of inducing variables can be changed at run time.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="inducing-variable-approximations" class="slide level3">
<h3>Inducing Variable Approximations</h3>
<ul>
<li><p>Date back to <span></span>. See <span></span> for a review.</p></li>
<li><p>We follow variational perspective of <span></span>.</p></li>
<li><p>This is an augmented variable method, followed by a collapsed variational approximation <span></span>.</p></li>
</ul>
<!--frame end-->
<!--frame failure start-->


<!--frame failure end-->
<!--frame start-->
</section>
<section id="variational-bound-on-pmathbfymathbfu" class="slide level3">
<h3>Variational Bound on <span class="math inline">\(p(\mathbf{y}|\mathbf{u})\)</span></h3>
<p><span class="math display">\[\begin{aligned}
    \log p(\mathbf{y}|\mathbf{u}) &amp; = \log \int p(\mathbf{y}|\mathbf{f}) p(\mathbf{f}|\mathbf{u}) \text{d}\mathbf{f}\\ &amp; = \int q(\mathbf{f}) \log \frac{p(\mathbf{y}|\mathbf{f}) p(\mathbf{f}|\mathbf{u})}{q(\mathbf{f})}\text{d}\mathbf{f}+ \text{KL}\left( q(\mathbf{f})\,\|\,p(\mathbf{f}|\mathbf{y}, \mathbf{u}) \right)
  \end{aligned}\]</span></p>

<ul>
<li>Example, set <span class="math inline">\(q(\mathbf{f})=p(\mathbf{f}|\mathbf{u})\)</span>, <span class="math display">\[\log p(\mathbf{y}|\mathbf{u}) \geq \log \int p(\mathbf{f}|\mathbf{u}) \log p(\mathbf{y}|\mathbf{f})\text{d}\mathbf{f}.\]</span> <span class="math display">\[p(\mathbf{y}|\mathbf{u}) \geq \exp \int p(\mathbf{f}|\mathbf{u}) \log p(\mathbf{y}|\mathbf{f})\text{d}\mathbf{f}.\]</span></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="optimal-compression-in-inducing-variables" class="slide level3">
<h3>Optimal Compression in Inducing Variables</h3>
<ul>
<li><p>Maximizing lower bound minimizes the KL divergence (information gain): <span class="math display">\[\text{KL}\left( p(\mathbf{f}|\mathbf{u})\,\|\,p(\mathbf{f}|\mathbf{y}, \mathbf{u}) \right) = \int p(\mathbf{f}|\mathbf{u}) \log \frac{p(\mathbf{f}|\mathbf{u})}{p(\mathbf{f}|\mathbf{y}, \mathbf{u})}\text{d}\mathbf{u}\]</span></p></li>
<li><p>This is minimized when the information stored about <span class="math inline">\(\mathbf{y}\)</span> is stored already in <span class="math inline">\(\mathbf{u}\)</span>.</p></li>
<li><p>The bound seeks an <em>optimal compression</em> from the <em>information gain</em> perspective.</p></li>
<li><p>If <span class="math inline">\(\mathbf{u}= \mathbf{f}\)</span> bound is exact (<span class="math inline">\(\mathbf{f}\)</span> <span class="math inline">\(d\)</span>-separates <span class="math inline">\(\mathbf{y}\)</span> from <span class="math inline">\(\mathbf{u}\)</span>).</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="choice-of-inducing-variables" class="slide level3">
<h3>Choice of Inducing Variables</h3>
<ul>
<li><p>Optimizing the bound directly not always practical.</p></li>
<li><p>Free to choose whatever heuristics for the inducing variables.</p></li>
<li><p>Can quantify which heuristics perform better through checking lower bound.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="factorizing-likelihoods" class="slide level3">
<h3>Factorizing Likelihoods</h3>
<ul>
<li><p>If the likelihood, <span class="math inline">\(p(\mathbf{y}|\mathbf{f})\)</span>, factorizes     </p></li>
<li><p>&lt;8-&gt; Then the bound factorizes.</p></li>
<li><p>&lt;10-&gt; Now need a choice of distributions for <span class="math inline">\(\mathbf{f}\)</span> and <span class="math inline">\(\mathbf{y}|\mathbf{f}\)</span> …</p></li>
</ul>
<!--frame end-->
</section>
<section id="section-13" class="slide level3">
<h3></h3>
<p><span class="math display">\[\mathbf{f}, \mathbf{u}\sim \mathcal{N}\left(\mathbf{0},\begin{bmatrix}\mathbf{K}_{\mathbf{f}\mathbf{f}}&amp; \mathbf{K}_{\mathbf{f}\mathbf{u}}\\\mathbf{K}_{\mathbf{u}\mathbf{f}}&amp; \mathbf{K}_{\mathbf{u}\mathbf{u}}\end{bmatrix}\right)\]</span> <span class="math display">\[\mathbf{y}|\mathbf{f}= \prod_{i} \mathcal{N}\left(f,\sigma^2\right)\]</span></p>
<!--Variational Compression-->
</section>
<section id="gaussian-py_if_i" class="slide level3">
<h3>Gaussian <span class="math inline">\(p(y_i|f_i)\)</span></h3>
<p>For Gaussian likelihoods:  </p>
</section>
<section id="gaussian-process-over-mathbff-and-mathbfu" class="slide level3">
<h3>Gaussian Process Over <span class="math inline">\(\mathbf{f}\)</span> and <span class="math inline">\(\mathbf{u}\)</span></h3>
<p>Define: <span class="math display">\[q_{i, i} = \text{var}_{p(f_i|\mathbf{u})}\left( f_i \right) = \left&lt;f_i^2\right&gt;_{p(f_i|\mathbf{u})} - \left&lt;f_i\right&gt;_{p(f_i|\mathbf{u})}^2\]</span> We can write: <span class="math display">\[c_i = \exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)\]</span> If joint distribution of <span class="math inline">\(p(\mathbf{f}, \mathbf{u})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} = k_{i, i} - \mathbf{k}_{i, \mathbf{u}}^\top \mathbf{K}_{\mathbf{u}, \mathbf{u}}^{-1} \mathbf{k}_{i, \mathbf{u}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span class="math inline">\(\mathbf{u}\)</span> but <em>is</em> a function of <span class="math inline">\({\bf X}_\mathbf{u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="total-conditional-variance" class="slide level3">
<h3>Total Conditional Variance</h3>
<ul>
<li><p>The sum of <span class="math inline">\(q_{i,i}\)</span> is the <em>total conditional variance</em>.</p></li>
<li><p>If conditional density <span class="math inline">\(p(\mathbf{f}|\mathbf{u})\)</span> is Gaussian then it has covariance <span class="math display">\[\mathbf{Q} = \mathbf{K}_{\mathbf{f}\mathbf{f}} - \mathbf{K}_{\mathbf{f}\mathbf{u}}\mathbf{K}_{\mathbf{u}\mathbf{u}}^{-1} \mathbf{K}_{\mathbf{u}\mathbf{f}}\]</span></p></li>
<li><p><span class="math inline">\(\text{tr}\left(\mathbf{Q}\right) = \sum_{i}q_{i,i}\)</span> is known as total variance.</p></li>
<li><p>Because it is on conditional distribution we call it <em>total conditional variance</em>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="capacity-of-a-density" class="slide level3">
<h3>Capacity of a Density</h3>
<ul>
<li><p>Measure the ’capacity of a density’.</p></li>
<li><p>Determinant of covariance represents ’volume’ of density.</p></li>
<li><p>log determinant is entropy: sum of <em>log</em> eigenvalues of covariance.</p></li>
<li><p>trace of covariance is total variance: sum of eigenvalues of covariance.</p></li>
<li><p><span class="math inline">\(\lambda &gt; \log \lambda\)</span> then total conditional variance upper bounds entropy.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="alternative-view" class="slide level3">
<h3>Alternative View</h3>
<p>Exponentiated total variance bounds determinant. <span class="math display">\[\left|\mathbf{Q}\right| &lt; \exp \text{tr}\left(\mathbf{Q}\right)\]</span> Because <span class="math display">\[\prod_{i=1}^k \lambda_i &lt; \prod_{i=1}^k \exp(\lambda_i)\]</span> where <span class="math inline">\(\{\lambda_i\}_{i=1}^k\)</span> are the <em>positive</em> eigenvalues of <span class="math inline">\(\mathbf{Q}\)</span> This in turn implies <span class="math display">\[\left|\mathbf{Q}\right| &lt; \prod_{i=1}^k \exp\left(q_{i,i}\right)\]</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="communication-channel" class="slide level3">
<h3>Communication Channel</h3>
<ul>
<li><p>Conditional density <span class="math inline">\(p(\mathbf{f}|\mathbf{u})\)</span> can be seen as a <em>communication channel</em>.</p></li>
<li><p>Normally we have: <span> <span class="math display">\[\text{Transmitter} \xrightarrow{\mathbf{u}} \begin{smallmatrix}p(\mathbf{f}|\mathbf{u}) \\ \text{Channel}\end{smallmatrix} \xrightarrow{\mathbf{f}} \text{Receiver}\]</span></span> and we control <span class="math inline">\(p(\mathbf{u})\)</span> (the source density).</p></li>
<li><p><em>Here</em> we can also control the transmission channel <span class="math inline">\(p(\mathbf{f}|\mathbf{u})\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="lower-bound-on-likelihood" class="slide level3">
<h3>Lower Bound on Likelihood</h3>
<p>Substitute variational bound into marginal likelihood: <span class="math display">\[p(\mathbf{y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{y}|\left&lt;\mathbf{f}\right&gt;,\sigma^2\mathbf{I}\right)p(\mathbf{u}) \text{d}\mathbf{u}\]</span> Note that: <span class="math display">\[\left&lt;\mathbf{f}\right&gt;_{p(\mathbf{f}|\mathbf{u})} = \mathbf{K}_{\mathbf{f}, \mathbf{u}} \mathbf{K}_{\mathbf{u}, \mathbf{u}}^{-1}\mathbf{u}\]</span> is <em>linearly</em> dependent on <span class="math inline">\(\mathbf{u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="deterministic-training-conditional" class="slide level3">
<h3>Deterministic Training Conditional</h3>
<p>Making the marginalization of <span class="math inline">\(\mathbf{u}\)</span> straightforward. In the Gaussian case: <span class="math display">\[p(\mathbf{u}) = \mathcal{N}\left(\mathbf{u}|\mathbf{0},\mathbf{K}_{\mathbf{u},\mathbf{u}}\right)\]</span>      </p>
<!--frame end-->
<!--frame start-->
</section>
<section id="variational-marginalisation-of-mathbff" class="slide level3">
<h3>Variational marginalisation of <span class="math inline">\(\mathbf{f}\)</span></h3>
<p><span class="math display">\[\log p(\mathbf{y}|\mathbf{u}) = \log\int p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|\mathbf{u},{\bf X})\text{d}\mathbf{f}\]</span></p>

<p><span class="math display">\[\log p(\mathbf{y}|\mathbf{u}) = \log \mathbb{E}_{p(\mathbf{f}|\mathbf{u},{\bf X})}\left[p(\mathbf{y}|\mathbf{f})\right]\]</span>  <span class="math display">\[\log p(\mathbf{y}|\mathbf{u}) \geq  \mathbb{E}_{p(\mathbf{f}|\mathbf{u},{\bf X})}\left[\log p(\mathbf{y}|\mathbf{f})\right]\triangleq \log\widetilde p(\mathbf{y}|\mathbf{u})\]</span> </p>
<p><span> No inversion of <span class="math inline">\(\mathbf{K}_{\mathbf{f}\mathbf{f}}\)</span> required</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="variational-marginalisation-of-mathbff-another-way" class="slide level3">
<h3>Variational marginalisation of <span class="math inline">\(\mathbf{f}\)</span> (another way)</h3>

<p><span class="math display">\[p(\mathbf{y}|\mathbf{u}) = \frac{p(\mathbf{y}|\mathbf{f})p(\mathbf{f}|\mathbf{u})}{p(\mathbf{f}|\mathbf{y}, \mathbf{u})}\]</span>  <span class="math display">\[\log p(\mathbf{y}|\mathbf{u}) = \log p(\mathbf{y}|\mathbf{f}) + \log \frac{p(\mathbf{f}|\mathbf{u})}{p(\mathbf{f}|\mathbf{y}, \mathbf{u})}\]</span>  <span class="math display">\[\log p(\mathbf{y}|\mathbf{u}) = \bbE_{p(\mathbf{f}|\mathbf{u})}\big[\log p(\mathbf{y}|\mathbf{f})\big] + \bbE_{p(\mathbf{f}|\mathbf{u})}\big[\log \frac{p(\mathbf{f}|\mathbf{u})}{p(\mathbf{f}|\mathbf{y}, \mathbf{u})}\big]\]</span>  <span class="math display">\[\log p(\mathbf{y}|\mathbf{u}) = \widetilde p(\mathbf{y}|\mathbf{u}) + \textsc{KL}[p(\mathbf{f}|\mathbf{u})||p(\mathbf{f}|\mathbf{y}, \mathbf{u})]\]</span></p>
<p><span> No inversion of <span class="math inline">\(\mathbf{K}_{\mathbf{f}\mathbf{f}}\)</span> required</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="a-lower-bound-on-the-likelihood" class="slide level3">
<h3>A Lower Bound on the Likelihood</h3>
<p><span class="math display">\[\widetilde p(\mathbf{y}|\mathbf{u})  = \prod_{i=1}^n\widetilde p(y_i|\mathbf{u})\]</span> <span class="math display">\[\widetilde p(y|\mathbf{u}) = \mathcal{N}\left(y|\mathbf{k}_{fu}\mathbf{K}_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{u},\sigma^2\right) \,{\color{red}\exp\left\{-\tfrac{1}{2\sigma^2}\left(k_{ff}- \mathbf{k}_{fu}\mathbf{K}_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{k}_{uf}\right)\right\}}\]</span></p>
<p><span>A straightforward likelihood approximation, and a penalty term</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="now-we-can-marginalise-mathbfu" class="slide level3">
<h3>Now we can marginalise <span class="math inline">\(\mathbf{u}\)</span></h3>
<p><span class="math display">\[\widetilde p(\mathbf{u}|\mathbf{y},\mathbf{Z}) = \frac{\widetilde p(\mathbf{y}|\mathbf{u})p(\mathbf{u}|\mathbf{Z})}{\int \widetilde p(\mathbf{y}|\mathbf{u})p(\mathbf{u}|\mathbf{Z})\dif{\mathbf{u}}}\]</span></p>
<ul>
<li><p>Computing the posterior costs <span class="math inline">\(\mathcal{O}(nm^2)\)</span></p></li>
<li><p>We also get a lower bound of the marginal likelihood</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="what-does-the-penalty-term-do" class="slide level3">
<h3>What does the penalty term do?</h3>
<p><span class="math display">\[{\color{red}\sum_{i=1}^n-\tfrac{1}{2\sigma^2}\left(k_{ff}- \mathbf{k}_{fu}\mathbf{K}_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{k}_{uf}\right)}\]</span></p>

<!--frame end-->
<!--frame start-->
</section>
<section id="what-does-the-penalty-term-do" class="slide level3">
<h3>What does the penalty term do?</h3>
<p><span class="math display">\[{\color{red}\sum_{i=1}^n-\tfrac{1}{2\sigma^2}\left(k_{ff}- \mathbf{k}_{fu}\mathbf{K}_{\mathbf{u}\mathbf{u}}^{-1}\mathbf{k}_{uf}\right)}\]</span></p>

<!--frame end-->
<!--frame start-->
</section>
<section id="what-does-the-penalty-term-do" class="slide level3">
<h3>What does the penalty term do?</h3>
<figure>
<img data-src="../../../gp/tex/diagrams/cov_approx" alt="image" style="width:60.0%" /><figcaption>image</figcaption>
</figure>
<figure>
<img data-src="../../../gp/tex/diagrams/cov_approx_opt" alt="image" style="width:60.0%" /><figcaption>image</figcaption>
</figure>
<!--frame end-->
<!--frame start-->
</section>
<section id="how-good-is-the-inducing-approximation" class="slide level3">
<h3>How good is the inducing approximation?</h3>
<p><span>It’s easy to show that as <span class="math inline">\(\mathbf{Z}\to {\bf X}\)</span>:</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{u}\to \mathbf{f}\)</span> (and the posterior is exact)</p></li>
<li><p>The penalty term is zero.</p></li>
<li><p>The cost returns to <span class="math inline">\(\mathcal{O}(n^3)\)</span></p></li>
</ul>

<ul>
<li><p><br />
</p></li>
<li></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="predictions" class="slide level3">
<h3>Predictions</h3>

<!--frame end-->
<!--frame start-->
</section>
<section id="recap" class="slide level3">
<h3>Recap</h3>
<p><span>So far we:</span></p>
<ul>
<li><p>introduced <span class="math inline">\(\mathbf{Z}, \mathbf{u}\)</span></p></li>
<li><p>approximated the intergral over <span class="math inline">\(\mathbf{f}\)</span> variationally</p></li>
<li><p>captured the information in <span class="math inline">\(\widetilde p(\mathbf{u}|\mathbf{y})\)</span></p></li>
<li><p>obtained a lower bound on the marginal likeihood</p></li>
<li><p>saw the effect of the penalty term</p></li>
<li><p>prediction for new points</p></li>
</ul>
<p><span>Omitted details:</span></p>
<ul>
<li><p>optimization of the covariance parameters using the bound</p></li>
<li><p>optimization of Z (simultaneously)</p></li>
<li><p>the form of <span class="math inline">\(\widetilde p(\mathbf{u}|\mathbf{y})\)</span></p></li>
<li><p>historical approximations</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="other-approximations" class="slide level3">
<h3>Other approximations</h3>
<p><span>Subset selection</span> </p>
<ul>
<li><p>Random or systematic</p></li>
<li><p>Set <span class="math inline">\(\mathbf{Z}\)</span> to subset of <span class="math inline">\({\bf X}\)</span></p></li>
<li><p>Set <span class="math inline">\(\mathbf{u}\)</span> to subset of <span class="math inline">\(\mathbf{f}\)</span></p></li>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{y}|\mathbf{u})\)</span>:</p>
<ul>
<li><p>$ p(_i) = p(_i_i) i$</p></li>
<li><p>$ p(_i) = 1  i$</p></li>
</ul></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="other-approximations" class="slide level3">
<h3>Other approximations</h3>

<ul>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{y}|\mathbf{u})\)</span>:</p>
<ul>
<li>$ p(_i) = (_i, [_i])$</li>
</ul></li>
<li><p>As our variational formulation, but without penalty</p></li>
</ul>
<p>Optimization of <span class="math inline">\(\mathbf{Z}\)</span> is difficult</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="other-approximations" class="slide level3">
<h3>Other approximations</h3>
<p><span>Fully Independent Training Conditional</span> </p>
<ul>
<li><p>Approximation to <span class="math inline">\(p(\mathbf{y}|\mathbf{u})\)</span>:</p></li>
<li><p>$ p() = _ip(_i) $</p></li>
</ul>
<p>Optimization of <span class="math inline">\(\mathbf{Z}\)</span> is still difficult, and there are some weird heteroscedatic effects</p>
<!--frame end-->
<!--frame start-->
<p>[label=bayesGplvm]</p>
</section>
<section id="selecting-data-dimensionality" class="slide level3">
<h3>Selecting Data Dimensionality</h3>
<ul>
<li><p>GP-LVM Provides probabilistic non-linear dimensionality reduction.</p></li>
<li><p>How to select the dimensionality?</p></li>
<li><p>Need to estimate marginal likelihood.</p></li>
<li><p>In standard GP-LVM it increases with increasing <span class="math inline">\(q\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame failure start-->


<!--frame failure end-->
</section>
<section id="standard-variational-approach-fails" class="slide level3">
<h3>Standard Variational Approach Fails</h3>
<ul>
<li><p>&lt;1-&gt; Standard variational bound has the form: <span class="math display">\[\mathcal{L}= \left&lt;\log p(\mathbf{y}|\mathbf{Z})\right&gt;_{q(\mathbf{Z})} + \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)\]</span></p></li>
<li><p>&lt;2-&gt; Requires expectation of <span class="math inline">\(\log p(\mathbf{y}|\mathbf{Z})\)</span> under <span class="math inline">\(q(\mathbf{Z})\)</span>. <span class="math display">\[\log p(\mathbf{y}|\mathbf{Z}) = -\frac{1}{2}\mathbf{y}^\top\left(\mathbf{K}_{\mathbf{f}, \mathbf{f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{y}-\frac{1}{2}\log \left|\mathbf{K}_{\mathbf{f}, \mathbf{f}}+\sigma^2 \mathbf{I}\right| -\frac{n}{2}\log 2\pi\]</span></p></li>
<li><p>&lt;3-&gt; Extremely difficult to compute because <span class="math inline">\(\mathbf{K}_{\mathbf{f}, \mathbf{f}}\)</span> is dependent on <span class="math inline">\(\mathbf{Z}\)</span> and appears in the inverse.</p></li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm" class="slide level3">
<h3>Variational Bayesian GP-LVM</h3>
<ul>
<li><p>&lt;1-&gt;Consider collapsed variational bound,   </p></li>
<li><p>&lt;4-&gt; Apply variational lower bound to the inner integral. </p></li>
<li><p>&lt;6-&gt; Which is analytically tractable for Gaussian <span class="math inline">\(q(\mathbf{Z})\)</span> and some covariance functions.</p></li>
</ul>
</section>
<section id="required-expectations" class="slide level3">
<h3>Required Expectations</h3>
<ul>
<li><p>Need expectations under <span class="math inline">\(q(\mathbf{Z})\)</span> of: <span class="math display">\[\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{k}_{i, \mathbf{u}}^\top \mathbf{K}_{\mathbf{u}, \mathbf{u}}^{-1} \mathbf{k}_{i, \mathbf{u}}\right]\]</span> and <span class="math display">\[\log \mathcal{N}\left(\mathbf{y}|\left&lt;\mathbf{f}\right&gt;_{p(\mathbf{f}|\mathbf{u},\mathbf{\MakeUppercase{y}})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log 2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{f}, \mathbf{u}}\mathbf{K}_{\mathbf{u},\mathbf{u}}^{-1}\mathbf{u}\right)^2\]</span></p></li>
<li><p>This requires the expectations <span class="math display">\[\left&lt;\mathbf{K}_{\mathbf{f},\mathbf{u}}\right&gt;_{q(\mathbf{Z})}\]</span> and <span class="math display">\[\left&lt;\mathbf{K}_{\mathbf{f},\mathbf{u}}\mathbf{K}_{\mathbf{u},\mathbf{u}}^{-1}\mathbf{K}_{\mathbf{u},\mathbf{f}}\right&gt;_{q(\mathbf{Z})}\]</span> which can be computed analytically for some covariance functions.</p></li>
</ul>
</section>
<section id="variational-compression-1" class="slide level3">
<h3>Variational Compression</h3>
<p><br />
<br />
</p>
<ul>
<li><p>Augment each layer with inducing variables <span class="math inline">\(\mathbf{u}_i\)</span>.</p></li>
<li><p>Apply variational compression, <span class="math display">\[\begin{align}
      p(\dataVector, \{\hiddenVector_i\}_{i=1}^{\numLayers-1}|\{\inducingVector_i\}_{i=1}^{\numLayers}, \inputMatrix) \geq &amp; 
      \tilde p(\dataVector|\inducingVector_{\numLayers}, \hiddenVector_{\numLayers-1})\prod_{i=2}^{\numLayers-1} \tilde p(\hiddenVector_i|\inducingVector_i,\hiddenVector_{i-1}) \tilde p(\hiddenVector_1|\inducingVector_i,\inputMatrix) \nonumber \\
      &amp; \times
      \exp\left(\sum_{i=1}^\numLayers -\frac{1}{2\sigma^2_i}\trace{\conditionalCovariance_{i}}\right)
      \label{eq:deep_structure}
    \end{align}\]</span> where <span class="math display">\[\tilde p(\mathbf{h}_i|\mathbf{u}_i,\mathbf{h}_{i-1})
    = \mathcal{N}\left(\mathbf{h}_i|\mathbf{K}_{\mathbf{h}_{i}\mathbf{u}_{i}}\mathbf{K}_{\mathbf{u}_i\mathbf{u}_i}^{-1}\mathbf{u}_i,\sigma^2_i\mathbf{I}\right).\]</span></p></li>
</ul>
</section>
<section id="nested-variational-compression" class="slide level3">
<h3>Nested Variational Compression</h3>
<p><br />
</p>
<ul>
<li><p>By sustaining explicity distributions over inducing variables James Hensman has developed a nested variatnt of variational compression.</p></li>
<li><p>Exciting thing: it mathematically looks like a deep neural network, but with inducing variables in the place of basis functions.</p></li>
<li><p>Additional complexity control term in the objective function.</p></li>
</ul>

</section>
<section id="nested-bound" class="slide level3">
<h3>Nested Bound</h3>
<span class="math display">\[\begin{align}
    \log p(\dataVector|\inputMatrix )  \geq &amp;
    %
    -\frac{1}{\sigma_1^2} \trace{\conditionalCovariance_1}
    % 
    -\sum_{i=2}^\numLayers \frac{1}{2\sigma_i^2} \left(\psi_{i}
    % 
    - \trace{{\boldsymbol \Phi}_{i}\kernelMatrix_{\inducingVector_{i} \inducingVector_{i}}^{-1}}\right) \nonumber \\
    %
    &amp; - \sum_{i=1}^{\numLayers}\KL{q(\inducingVector_i)}{p(\inducingVector_i)} \nonumber \\
    %
    &amp; - \sum_{i=2}^{\numLayers}\frac{1}{2\sigma^2_{i}}\trace{({\boldsymbol
        \Phi}_i - {\boldsymbol \Psi}_i^\top{\boldsymbol \Psi}_i)
      \kernelMatrix_{\inducingVector_{i}
        \inducingVector_{i}}^{-1}
      \expDist{\inducingVector_{i}\inducingVector_{i}^\top}{q(\inducingVector_{i})}\kernelMatrix_{\inducingVector_{i}\inducingVector_{i}}^{-1}} \nonumber \\
    %
    &amp; + {\only&lt;2&gt;{\color{\redColor}}\log \gaussianDist{\dataVector}{{\boldsymbol
        \Psi}_{\numLayers}\kernelMatrix_{\inducingVector_{\numLayers}
        \inducingVector_{\numLayers}}^{-1}{\mathbf
        m}_\numLayers}{\sigma^2_\numLayers\eye}}
    \label{eq:deep_bound}
  \end{align}\]</span>
</section>
<section id="required-expectations-1" class="slide level3">
<h3>Required Expectations</h3>
<p><span class="math display">\[{\only&lt;1&gt;{\color{\redColor}}\log \mathcal{N}\left(\mathbf{y}|{\only&lt;2-&gt;{\color{\blueColor}}{\boldsymbol
          \Psi}_{\ell}}\mathbf{K}_{\mathbf{u}_{\ell}
          \mathbf{u}_{\ell}}^{-1}{\mathbf
          m}_\ell,\sigma^2_\ell\mathbf{I}\right)}\]</span> where   </p>
<!--frame start-->
</section>
<section id="gaussian-py_if_i-1" class="slide level3">
<h3>Gaussian <span class="math inline">\(p(y_i|f_i)\)</span></h3>
<p>For Gaussian likelihoods:  </p>
<!--frame end-->
<!--frame start-->
</section>
<section id="gaussian-process-over-mathbff-and-mathbfu-1" class="slide level3">
<h3>Gaussian Process Over <span class="math inline">\(\mathbf{f}\)</span> and <span class="math inline">\(\mathbf{u}\)</span></h3>
<p>Define: <span class="math display">\[q_{i, i} = \text{var}_{p(f_i|\mathbf{u})}\left( f_i \right) = \left&lt;f_i^2\right&gt;_{p(f_i|\mathbf{u})} - \left&lt;f_i\right&gt;_{p(f_i|\mathbf{u})}^2\]</span> We can write: <span class="math display">\[c_i = \exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)\]</span> If joint distribution of <span class="math inline">\(p(\mathbf{f}, \mathbf{u})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} = k_{i, i} - \mathbf{k}_{i, \mathbf{u}}^\top \mathbf{K}_{\mathbf{u}, \mathbf{u}}^{-1} \mathbf{k}_{i, \mathbf{u}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span class="math inline">\(\mathbf{u}\)</span> but <em>is</em> a function of <span class="math inline">\({\bf X}_\mathbf{u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="lower-bound-on-likelihood-1" class="slide level3">
<h3>Lower Bound on Likelihood</h3>
<p>Substitute variational bound into marginal likelihood: <span class="math display">\[p(\mathbf{y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{y}|\left&lt;\mathbf{f}\right&gt;,\sigma^2\mathbf{I}\right)p(\mathbf{u}) \text{d}\mathbf{u}\]</span> Note that: <span class="math display">\[\left&lt;\mathbf{f}\right&gt;_{p(\mathbf{f}|\mathbf{u})} = \mathbf{K}_{\mathbf{f}, \mathbf{u}} \mathbf{K}_{\mathbf{u}, \mathbf{u}}^{-1}\mathbf{u}\]</span> is <em>linearly</em> dependent on <span class="math inline">\(\mathbf{u}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="deterministic-training-conditional-1" class="slide level3">
<h3>Deterministic Training Conditional</h3>
<p>Making the marginalization of <span class="math inline">\(\mathbf{u}\)</span> straightforward. In the Gaussian case: <span class="math display">\[p(\mathbf{u}) = \mathcal{N}\left(\mathbf{u}|\mathbf{0},\mathbf{K}_{\mathbf{u},\mathbf{u}}\right)\]</span>      </p>
<!--frame end-->
</section>
<section id="efficient-computation" class="slide level3">
<h3>Efficient Computation</h3>
<ul>
<li>Thang and Turner paper</li>
</ul>
</section>
<section id="other-limitations" class="slide level3">
<h3>Other Limitations</h3>
<ul>
<li>Joint Gaussianity is analytic, but not flexible.</li>
</ul>
<p>include{../_deepgp/includes/what-is-a-deep-gp.md} include{../_deepgp/includes/deep_nn_gp.md}</p>
<!--Deep Gaussian Process Models-->
<p>include{../_deepgp/includes/deeptheory.md}</p>
<!--Bayesian GP-LVM-->
<!--include{../_gplvm/includes/ard_gplvm.md} -->
<!-- <!--frame start-->
<p>[label=bayesGplvm]</p>
</section>
<section id="selecting-data-dimensionality-1" class="slide level3">
<h3>Selecting Data Dimensionality</h3>
<ul>
<li><p>GP-LVM Provides probabilistic non-linear dimensionality reduction.</p></li>
<li><p>How to select the dimensionality?</p></li>
<li><p>Need to estimate marginal likelihood.</p></li>
<li><p>In standard GP-LVM it increases with increasing <span class="math inline">\(q\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame failure start-->


<!--frame failure end-->
<p>–&gt; <!-- ### Standard Variational Approach Fails

-   <1-> Standard variational bound has the form:
    $$\likelihoodBound = \expDist{\log p(\dataVector|\latentMatrix)}{q(\latentMatrix)} + \KL{q(\latentMatrix)}{p(\latentMatrix)}$$

-   <2-> Requires expectation of
    $\log p(\dataVector|\latentMatrix)$ under $q(\latentMatrix)$.
    $$\log p(\dataVector|\latentMatrix) = -\frac{1}{2}\dataVector^\top\left(\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2\eye\right)^{-1}\dataVector -\frac{1}{2}\log \det{\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2 \eye} -\frac{\numData}{2}\log 2\pi$$

-   <3-> Extremely difficult to compute because
    $\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}$ is
    dependent on $\latentMatrix$ and appears in the inverse.

### Variational Bayesian GP-LVM

-   <1->Consider collapsed variational bound, \only<1>{\[
              p(\dataVector)\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expSamp{\mappingFunctionVector}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
              \]} \only<2>{\[
              p(\dataVector|\latentMatrix )\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
              \]} \only<3->{\[
              \int p(\dataVector|\latentMatrix)p(\latentMatrix) \text{d}\latentMatrix \geq \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye} p(\latentMatrix)\text{d}\latentMatrix p(\inducingVector) \text{d}\inducingVector   
              \]}

-   <4-> Apply variational lower bound to the inner integral.
    \only<5->{\small\begin{align*}
              \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}& p(\latentMatrix)\text{d}\latentMatrix\\ \geq & \expDist{\sum_{i=1}^\numData\log  c_i}{q(\latentMatrix)}\\& +\expDist{\log\gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}}{q(\latentMatrix)}\\& + \KL{q(\latentMatrix)}{p(\latentMatrix)}    
              \end{align*}}

-   <6-> Which is analytically tractable for Gaussian
    $q(\latentMatrix)$ and some covariance functions.

### Required Expectations

-   Need expectations under $q(\latentMatrix)$ of:
    $$\log c_i = \frac{1}{2\dataStd^2} \left[\kernelScalar_{i, i} - \kernelVector_{i, \inducingVector}^\top \kernelMatrix_{\inducingVector, \inducingVector}^{-1} \kernelVector_{i, \inducingVector}\right]$$
    and
    $$\log \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector,\dataMatrix)}}{\dataStd^2\eye} = -\frac{1}{2}\log 2\pi\dataStd^2 - \frac{1}{2\dataStd^2}\left(\dataScalar_i - \kernelMatrix_{\mappingFunctionVector, \inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\inducingVector\right)^2$$

-   This requires the expectations
    $$\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}}{q(\latentMatrix)}$$
    and
    $$\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\kernelMatrix_{\inducingVector,\mappingFunctionVector}}{q(\latentMatrix)}$$
    which can be computed analytically for some covariance functions.


 --></p>
<!-- <!--frame start-->
</section>
<section id="leads-to-other-approximations" class="slide level3">
<h3>Leads to Other Approximations …</h3>
<ul>
<li><p>Let’s be explicity about storing approximate posterior of <span class="math inline">\(\mathbf{u}\)</span>, <span class="math inline">\(q(\mathbf{u})\)</span>.</p></li>
<li><p>Now we have <span class="math display">\[p(\mathbf{y}^*|\mathbf{y}) = \int p(\mathbf{y}^*| \mathbf{u}) q(\mathbf{u}| \mathbf{y}) \mathbf{u}\]</span></p></li>
<li><p>Inducing variables look a lot like regular parameters.</p></li>
<li><p><em>But</em>: their dimensionality does not need to be set at design time.</p></li>
<li><p>They can be modified arbitrarily at run time without effecting the model likelihood.</p></li>
<li><p>They only effect the quality of compression and the lower bound.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="in-gps-for-big-data" class="slide level3">
<h3>In GPs for Big Data</h3>
<ul>
<li><p>Exploit the resulting factorization … <span class="math display">\[p(\mathbf{y}^*|\mathbf{y}) = \int p(\mathbf{y}^*| \mathbf{u}) q(\mathbf{u}| \mathbf{y}) \mathbf{u}\]</span> </p></li>
<li><p>The distribution now <em>factorizes</em>: <span class="math display">\[p(\mathbf{y}^*|\mathbf{y}) = \int \prod_{i=1}^{n^*}p(y^*_i| \mathbf{u}) q(\mathbf{u}| \mathbf{y}) \mathbf{u}\]</span></p></li>
<li><p>This factorization can be exploited for stochastic variational inference [Hoffman:stochastic12].</p></li>
</ul>
<!--frame end-->
<p>–&gt; <!--<!--frame start--> ### Nonparametrics for Very Large Data Sets</p>
<p>Modern data availability<br />
<img data-src="../../../ml/tex/diagrams/house_price_country.png" alt="image" style="width:60.0%" /></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="nonparametrics-for-very-large-data-sets" class="slide level3">
<h3>Nonparametrics for Very Large Data Sets</h3>

<p><img data-src="../../../ml/tex/diagrams/house_price_peak_district.png" alt="image" style="width:60.0%" /><br />
</p>
<!--frame end-->
<!--frame start-->
<p>Hensman:bigdata13<br />
</p>
<p><br />
</p>
<!--frame end-->
<p>–&gt;</p>
</section>
<section id="structures-for-extracting-information-from-data" class="slide level3">
<h3>Structures for Extracting Information from Data</h3>

</section>
<section id="section-14" class="slide level3">
<h3></h3>
<p>Damianou:deepgp13<br />
</p>
<p><a href="http://jmlr.org/proceedings/papers/v31/damianou13a.pdf">\includegraphics[page=1,trim=0cm 16cm 0cm 0.5cm, width=0.4, clip=true, width=1.0]{../../../gp/tex/diagrams/damianou13a.pdf}</a></p>
<!--frame failure start-->




<!--frame failure end-->
<!--frame failure start-->




<!--frame failure end-->
<!--frame start-->
</section>
<section id="deep-gaussian-processes" class="slide level3">
<h3>Deep Gaussian Processes</h3>

<ul>
<li><p>Deep architectures allow abstraction of features <span></span>.</p></li>
<li><p>We use variational approach to stack GP models.</p></li>
</ul>
<!--frame end-->
<pre class="sourceCode numberSource octave numberLines" id="mycode" data-startFrom="0"><code class="sourceCode octave"><div class="sourceLine" id="mycode-0" data-line-number="0"></div>
<div class="sourceLine" id="mycode-1" data-line-number="1"><span class="co">%}</span></div>
<div class="sourceLine" id="mycode-2" data-line-number="2">  importLatest(<span class="st">&#39;GPmat&#39;</span>)</div>
<div class="sourceLine" id="mycode-3" data-line-number="3">  gpmatToolboxes;</div>
<div class="sourceLine" id="mycode-4" data-line-number="4">  <span class="fu">randn</span>(<span class="st">&#39;seed&#39;</span>, <span class="fl">1e6</span>);</div>
<div class="sourceLine" id="mycode-5" data-line-number="5">  <span class="fu">rand</span>(<span class="st">&#39;seed&#39;</span>, <span class="fl">1e6</span>);</div>
<div class="sourceLine" id="mycode-6" data-line-number="6">  textWidth = <span class="fl">13</span>;</div>
<div class="sourceLine" id="mycode-7" data-line-number="7">  dirName = <span class="st">&#39;../../../health/tex/diagrams/&#39;</span>;</div>
<div class="sourceLine" id="mycode-8" data-line-number="8">  blueColor = [<span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">1</span>];</div>
<div class="sourceLine" id="mycode-9" data-line-number="9">  redColor = [<span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span>];</div>
<div class="sourceLine" id="mycode-10" data-line-number="10">  magentaColor = [<span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span>];</div>
<div class="sourceLine" id="mycode-11" data-line-number="11">  blackColor = [<span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span>];</div>
<div class="sourceLine" id="mycode-12" data-line-number="12">  negative = <span class="bn">false</span>;</div>
<div class="sourceLine" id="mycode-13" data-line-number="13">  if blackBackground</div>
<div class="sourceLine" id="mycode-14" data-line-number="14">    negative = <span class="bn">true</span>;</div>
<div class="sourceLine" id="mycode-15" data-line-number="15">    blueColor =  <span class="fl">1</span>-blueColor;</div>
<div class="sourceLine" id="mycode-16" data-line-number="16">    redColor = <span class="fl">1</span>-redColor;</div>
<div class="sourceLine" id="mycode-17" data-line-number="17">    magentaColor = <span class="fl">1</span>-magentaColor;</div>
<div class="sourceLine" id="mycode-18" data-line-number="18">    blackColor = <span class="fl">1</span>- blackColor;</div>
<div class="sourceLine" id="mycode-19" data-line-number="19">  end</div>
<div class="sourceLine" id="mycode-20" data-line-number="20"><span class="co">%{</span></div></code></pre>
\begin{frame}[fragile]


\begin{columns}[c]              


 \only&lt;2,4,6,8,10&gt;{

<p>\end{center}</p>
<!--frame failure end-->
<!--Conclusions-->
<!-- include{../_gplvm/includes/ard_model.md} -->
<!-- ![image](../../../_gplvm/tex/diagrams/andreasDeepTalk){width="80%"}


 -->
<!--Gaussian Process Dynamical Systems-->
<!-- <!--frame failure start-->




<!--frame failure end-->
<!--frame start-->
</section>
<section id="gaussian-process-over-latent-space" class="slide level3">
<h3>Gaussian Process over Latent Space</h3>
<ul>
<li><p>Assume a GP prior for <span class="math inline">\(p(\mathbf{Z})\)</span>.</p></li>
<li><p>Input to the process is time, <span class="math inline">\(p(\mathbf{Z}|t)\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="interpolation-of-hd-video" class="slide level3">
<h3>Interpolation of HD Video</h3>

<!--frame end-->
<p>–&gt;</p>
<!--Shared GP-LVM-->
<!-- <!--frame failure start-->



<!--frame failure end-->
<!--frame failure start-->




<!--frame failure end-->
<!--frame failure start-->


<pre><code>\def\layersep{2cm}
\begin{center}
  \begin{tikzpicture}[node distance=\layersep]</code></pre>
<p> = [text width=4em, text centered] % Draw the input layer nodes / in {1,…,8} % This is the same as writing / in {1/1,2/2,3/3,4/4} (Y-) at (, 0) {<span class="math inline">\(y_\x\)</span>};</p>
<pre><code>% Draw the hidden layer nodes
\foreach \name / \x in {1,...,6}
    \path[xshift=1cm]
        node[latent] (X-\name) at (\x cm, \layersep) {$\latentScalar_\x$};

% Connect every node in the latent layer with every node in the
% data layer.
\foreach \source in {1,...,6}
    \foreach \dest in {1,...,8}
        \draw[-&gt;] (X-\source) -- (Y-\dest);



%  Annotate the layers
\node[annot,left of=X-1, node distance=1cm] (ls) {Latent space};
\node[annot,left of=Y-1, node distance=1cm] (ds) {Data space};</code></pre>
<p>\end{tikzpicture} \end{center}</p>
<!--frame failure end-->
<!--frame failure start-->

<pre><code>\def\layersep{2cm}
\begin{center}
  \begin{tikzpicture}[node distance=\layersep]</code></pre>
<p> = [text width=4em, text centered] % Draw the input layer nodes / in {1,…,4} % This is the same as writing / in {1/1,2/2,3/3,4/4} (Y-) at (, 0) {<span class="math inline">\(y^{(1)}_\x\)</span>};</p>
<pre><code>\foreach \name / \x in {1,...,4}
% This is the same as writing \foreach \name / \x in {1/1,2/2,3/3,4/4}
    \node[obs] (Z-\name) at (\x+5, 0) {$\dataScalar^{(2)}_\x$};

% Draw the hidden layer nodes
\foreach \name / \x in {1,...,6}
    \path[xshift=2cm]
        node[latent] (X-\name) at (\x cm, \layersep) {$\latentScalar_\x$};

% Connect every node in the latent layer with every node in the
% data layer.
\foreach \source in {1,...,6}
    \foreach \dest in {1,...,4}
        \draw[-&gt;] (X-\source) -- (Y-\dest);

\foreach \source in {1,...,6}
    \foreach \dest in {1,...,4}
        \draw[-&gt;] (X-\source) -- (Z-\dest);


%  Annotate the layers
\node[annot,left of=X-1, node distance=1cm] (ls) {Latent space};
\node[annot,left of=Y-1, node distance=1cm] (ds) {Data space};</code></pre>
<p>\end{tikzpicture} Separate ARD parameters for mappings to <span class="math inline">\(\mathbf{\MakeUppercase{y}}^{(1)}\)</span> and <span class="math inline">\(\mathbf{\MakeUppercase{y}}^{(2)}\)</span>. \end{center}</p>
<!--frame failure end-->
<!--frame start-->
<figure>
<img data-src="../../../gplvm/tex/diagrams/andreasDeepTalk" alt="image" /><figcaption>image</figcaption>
</figure>
<!--frame end-->
<!--frame start-->
<figure>
<img data-src="../../../gplvm/tex/diagrams/andreasDeepTalk" alt="image" /><figcaption>image</figcaption>
</figure>
<!--frame end-->
<!--frame start-->
<figure>
<img data-src="../../../gplvm/tex/diagrams/andreasDeepTalk" alt="image" /><figcaption>image</figcaption>
</figure>
<!--frame end-->
<!--frame start-->
</section>
<section id="manifold-relevance-determination" class="slide level3">
<h3>Manifold Relevance Determination</h3>

<!--frame end-->
<p>–&gt;</p>
</section>
<section id="what-can-we-do-that-google-cant" class="slide level3">
<h3>What Can We Do that Google Can’t?</h3>
<ul>
<li><p>Google’s resources give them access to volumes of data (or Facebook, or Microsoft, or Amazon).</p></li>
<li><p>Is there anything for Universities to contribute?</p></li>
<li><p>Assimilation of multiple views of the patient: each perhaps from a different patient.</p></li>
<li><p>This may be done by small companies (with support of Universities).</p></li>
<li><p>A Facebook app for your personalised health.</p></li>
<li><p>These methodologies are part of that picture.</p></li>
</ul>
<!--frame failure start-->




<!--frame failure end-->
<!--frame start-->
</section>
<section id="deep-health-power-ranger-model-of-research" class="slide level3">
<h3>Deep Health: Power Ranger Model of Research</h3>


<p><img data-src="../../../health/tex/diagrams/deep_health_power_rangers.jpg" alt="image" style="width:50.0%" /><br />
Thanks to Alan Saul for creating the image.</p>
<!--frame end-->
<!--include{../_deepgp/includes/multi-fidelity-modelling.md}-->
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: lawrennd</li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
