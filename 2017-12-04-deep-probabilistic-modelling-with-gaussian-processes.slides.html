<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Neil D. Lawrence">
  <title>Deep Probabilistic Modelling with with Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
      <script type="text/javascript" src="svg_mathjax.js">
      </script>
      <script type="text/javascript">
          new Svg_MathJax().install();
      </script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Deep Probabilistic Modelling with with Gaussian Processes</h1>
  <p class="author">Neil D. Lawrence</p>
</section>

<section class="slide level3">

<!--Notes from Stefanos: Hey Neil, 

Just realised that there was no comment on the fact that a DGP is not a GP, only the current layer conditioned on all previous ones.

I don't know if you want to clarify that. I believe that the majority of the audience won't have that knowledge and they may leave with the wrong impression.

Although, I don't know where is the right time to introduce that in the talk.

Hope that's helpful.

Cheers,
Stefanos

Comments from Rich!


CMB samples -> Life
-->

<!-- Introduction to GPs -->
</section>
<section id="what-is-machine-learning" class="slide level3" data-transition="none">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \rightarrow \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{data}\)</span> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{model}\)</span> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{prediction}\)</span> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
</section>
<section id="artificial-intelligence" class="slide level3" data-transition="none">
<h3>Artificial Intelligence</h3>
<ul>
<li>Machine learning is a mainstay because of importance of prediction.</li>
</ul>
</section>
<section id="what-is-machine-learning-1" class="slide level3" data-transition="none">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \rightarrow \text{prediction}\]</span></p>
<div class="fragment">
<ul>
<li>To combine data with a model need:</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>a prediction function</strong> <span class="math inline">\(f(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>an objective function</strong> <span class="math inline">\(E(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</div>
</section>
<section id="uncertainty" class="slide level3" data-transition="none">
<h3>Uncertainty</h3>
<ul>
<li><p>Uncertainty in prediction arises from:</p></li>
<li><p>scarcity of training data and</p></li>
<li><p>mismatch between the set of prediction functions we choose and all possible prediction functions.</p></li>
<li><p>Also uncertainties in objective, leave those for another day.</p></li>
</ul>
</section>
<section id="neural-networks-and-prediction-functions" class="slide level3" data-transiation="None">
<h3>Neural Networks and Prediction Functions</h3>
<ul>
<li><p>adaptive non-linear function models inspired by simple neuron models <span class="citation" data-cites="McCulloch:neuron43">(McCulloch and Pitts, 1943)</span></p></li>
<li><p>have become popular because of their ability to model data.</p></li>
<li><p>can be composed to form highly complex functions</p></li>
<li><p>start by focussing on one hidden layer</p></li>
</ul>
</section>
<section id="prediction-function-of-one-hidden-layer" class="slide level3" data-transiation="None">
<h3>Prediction Function of One Hidden Layer</h3>
<p><span class="math display">\[
f({\bf x}) = \left.\mathbf{w}^{(2)}\right.^\top \boldsymbol{\phi}(\mathbf{W}_{1}, {\bf x})
\]</span></p>
<p><span class="math inline">\(f(\cdot)\)</span> is a scalar function with vector inputs,</p>
<p><span class="math inline">\(\boldsymbol{\phi}(\cdot)\)</span> is a vector function with vector inputs.</p>
<ul>
<li><p>dimensionality of the vector function is known as the number of hidden units, or the number of neurons.</p></li>
<li><p>elements of <span class="math inline">\(\boldsymbol{\phi}(\cdot)\)</span> are the <em>activation</em> function of the neural network</p></li>
<li><p>elements of <span class="math inline">\(\mathbf{W}_{1}\)</span> are the parameters of the activation functions.</p></li>
</ul>
</section>
<section id="relations-with-classical-statistics" class="slide level3" data-transiation="None">
<h3>Relations with Classical Statistics</h3>
<ul>
<li><p>In statistics activation functions are known as <em>basis functions</em>.</p></li>
<li><p>would think of this as a <em>linear model</em>: not linear predictions, linear in the parameters</p></li>
<li><p><span class="math inline">\(\mathbf{W}_{1}\)</span> are <em>static</em> parameters.</p></li>
</ul>
</section>
<section id="adaptive-basis-functions" class="slide level3" data-transiation="None">
<h3>Adaptive Basis Functions</h3>
<ul>
<li><p>In machine learning we optimize <span class="math inline">\(\mathbf{W}_{1}\)</span> as well as <span class="math inline">\(\mathbf{W}_{2}\)</span> (which would normally be denoted in statistics by <span class="math inline">\(\boldsymbol{\beta}\)</span>).</p></li>
<li><p>This tutorial: revisit that decision: follow the path of Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> and David MacKay <span class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span>.</p></li>
<li><p>Consider the probabilistic approach.</p></li>
</ul>
</section>
<section id="probabilistic-modelling" class="slide level3" data-transition="None">
<h3>Probabilistic Modelling</h3>
<ul>
<li>Probabilistically we want, <span class="math display">\[
p(y_*|\mathbf{y}, {\bf X}, {\bf x}_*),
\]</span> <span class="math inline">\(y_*\)</span> is a test output <span class="math inline">\({\bf x}_*\)</span> is a test input <span class="math inline">\({\bf X}\)</span> is a training input matrix <span class="math inline">\(\mathbf{y}\)</span> is training outputs</li>
</ul>
</section>
<section id="joint-model-of-world" class="slide level3" data-transition="None">
<h3>Joint Model of World</h3>
<p><span class="math display">\[
p(y_*|\mathbf{y}, {\bf X}, {\bf x}_*) = \int p(y_*|{\bf x}_*, \mathbf{W}) p(\mathbf{W}| \mathbf{y}, {\bf X}) \text{d} \mathbf{W}
\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\mathbf{W}\)</span> contains <span class="math inline">\(\mathbf{W}_1\)</span> and <span class="math inline">\(\mathbf{W}_2\)</span></p>
<p><span class="math inline">\(p(\mathbf{W}| \mathbf{y}, {\bf X})\)</span> is posterior density</p>
</div>
</section>
<section id="likelihood" class="slide level3" data-transition="None">
<h3>Likelihood</h3>
<p><span class="math inline">\(p(y|{\bf x}, \mathbf{W})\)</span> is the <em>likelihood</em> of data point</p>
<div class="fragment">
<p>Normally assume independence: <span class="math display">\[
p(\mathbf{y}|{\bf X}, \mathbf{W}) \prod_{i=1}^np(y_i|{\bf x}_i, \mathbf{W}),\]</span></p>
</div>
</section>
<section id="likelihood-and-prediction-function" class="slide level3" data-transition="None">
<h3>Likelihood and Prediction Function</h3>
<p><span class="math display">\[
p(y_i | f({\bf x}_i)) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{\left(y_i - f({\bf x}_i)\right)^2}{2\sigma^2}\right)
\]</span></p>
</section>
<section id="unsupervised-learning" class="slide level3" data-transition="None">
<h3>Unsupervised Learning</h3>
<ul>
<li><p>Can also consider priors over latents <span class="math display">\[
p(\mathbf{y}_*|\mathbf{y}) = \int p(\mathbf{y}_*|{\bf X}_*, \mathbf{W}) p(\mathbf{W}| \mathbf{y}, {\bf X}) p({\bf X}) p({\bf X}_*) \text{d} \mathbf{W}\text{d} {\bf X}\text{d}{\bf X}_*
\]</span></p></li>
<li><p>This gives <em>unsupervised learning</em>.</p></li>
</ul>
</section>
<section id="probabilistic-inference" class="slide level3" data-transition="None">
<h3>Probabilistic Inference</h3>
<ul>
<li><p>Data: <span class="math inline">\(\mathbf{y}\)</span></p></li>
<li><p>Model: <span class="math inline">\(p(\mathbf{y}, \mathbf{y}^*)\)</span></p></li>
<li><p>Prediction: <span class="math inline">\(p(\mathbf{y}^*| \mathbf{y})\)</span></p></li>
</ul>
</section>
<section id="graphical-models" class="slide level3" data-transition="None">
<h3>Graphical Models</h3>
<ul>
<li><p>Represent joint distribution through <em>conditional dependencies</em>.</p></li>
<li><p>E.g. Markov chain</p></li>
</ul>
<p><span class="math display">\[p(\mathbf{y}) = p(y_n| y_{n-1}) p(y_{n-1}|y_{n-2}) \dots p(y_{2} | y_{1})\]</span></p>
<object class="svgplot" data="../slides/diagrams/markov.svg">
</object>
</section>
<section id="section" class="slide level3" data-transition="None">
<h3></h3>
<p>Predict Perioperative Risk of Clostridium Difficile Infection Following Colon Surgery</p>
<p><img src="../slides/diagrams/bayes-net-diagnosis.png" class="negate" width="40%" align="center" style="background:none; border:none; box-shadow:none;"></p>
<p><span class="citation" data-cites="Steele:predictive12">(Steele et al., 2012)</span></p>
</section>
<section id="performing-inference" class="slide level3" data-transition="None">
<h3>Performing Inference</h3>
<ul>
<li><p>Easy to write in probabilities</p></li>
<li><p>But underlying this is a wealth of computational challenges.</p></li>
<li><p>High dimensional integrals typically require approximation.</p></li>
</ul>
</section>
<section id="linear-models" class="slide level3" data-transition="None">
<h3>Linear Models</h3>
<ul>
<li><p>In statistics, focussed more on <em>linear</em> model implied by <span class="math display">\[
  f({\bf x}) = \left.\mathbf{w}\right.^{(2)}^\top \boldsymbol{\phi}(\mathbf{W}_1, {\bf x})
  \]</span></p></li>
<li><p>Hold <span class="math inline">\(\mathbf{W}_1\)</span> fixed for given analysis.</p></li>
<li><p>Gaussian prior for <span class="math inline">\(\mathbf{W}\)</span>, <span class="math display">\[
  \mathbf{w}^{(2)} \sim \mathcal{N}\left(\mathbf{0},\mathbf{C}\right).
  \]</span> <span class="math display">\[
  y_i = f({\bf x}_i) + \epsilon_i,
  \]</span> where <span class="math display">\[
  \epsilon_i \sim \mathcal{N}\left(0,\sigma^2\right)
  \]</span></p></li>
</ul>
</section>
<section id="linear-gaussian-models" class="slide level3" data-transition="None">
<h3>Linear Gaussian Models</h3>
<ul>
<li>Normally integrals are complex but for this Gaussian linear case they are trivial.</li>
</ul>
</section>
<section id="multivariate-gaussian-properties" class="slide level3" data-transition="none">
<h3>Multivariate Gaussian Properties</h3>
<!--frame start-->
</section>
<section id="recall-univariate-gaussian-properties" class="slide level3" data-transition="None">
<h3>Recall Univariate Gaussian Properties</h3>
<div class="fragment">
<ol type="1">
<li>Sum of Gaussian variables is also Gaussian.</li>
</ol>
<p><span class="math display">\[{y}_i \sim {\mathcal{N}\left({\mu}_i,\sigma_i^2\right)}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\sum_{i=1}^{{n}} {y}_i \sim {\mathcal{N}\left(\sum_{i=1}^{n}{\mu}_i,\sum_{i=1}^{n}\sigma_i^2\right)}\]</span></p>
</div>
<div class="fragment">
<ol start="2" type="1">
<li>Scaling a Gaussian leads to a Gaussian.</li>
</ol>
</div>
<div class="fragment">
<p><span class="math display">\[{y}\sim {\mathcal{N}\left({\mu},\sigma^2\right)}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[{w}{y}\sim {\mathcal{N}\left({w}{\mu},{w}^2 \sigma^2\right)}\]</span></p>
</div>
</section>
<section id="multivariate-consequence" class="slide level3" data-transition="None">
<h3>Multivariate Consequence</h3>
<div data-align="left">
If
</div>
<p><span class="math display">\[{{\bf {x}}}\sim {\mathcal{N}\left({\boldsymbol{{\mu}}},\boldsymbol{\Sigma}\right)}\]</span></p>
<div class="fragment">
<div data-align="left">
And
</div>
<p><span class="math display">\[{\mathbf{{y}}}= {\mathbf{W}}{{\bf {x}}}\]</span></p>
</div>
<div class="fragment">
<div data-align="left">
Then
</div>
<p><span class="math display">\[{\mathbf{{y}}}\sim {\mathcal{N}\left({\mathbf{W}}{\boldsymbol{{\mu}}},{\mathbf{W}}\boldsymbol{\Sigma}{\mathbf{W}}^\top\right)}\]</span></p>
</div>
</section>
<section id="linear-gaussian-models-1" class="slide level3" data-transition="None">
<h3>Linear Gaussian Models</h3>
<ol type="1">
<li>linear Gaussian models are easier to deal with</li>
<li>Even the parameters <em>within</em> the process can be handled, by considering a particular limit.</li>
</ol>
</section>
<section id="multivariate-gaussian-properties-1" class="slide level3" data-transition="None">
<h3>Multivariate Gaussian Properties</h3>
<ul>
<li>If <span class="math display">\[
\mathbf{y}= \mathbf{W}{\bf x}+ \boldsymbol{\epsilon},
\]</span></li>
<li>Assume <span class="math display">\[\begin{align}
{\bf x}&amp; \sim \mathcal{N}\left(\boldsymbol{\mu},\mathbf{C}\right)\\
\boldsymbol{\epsilon}&amp; \sim \mathcal{N}\left(\mathbf{0},\boldsymbol{\Sigma}\right)
\end{align}\]</span></li>
<li>Then <span class="math display">\[
\mathbf{y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{\Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{\Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic Principal Component Analysis <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>, because we integrated out the inputs (or <em>latent</em> variables they would be called in that case).</li>
</ul>
</section>
<section id="non-linear-on-inputs" class="slide level3" data-transition="None">
<h3>Non linear on Inputs</h3>
<ul>
<li>Set each activation function computed at each data point to be <span class="math display">\[
\phi_{i,j} = \phi(\mathbf{w}^{(1)}_{j}, {\bf x}_{i})
\]</span> Define <em>design matrix</em> <span class="math display">\[
\boldsymbol{\Phi}= 
\begin{bmatrix}
\phi_{1, 1} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, h} \\
\phi_{1, 2} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi_{n, 1} &amp; \phi_{n, 2} &amp; \dots &amp; \phi_{n, h}
\end{bmatrix}.
\]</span></li>
</ul>
</section>
<section id="matrix-representation-of-a-neural-network" class="slide level3" data-transition="None">
<h3>Matrix Representation of a Neural Network</h3>
<p><span class="math display">\[f\left({\bf x}\right) = \boldsymbol{\phi}\left({\bf x}\right)^\top \mathbf{w}+ \epsilon\]</span></p>
<div class="fragment">
<p><span class="math display">\[\mathbf{y}= \boldsymbol{\Phi}\mathbf{w}+ \boldsymbol{\epsilon}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\boldsymbol{\epsilon}\sim \mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right)\]</span></p>
</div>
</section>
<section id="prior-density" class="slide level3" data-transition="None">
<h3>Prior Density</h3>
<ul>
<li><p>Define <span class="math display">\[
\mathbf{w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right),
\]</span></p></li>
<li><p>Rules of multivariate Gaussians to see that, <span class="math display">\[
\mathbf{y}\sim \mathcal{N}\left(\mathbf{0},\alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top + \sigma^2 \mathbf{I}\right).
\]</span></p></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top + \sigma^2 \mathbf{I}.
\]</span></p>
</section>
<section id="joint-gaussian-density" class="slide level3" data-transition="None">
<h3>Joint Gaussian Density</h3>
<ul>
<li>Elements are a function <span class="math inline">\(k_{i,j} = k\left({\bf x}_i, {\bf x}_j\right)\)</span></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{\Phi}\boldsymbol{\Phi}^\top + \sigma^2 \mathbf{I}.
\]</span></p>
</section>
<section id="covariance-function" class="slide level3" data-transition="None">
<h3>Covariance Function</h3>
<p><span class="math display">\[
k_f\left({\bf x}_i, {\bf x}_j\right) = \alpha \boldsymbol{\phi}\left(\mathbf{W}_1, {\bf x}_i\right)^\top \boldsymbol{\phi}\left(\mathbf{W}_1, {\bf x}_j\right)
\]</span></p>
<ul>
<li>formed by inner products of the rows of the <em>design matrix</em>.</li>
</ul>
</section>
<section id="gaussian-process" class="slide level3" data-transition="None">
<h3>Gaussian Process</h3>
<ul>
<li><p>Instead of making assumptions about our density over each data point, <span class="math inline">\(y_i\)</span> as i.i.d.</p></li>
<li><p>make a joint Gaussian assumption over our data.</p></li>
<li><p>covariance matrix is now a function of both the parameters of the activation function, <span class="math inline">\(\mathbf{W}_1\)</span>, and the input variables, <span class="math inline">\({\bf X}\)</span>.</p></li>
<li><p>Arises from integrating out <span class="math inline">\(\mathbf{w}^{(2)}\)</span>.</p></li>
</ul>
</section>
<section id="basis-functions" class="slide level3" data-transition="None">
<h3>Basis Functions</h3>
<ul>
<li><p>Can be very complex, such as deep kernels, <span class="citation" data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or could even put a convolutional neural network inside.</p></li>
<li><p>Viewing a neural network in this way is also what allows us to beform sensible <em>batch</em> normalizations <span class="citation" data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</p></li>
</ul>
</section>
<section id="non-degenerate-gaussian-processes" class="slide level3" data-transition="None">
<h3>Non-degenerate Gaussian Processes</h3>
<ul>
<li><p>This process is <em>degenerate</em>.</p></li>
<li><p>Covariance function is of rank at most <span class="math inline">\(h\)</span>.</p></li>
<li><p>As <span class="math inline">\(n\rightarrow \infty\)</span>, covariance matrix is not full rank.</p></li>
<li><p>Leading to <span class="math inline">\(\left|\mathbf{K}\right| = 0\)</span></p></li>
</ul>
</section>
<section id="infinite-networks" class="slide level3" data-transition="None">
<h3>Infinite Networks</h3>
<ul>
<li>In ML Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> asked “what would happen if you took <span class="math inline">\(h\rightarrow \infty\)</span>?”</li>
</ul>
<p><a href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf"><img src="./diagrams/neal-infinite-priors.png" width="80%" style="background:none; border:none; box-shadow:none;" align="center"></a></p>
<p><em>Page 37 of Radford Neal’s 1994 thesis</em></p>
</section>
<section id="roughly-speaking" class="slide level3" data-transition="None">
<h3>Roughly Speaking</h3>
<ul>
<li><p>Instead of <span class="math display">\[\begin{align*}
k_f\left({\bf x}_i, {\bf x}_j\right) &amp; = \alpha \boldsymbol{\phi}\left(\mathbf{W}_1, {\bf x}_i\right)^\top \boldsymbol{\phi}\left(\mathbf{W}_1, {\bf x}_j\right)\\
&amp; = \alpha \sum_k \phi\left(\mathbf{w}^{(1)}_k, {\bf x}_i\right) \phi\left(\mathbf{w}^{(1)}_k, {\bf x}_j\right)
\end{align*}\]</span></p></li>
<li><p>Sample infinitely many from a prior density, <span class="math inline">\(p(\mathbf{w}^{(1)})\)</span>, <span class="math display">\[
k_f\left({\bf x}_i, {\bf x}_j\right) = \alpha \int \phi\left(\mathbf{w}^{(1)}, {\bf x}_i\right) \phi\left(\mathbf{w}^{(1)}, {\bf x}_j\right) p(\mathbf{w}^{(1)}) \text{d}\mathbf{w}^{(1)}
\]</span></p></li>
<li><p>Also applies for non-Gaussian <span class="math inline">\(p(\mathbf{w}^{(1)})\)</span> because of the <em>central limit theorem</em>.</p></li>
</ul>
</section>
<section id="simple-probabilistic-program" class="slide level3" data-transition="None">
<h3>Simple Probabilistic Program</h3>
<ul>
<li><p>If <span class="math display">\[\begin{align*}
\mathbf{w}^{(1)} &amp; \sim p(\cdot)\\
\phi_i &amp; = \phi\left(\mathbf{w}^{(1)}, {\bf x}_i\right), 
\end{align*}\]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a Gaussian process.</p></li>
</ul>
</section>
<section id="further-reading" class="slide level3" data-transition="None">
<h3>Further Reading</h3>
<ul>
<li><p>Chapter 2 of Neal’s thesis</p></li>
<li><p>Rest of Neal’s thesis.</p></li>
<li><p>David MacKay’s PhD thesis <span class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span></p></li>
</ul>
</section>
<section id="section-1" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/gp_prior_samples_few.svg">
</object>
</section>
<section id="section-2" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/gp_prior_samples.svg">
</object>
</section>
<section id="section-3" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-4" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/gp_rejection_samples.svg">
</object>
<!-- ### Two Dimensional Gaussian Distribution -->
<!-- include{../../_ml/includes/two_d_gaussian.md} -->
</section>
<section id="distributions-over-functions" class="slide level3" data-transition="none">
<h3>Distributions over Functions</h3>
</section>
<section id="sampling-a-function" class="slide level3" data-transition="none">
<h3>Sampling a Function</h3>
<p><strong>Multi-variate Gaussians</strong></p>
<ul>
<li><p>We will consider a Gaussian with a particular structure of covariance matrix.</p></li>
<li><p>Generate a single sample from this 25 dimensional Gaussian distribution, <span class="math inline">\(\mathbf{f}=\left[f_{1},f_{2}\dots f_{25}\right]\)</span>.</p></li>
<li><p>We will plot these points against their index.</p></li>
</ul>
</section>
<section id="gaussian-distribution-sample" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample000.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-1" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample001.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-2" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample002.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-3" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample003.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-4" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample004.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-5" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample005.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-6" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample006.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-7" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample007.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="gaussian-distribution-sample-8" class="slide level3" data-transition="none">
<h3>Gaussian Distribution Sample</h3>
<object data="../slides/diagrams/two_point_sample008.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_2-from-f_1" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{2}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample009.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_2-from-f_1-1" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{2}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample010.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_2-from-f_1-2" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{2}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample011.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_2-from-f_1-3" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{2}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample012.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_5-from-f_1" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{5}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample013.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_5-from-f_1-1" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{5}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample014.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_5-from-f_1-2" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{5}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample015.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="prediction-of-f_5-from-f_1-3" class="slide level3" data-transition="none">
<h3>Prediction of <span class="math inline">\(f_{5}\)</span> from <span class="math inline">\(f_{1}\)</span></h3>
<object data="../slides/diagrams/two_point_sample016.svg" class="svgplot">
</object>
<p>A 25 dimensional correlated random variable (values ploted against index)</p>
</section>
<section id="key-object" class="slide level3" data-transition="none">
<h3>Key Object</h3>
<ul>
<li><p>Covariance function, <span class="math inline">\(\mathbf{K}\)</span></p></li>
<li><p>Determines properties of samples.</p></li>
<li><p>Function of <span class="math inline">\({\bf X}\)</span>, <span class="math display">\[k_{i,j} = k({\bf x}_i, {\bf x}_j)\]</span></p></li>
</ul>
</section>
<section id="linear-algebra" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D({\bf x}_*) = \mathbf{k}({\bf x}_*, {\bf X}) \mathbf{K}^{-1}
\mathbf{y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{f}, *}\]</span></p></li>
</ul>
</section>
<section id="linear-algebra-1" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D({\bf x}_*) = \mathbf{k}({\bf x}_*, {\bf X}) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{f}, *}\]</span></p></li>
</ul>
</section>
<section id="section-5" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-6" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/gp_rejection_samples.svg">
</object>
</section>
<section id="section-7" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/gp_prediction.svg">
</object>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level3" data-transition="none">
<h3>Exponentiated Quadratic Covariance</h3>
<p><span class="math display">\[k(\mathbf{x}, \mathbf{x}^\prime) 
= \alpha \exp\left(-\frac{\left\Vert \mathbf{x} - \mathbf{x}^\prime\right\Vert^2_2}{2\ell^2}\right)\]</span></p>
<table>
<tr>
<td>
<object data="../slides/diagrams/eq_covariance.svg">
</object>
</td>
<td>
<img src="../slides/diagrams/eq_covariance.gif" class="negate" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td>
<ul>
<li><p>Gold medal times for Olympic Marathon since 1896.</p></li>
<li><p>Marathons before 1924 didn’t have a standardised distance.</p></li>
<li><p>Present results using pace per km.</p></li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<img data-src="../_ml/diagrams/Stephen_Kiprotich.jpg" alt="image" /> <small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data</h3>
<object data="../slides/diagrams/olympic-marathon.svg" class="svgplot">
</object>
</section>
<section id="olympic-marathon-data-2" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data</h3>
<object data="../slides/diagrams/olympic-marathon-gp.svg" class="svgplot">
</object>
</section>
<section id="basis-function-covariance" class="slide level3" data-transition="none">
<h3>Basis Function Covariance</h3>
<p><span class="math display">\[k(\mathbf{x}, \mathbf{x}^\prime) 
= \boldsymbol{\phi}(\mathbf{x})^\top \boldsymbol{\phi(\mathbf{x}^\prime)}\]</span></p>
<table>
<tr>
<td>
<object data="../slides/diagrams/basis_covariance.svg">
</object>
</td>
<td>
<img src="../slides/diagrams/basis_covariance.gif" class="negate" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="brownian-covariance" class="slide level3" data-transition="none">
<h3>Brownian Covariance</h3>
<p><span class="math display">\[k(t, t^\prime) = \alpha \min(t, t^\prime)\]</span></p>
<table>
<tr>
<td>
<object data="../slides/diagrams/brownian_covariance.svg">
</object>
</td>
<td>
<img src="../slides/diagrams/brownian_covariance.gif" class="negate" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="multilayer-perceptron-covariance" class="slide level3" data-transition="none">
<h3>Multilayer Perceptron Covariance</h3>
<p><span class="math display">\[k(\mathbf{x}, \mathbf{x}^\prime) = 
\alpha \arcsin\left(\frac{w \mathbf{x}^\top \mathbf{x}^\prime + b}
{\sqrt{\left(w \mathbf{x}^\top \mathbf{x} + b + 1\right)
\left(w \left.\mathbf{x}^\prime\right.^\top \mathbf{x}^\prime + b + 1\right)}}\right)\]</span></p>
<table>
<tr>
<td>
<object data="../slides/diagrams/mlp_covariance.svg">
</object>
</td>
<td>
<img src="../slides/diagrams/mlp_covariance.gif" class="negate" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="section-8" class="slide level3" data-transition="none">
<h3></h3>
<p><img src="../slides/diagrams/Planck_CMB.png" align="center" width="70%" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-9" class="slide level3" data-transition="none">
<h3></h3>
<div style="fontsize:120px;vertical-align:middle;">
<img src="../slides/diagrams/earth_PNG37.png" width="20%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(=f\Bigg(\)</span> <img src="../slides/diagrams/Planck_CMB.png"  width="50%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(\Bigg)\)</span>
</div>
</section>
<section id="deep-gaussian-processes" class="slide level3" data-transition="None">
<h3>Deep Gaussian Processes</h3>
</section>
<section id="approximations" class="slide level3" data-transition="None">
<h3>Approximations</h3>
<p><img src="../slides/diagrams/sparse-gps-1.png" align="center" width="90%" style="background:none; border:none; box-shadow:none;"></p>
<p><em>Image credit: Kai Arulkumaran</em></p>
</section>
<section id="approximations-1" class="slide level3" data-transition="None">
<h3>Approximations</h3>
<p><img src="../slides/diagrams/sparse-gps-2.png" align="center" width="90%" style="background:none; border:none; box-shadow:none;"></p>
<p><em>Image credit: Kai Arulkumaran</em></p>
</section>
<section id="approximations-2" class="slide level3" data-transition="None">
<h3>Approximations</h3>
<p><img src="../slides/diagrams/sparse-gps-3.png" align="center" width="45%" style="background:none; border:none; box-shadow:none;"></p>
<p><em>Image credit: Kai Arulkumaran</em></p>
</section>
<section id="approximations-3" class="slide level3" data-transition="None">
<h3>Approximations</h3>
<p><img src="../slides/diagrams/sparse-gps-4.png" align="center" width="45%" style="background:none; border:none; box-shadow:none;"></p>
<p><em>Image credit: Kai Arulkumaran</em></p>
</section>
<section id="full-gaussian-process-fit" class="slide level3" data-transition="None">
<h3>Full Gaussian Process Fit</h3>
<object data="../slides/diagrams/gp-approx-full.svg" class="svgplot">
</object>
</section>
<section id="inducing-variable-fit" class="slide level3" data-transition="None">
<h3>Inducing Variable Fit</h3>
<object data="../slides/diagrams/gp-approx-sparse-init.svg" class="svgplot">
</object>
</section>
<section id="inducing-variable-param-optimize" class="slide level3" data-transition="None">
<h3>Inducing Variable Param Optimize</h3>
<object data="../slides/diagrams/gp-approx-sparse-fit-1.svg" class="svgplot">
</object>
</section>
<section id="inducing-variable-full-optimize" class="slide level3" data-transition="None">
<h3>Inducing Variable Full Optimize</h3>
<object data="../slides/diagrams/gp-approx-sparse-fit-2.svg" class="svgplot">
</object>
</section>
<section id="full-gaussian-process-fit-1" class="slide level3" data-transition="None">
<h3>Full Gaussian Process Fit</h3>
<object data="../slides/diagrams/gp-approx-full.svg" class="svgplot">
</object>
</section>
<section id="mordern-review" class="slide level3">
<h3>Mordern Review</h3>
<ul>
<li><p><em>A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</em> <span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span></p></li>
<li><p><em>Deep Gaussian Processes and Variational Propagation of Uncertainty</em> <span class="citation" data-cites="Damianou:thesis2015">Damianou (2015)</span></p></li>
</ul>
</section>
<section id="deep-neural-network" class="slide level3" data-transition="None">
<h3>Deep Neural Network</h3>
<object data="../slides/diagrams/deep-nn1.svg" class="svgplot">
</object>
</section>
<section id="deep-neural-network-1" class="slide level3" data-transition="None">
<h3>Deep Neural Network</h3>
<object data="../slides/diagrams/deep-nn2.svg" class="svgplot">
</object>
</section>
<section id="mathematically" class="slide level3" data-transition="None">
<h3>Mathematically</h3>
<p><span class="math display">\[
\begin{align}
    \mathbf{h}_{1} &amp;= \phi\left(\mathbf{W}_1 {\bf x}\right)\\
    \mathbf{h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{h}_{1}\right)\\
    \mathbf{h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{h}_{2}\right)\\
    \mathbf{y}&amp;= \mathbf{w}_4 ^\top\mathbf{h}_{3}
\end{align}
\]</span></p>
</section>
<section id="overfitting" class="slide level3" data-transition="None">
<h3>Overfitting</h3>
<ul>
<li><p>Potential problem: if number of nodes in two adjacent layers is big, corresponding <span class="math inline">\(\mathbf{W}\)</span> is also very big and there is the potential to overfit.</p></li>
<li><p>Proposed solution: “dropout”.</p></li>
<li><p>Alternative solution: parameterize <span class="math inline">\(\mathbf{W}\)</span> with its SVD. <span class="math display">\[\mathbf{W}= \mathbf{U}\boldsymbol{\Lambda}\mathbf{V}^\top\]</span> or <span class="math display">\[\mathbf{W}= \mathbf{U}\mathbf{V}^\top\]</span> where if <span class="math inline">\(\mathbf{W}\in \Re^{k_1\times k_2}\)</span> then <span class="math inline">\(\mathbf{U}\in \Re^{k_1\times q}\)</span> and <span class="math inline">\(\mathbf{V}\in \Re^{k_2\times q}\)</span>, i.e. we have a low rank matrix factorization for the weights.</p></li>
</ul>
</section>
<section id="low-rank-approximation" class="slide level3">
<h3>Low Rank Approximation</h3>
<object data="../slides/diagrams/wisuvt.svg" class="svgplot">
</object>
</section>
<section id="deep-neural-network-2" class="slide level3" data-transition="None">
<h3>Deep Neural Network</h3>
<object data="./diagrams/deep-nn-bottleneck1.svg" class="svgplot">
</object>
</section>
<section id="deep-neural-network-3" class="slide level3" data-transition="None">
<h3>Deep Neural Network</h3>
<object data="./diagrams/deep-nn-bottleneck2.svg" class="svgplot">
</object>
</section>
<section id="mathematically-1" class="slide level3" data-transition="None">
<h3>Mathematically</h3>
<p><span class="math display">\[
\begin{align}
    \mathbf{z}_{1} &amp;= \mathbf{V}^\top_1 {\bf x}\\
    \mathbf{h}_{1} &amp;= \phi\left(\mathbf{U}_1 \mathbf{z}_{1}\right)\\
    \mathbf{z}_{2} &amp;= \mathbf{V}^\top_2 \mathbf{h}_{1}\\
    \mathbf{h}_{2} &amp;= \phi\left(\mathbf{U}_2 \mathbf{z}_{2}\right)\\
    \mathbf{z}_{3} &amp;= \mathbf{V}^\top_3 \mathbf{h}_{2}\\
    \mathbf{h}_{3} &amp;= \phi\left(\mathbf{U}_3 \mathbf{z}_{3}\right)\\
    \mathbf{y}&amp;= \mathbf{w}_4^\top\mathbf{h}_{3}
\end{align}
\]</span></p>
</section>
<section id="a-cascade-of-neural-networks" class="slide level3" data-transition="None">
<h3>A Cascade of Neural Networks</h3>
<p><span class="math display">\[\begin{align}
    \mathbf{z}_{1} &amp;= \mathbf{V}^\top_1 {\bf x}\\
    \mathbf{z}_{2} &amp;= \mathbf{V}^\top_2 \phi\left(\mathbf{U}_1 \mathbf{z}_{1}\right)\\
    \mathbf{z}_{3} &amp;= \mathbf{V}^\top_3 \phi\left(\mathbf{U}_2 \mathbf{z}_{2}\right)\\
    \mathbf{y}&amp;= \mathbf{w}_4 ^\top \mathbf{z}_{3}
\end{align}
\]</span></p>
</section>
<section id="cascade-of-gaussian-processes" class="slide level3" data-transition="None">
<h3>Cascade of Gaussian Processes</h3>
<ul>
<li><p>Replace each neural network with a Gaussian process <span class="math display">\[
\begin{align}
  \mathbf{z}_{1} &amp;= \mathbf{f}_1\left({\bf x}\right)\\
  \mathbf{z}_{2} &amp;= \mathbf{f}_2\left(\mathbf{z}_{1}\right)\\
  \mathbf{z}_{3} &amp;= \mathbf{f}_3\left(\mathbf{z}_{2}\right)\\
  \mathbf{y}&amp;= \mathbf{f}_4\left(\mathbf{z}_{3}\right)
\end{align}
\]</span></p></li>
<li><p>Equivalent to prior over parameters, take width of each layer to infinity.</p></li>
</ul>
</section>
<section id="mathematically-2" class="slide level3" data-transition="None">
<h3>Mathematically</h3>
<ul>
<li>Composite <em>multivariate</em> function <span class="math display">\[\mathbf{g}({\bf x})=\mathbf{f}_5(\mathbf{f}_4(\mathbf{f}_3(\mathbf{f}_2(\mathbf{f}_1({\bf x})))))\]</span></li>
</ul>
</section>
<section id="equivalent-to-markov-chain" class="slide level3" data-transition="None">
<h3>Equivalent to Markov Chain</h3>
<ul>
<li>Composite <em>multivariate</em> function</li>
</ul>
<p><span class="math display">\[p(\mathbf{y}|{\bf x})= p(\mathbf{y}|\mathbf{f}_5)p(\mathbf{f}_5|\mathbf{f}_4)p(\mathbf{f}_4|\mathbf{f}_3)p(\mathbf{f}_3|\mathbf{f}_2)p(\mathbf{f}_2|\mathbf{f}_1)p(\mathbf{f}_1|{\bf x})\]</span></p>
<object class="svgplot" data="../slides/diagrams/deep-markov.svg">
</object>
</section>
<section id="section-10" class="slide level3" data-transition="None">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/deep-markov-vertical.svg">
</object>
</section>
<section id="why-deep" class="slide level3" data-transition="None">
<h3>Why Deep?</h3>
<ul>
<li><p>Gaussian processes give priors over functions.</p></li>
<li><p>Elegant properties:</p>
<ul>
<li>e.g. <em>Derivatives</em> of process are also Gaussian distributed (if they exist).</li>
</ul></li>
<li><p>For particular covariance functions they are ‘universal approximators’, i.e. all functions can have support under the prior.</p></li>
<li><p>Gaussian derivatives might ring alarm bells.</p></li>
<li><p>E.g. a priori they don’t believe in function ‘jumps’.</p></li>
</ul>
</section>
<section id="process-composition" class="slide level3" data-transition="none">
<h3>Process Composition</h3>
<ul>
<li><p>From a process perspective: <em>process composition</em>.</p></li>
<li><p>A (new?) way of constructing more complex <em>processes</em> based on simpler components.</p></li>
</ul>
</section>
<section id="section-11" class="slide level3" data-transition="None">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/deep-markov-vertical.svg">
</object>
</section>
<section id="section-12" class="slide level3" data-transition="None">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/deep-markov-vertical-side.svg">
</object>
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level3" data-transition="None">
<h3>Difficulty for Probabilistic Approaches</h3>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<object class="svgplot" data="../slides/diagrams/nonlinear-mapping-3d-plot.svg" align="center">
</object>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level3" data-transition="None">
<h3>Difficulty for Probabilistic Approaches</h3>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<object class="svgplot" data="../slides/diagrams/nonlinear-mapping-2d-plot.svg" align="center">
</object>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level3" data-transition="None">
<h3>Difficulty for Probabilistic Approaches</h3>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<object class="svgplot" data="../slides/diagrams/gaussian-through-nonlinear.svg" align="center">
</object>
</section>
<section id="deep-gaussian-processes-1" class="slide level3">
<h3>Deep Gaussian Processes</h3>
<ul>
<li><p>Deep architectures allow abstraction of features <span class="citation" data-cites="Bengio:deep09 Hinton:fast06 Salakhutdinov:quantitative08">(Bengio, 2009; Hinton and Osindero, 2006; Salakhutdinov and Murray, 2008)</span></p></li>
<li><p>We use variational approach to stack GP models.</p></li>
</ul>
</section>
<section id="stacked-pca" class="slide level3" data-transition="None">
<h3>Stacked PCA</h3>
<object class="svgplot" data="../slides/diagrams/stack-pca-sample-0.svg">
</object>
</section>
<section id="stacked-pca-1" class="slide level3" data-transition="None">
<h3>Stacked PCA</h3>
<object class="svgplot" data="../slides/diagrams/stack-pca-sample-1.svg">
</object>
</section>
<section id="stacked-pca-2" class="slide level3" data-transition="None">
<h3>Stacked PCA</h3>
<object class="svgplot" data="../slides/diagrams/stack-pca-sample-2.svg">
</object>
</section>
<section id="stacked-pca-3" class="slide level3" data-transition="None">
<h3>Stacked PCA</h3>
<object class="svgplot" data="../slides/diagrams/stack-pca-sample-3.svg">
</object>
</section>
<section id="stacked-pca-4" class="slide level3" data-transition="None">
<h3>Stacked PCA</h3>
<object class="svgplot" data="../slides/diagrams/stack-pca-sample-4.svg">
</object>
</section>
<section id="stacked-gp" class="slide level3" data-transition="None">
<h3>Stacked GP</h3>
<object class="svgplot" data="../slides/diagrams/stack-gp-sample-0.svg">
</object>
</section>
<section id="stacked-gp-1" class="slide level3" data-transition="None">
<h3>Stacked GP</h3>
<object class="svgplot" data="../slides/diagrams/stack-gp-sample-1.svg">
</object>
</section>
<section id="stacked-gp-2" class="slide level3" data-transition="None">
<h3>Stacked GP</h3>
<object class="svgplot" data="../slides/diagrams/stack-gp-sample-2.svg">
</object>
</section>
<section id="stacked-gp-3" class="slide level3" data-transition="None">
<h3>Stacked GP</h3>
<object class="svgplot" data="../slides/diagrams/stack-gp-sample-3.svg">
</object>
</section>
<section id="stacked-gp-4" class="slide level3" data-transition="None">
<h3>Stacked GP</h3>
<object class="svgplot" data="../slides/diagrams/stack-gp-sample-4.svg">
</object>
</section>
<section id="analysis-of-deep-gps" class="slide level3" data-transition="None">
<h3>Analysis of Deep GPs</h3>
<ul>
<li><p><em>Avoiding pathologies in very deep networks</em> <span class="citation" data-cites="Duvenaud:pathologies14">Duvenaud et al. (2014)</span> show that the derivative distribution of the process becomes more <em>heavy tailed</em> as number of layers increase.</p></li>
<li><p><em>How Deep Are Deep Gaussian Processes?</em> <span class="citation" data-cites="Dunlop:deep2017">Dunlop et al. (2017)</span> perform a theoretical analysis possible through conditional Gaussian Markov property.</p></li>
</ul>
</section>
<section id="section-13" class="slide level3">
<h3></h3>
<div>
<iframe src="https://www.youtube.com/embed/XhIvygQYFFQ" width="1120" height="630" allowtransparency="true" frameborder="0">
</iframe>
</div>
</section>
<section id="olympic-marathon-data-3" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data</h3>
<object data="../slides/diagrams/olympic-marathon.svg" class="svgplot">
</object>
</section>
<section id="olympic-marathon-data-gp" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data GP</h3>
<object data="../slides/diagrams/olympic-marathon-gp.svg" class="svgplot">
</object>
</section>
<section id="olympic-marathon-data-deep-gp" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data Deep GP</h3>
<object data="../slides/diagrams/olympic-marathon-deep-gp.svg" class="svgplot">
</object>
</section>
<section id="olympic-marathon-data-latent-1" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data Latent 1</h3>
<object data="../slides/diagrams/olympic-marathon-deep-gp-layer-1.svg" class="svgplot">
</object>
</section>
<section id="olympic-marathon-data-latent-2" class="slide level3" data-transition="None">
<h3>Olympic Marathon Data Latent 2</h3>
<object data="../slides/diagrams/olympic-marathon-deep-gp-layer-2.svg" class="svgplot">
</object>
</section>
<section id="robot-wifi-data" class="slide level3" data-transition="None">
<h3>Robot WiFi Data</h3>
<object data="../slides/diagrams/robot-wireless.svg" class="svgplot">
</object>
</section>
<section id="robot-wifi-data-gp" class="slide level3" data-transition="None">
<h3>Robot WiFi Data GP</h3>
<object data="../slides/diagrams/robot-wireless-gp-dim-1.svg" class="svgplot">
</object>
</section>
<section id="robot-wifi-data-gp-1" class="slide level3" data-transition="None">
<h3>Robot WiFi Data GP</h3>
<object data="../slides/diagrams/robot-wireless-deep-gp-dim-1.svg" class="svgplot">
</object>
</section>
<section id="robot-wifi-data-latent-space" class="slide level3" data-transition="None">
<h3>Robot WiFi Data Latent Space</h3>
<object data="../slides/diagrams/robot-wireless-ground-truth.svg" class="svgplot">
</object>
</section>
<section id="robot-wifi-data-latent-space-1" class="slide level3" data-transition="None">
<h3>Robot WiFi Data Latent Space</h3>
<object data="../slides/diagrams/robot-wireless-latent-space.svg" class="svgplot">
</object>
</section>
<section id="motion-capture" class="slide level3" data-transition="none">
<h3>Motion Capture</h3>
<ul>
<li><p>‘High five’ data.</p></li>
<li><p>Model learns structure between two interacting subjects.</p></li>
</ul>
</section>
<section id="shared-lvm" class="slide level3" data-transition="none">
<h3>Shared LVM</h3>
<object class="svgplot" data="../slides/diagrams/shared.svg">
</object>
</section>
<section id="section-14" class="slide level3" data-transition="none">
<h3></h3>
<p><img src="../slides/diagrams/deep-gp-high-five2.png"  width="100%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="digits-data-set" class="slide level3" data-transition="none">
<h3>Digits Data Set</h3>
<ul>
<li><p>Are deep hierarchies justified for small data sets?</p></li>
<li><p>We can lower bound the evidence for different depths.</p></li>
<li><p>For 150 6s, 0s and 1s from MNIST we found at least 5 layers are required.</p></li>
</ul>
</section>
<section id="section-15" class="slide level3" data-transition="none">
<h3></h3>
<p><img src="../slides/diagrams/deep-gp-usps-digits.png"  width="100%" class="negate" align="cener" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-16" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/digit-samples-deep-gp.svg">
</object>
</section>
<section id="section-17" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/usps-digits-latent.svg">
</object>
</section>
<section id="section-18" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/usps-digits-hidden-1-0.svg">
</object>
</section>
<section id="section-19" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/usps-digits-hidden-2-0.svg">
</object>
</section>
<section id="section-20" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/usps-digits-hidden-3-0.svg">
</object>
</section>
<section id="section-21" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../slides/diagrams/usps-digits-hidden-4-0.svg">
</object>
</section>
<section id="deep-health" class="slide level3" data-transition="None">
<h3>Deep Health</h3>
<object class="svgplot" data="../slides/diagrams/deep-health.svg" align="center">
</object>
</section>
<section id="at-this-years-nips" class="slide level3">
<h3>At this Year’s NIPS</h3>
<ul>
<li><em>Gaussian process based nonlinear latent structure discovery in multivariate spike train data</em> <span class="citation" data-cites="Anqi:gpspike2017">Wu et al. (2017)</span></li>
<li><em>Doubly Stochastic Variational Inference for Deep Gaussian Processes</em> <span class="citation" data-cites="Salimbeni:doubly2017">Salimbeni and Deisenroth (2017)</span></li>
<li><em>Deep Multi-task {G}aussian Processes for Survival Analysis with Competing Risks</em> <span class="citation" data-cites="Alaa:deep2017">Alaa and van der Schaar (2017)</span></li>
<li><em>Counterfactual Gaussian Processes for Reliable Decision-making and What-if Reasoning</em> <span class="citation" data-cites="Schulam:counterfactual17">Schulam and Saria (2017)</span></li>
</ul>
</section>
<section id="some-other-works" class="slide level3">
<h3>Some Other Works</h3>
<ul>
<li><em>Deep Survival Analysis</em> <span class="citation" data-cites="Ranganath-survival16">Ranganath et al. (2016)</span></li>
<li><em>Recurrent Gaussian Processes</em> <span class="citation" data-cites="Mattos:recurrent15">Mattos et al. (2015)</span></li>
<li><em>Gaussian Process Based Approaches for Survival Analysis</em> <span class="citation" data-cites="Saul:thesis2016">Saul (2016)</span></li>
</ul>
</section>
<section id="multi-fidelity-modelling" class="slide level3">
<h3>Multi-fidelity Modelling</h3>
<ul>
<li><p>Deep nets are powerful approach to images, speech, language.</p></li>
<li><p>Proposal: Deep GPs may also be a great approach, but better to deploy according to natural strengths.</p></li>
</ul>
</section>
<section id="uncertainty-quantification" class="slide level3">
<h3>Uncertainty Quantification</h3>
<ul>
<li><p>Probabilistic numerics, surrogate modelelling, emulation, and UQ.</p></li>
<li><p>Not a fan of AI as a term.</p></li>
<li><p>But we are faced with increasing amounts of <em>algorithmic decision making</em>.</p></li>
</ul>
</section>
<section id="ml-and-decision-making" class="slide level3">
<h3>ML and Decision Making</h3>
<ul>
<li><p>When trading off decisions: compute or acquire data?</p></li>
<li><p>There is a critical need for uncertainty.</p></li>
</ul>
</section>
<section id="uncertainty-quantification-1" class="slide level3" data-transition="None">
<h3>Uncertainty Quantification</h3>
<blockquote>
<p>Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known.</p>
</blockquote>
<ul>
<li>Interaction between physical and virtual worlds of major interest for Amazon.</li>
</ul>
</section>
<section id="example-formula-one-racing" class="slide level3" data-transition="None">
<h3>Example: Formula One Racing</h3>
<ul>
<li><p>Designing an F1 Car requires CFD, Wind Tunnel, Track Testing etc.</p></li>
<li><p>How to combine them?</p></li>
</ul>
</section>
<section id="multi-fidelity-emulation" class="slide level3" data-transition="None">
<h3>Multi-Fidelity Emulation</h3>
<ul>
<li>Normally assume that <span class="citation" data-cites="Kandasamy:multifidelity16">(Kandasamy et al., 2016,<span class="citation" data-cites="Alonso:virtual17">Alonso et al. (2017)</span>)</span>:</li>
</ul>
<p><span class="math display">\[f_i\left({\bf x}\right) = \rho f_{i-1}\left({\bf x}\right) + \delta_i\left({\bf x}\right)\]</span></p>
</section>
<section id="deep-gp-emulation" class="slide level3" data-transition="None">
<h3>Deep GP Emulation</h3>
<p>But with Deep Gaussian processes <span class="citation" data-cites="Perdikaris:multifidelity17">(Perdikaris et al., 2017)</span> we can consider the form</p>
<p><span class="math display">\[f_i\left({\bf x}\right) = g_{i}\left(f_{i-1}\left({\bf x}\right)\right) + \delta_i\left({\bf x}\right),\]</span></p>
</section>
<section id="notebook-example" class="slide level3">
<h3>Notebook Example</h3>
</section>
<section id="acknowledgments" class="slide level3">
<h3>Acknowledgments</h3>
<p>Stefanos Eleftheriadis, John Bronskill, Hugh Salimbeni, Rich Turner, Zhenwen Dai, Javier Gonzalez, Andreas Damianou, Mark Pullin.</p>
</section>
<section id="ongoing-code" class="slide level3">
<h3>Ongoing Code</h3>
<ul>
<li><p>Powerful framework but</p></li>
<li><p>Software isn’t there yet.</p></li>
<li><p>Our focus: Gaussian Processes driven by MXNet</p></li>
<li><p>Composition of GPs, Neural Networks, Other Models</p></li>
</ul>
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: @lawrennd</li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered allowframebreaks">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Alaa:deep2017">
<p>Alaa, A.M., van der Schaar, M., 2017. Deep multi-task Gaussian processes for survival analysis with competing risks, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 2326–2334.</p>
</div>
<div id="ref-Alonso:virtual17">
<p>Alonso, M., Berkenkamp, F., Hennig, P., Schoellig, A.P., Krause, A., Schaal, S., Trimpe, S., 2017. Virtual vs. Real: Trading off simulations and physical experiments in reinforcement learning with Bayesian optimization, in: Proceedings of the Ieee International Conference on Robotics and Automation (Icra). pp. 1557–1563.</p>
</div>
<div id="ref-Bengio:deep09">
<p>Bengio, Y., 2009. Learning Deep Architectures for AI. Found. Trends Mach. Learn. 2, 1–127. <a href="https://doi.org/10.1561/2200000006" class="uri">https://doi.org/10.1561/2200000006</a></p>
</div>
<div id="ref-Thang:unifying17">
<p>Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for gaussian process pseudo-point approximations using power expectation propagation. Journal of Machine Learning Research 18, 1–72.</p>
</div>
<div id="ref-Cho:deep09">
<p>Cho, Y., Saul, L.K., 2009. Kernel methods for deep learning, in: Bengio, Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.</p>
</div>
<div id="ref-Damianou:thesis2015">
<p>Damianou, A., 2015. Deep gaussian processes and variational propagation of uncertainty (PhD thesis). University of Sheffield.</p>
</div>
<div id="ref-Dunlop:deep2017">
<p>Dunlop, M.M., Girolami, M., Stuart, A.M., Teckentrup, A.L., 2017. How Deep Are Deep Gaussian Processes? ArXiv e-prints.</p>
</div>
<div id="ref-Duvenaud:pathologies14">
<p>Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding pathologies in very deep networks, in: Kaski, S., Corander, J. (Eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research. PMLR, Reykjavik, Iceland, pp. 202–210.</p>
</div>
<div id="ref-Hinton:fast06">
<p>Hinton, G.E., Osindero, S., 2006. A fast learning algorithm for deep belief nets. Neural Computation 18, 2006.</p>
</div>
<div id="ref-Ioffe:batch15">
<p>Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: Bach, F., Blei, D. (Eds.), Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, Lille, France, pp. 448–456.</p>
</div>
<div id="ref-Kandasamy:multifidelity16">
<p>Kandasamy, K., Dasarathy, G., Oliva, J.B., Schneider, J., Poczos, B., 2016. Multi-fidelity Gaussian Process Bandit Optimisation. ArXiv e-prints.</p>
</div>
<div id="ref-MacKay:bayesian92">
<p>MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis). California Institute of Technology.</p>
</div>
<div id="ref-Mattos:recurrent15">
<p>Mattos, C.L.C., Dai, Z., Damianou, A.C., Forth, J., Barreto, G.A., Lawrence, N.D., 2015. Recurrent gaussian processes. CoRR abs/1511.06644.</p>
</div>
<div id="ref-McCulloch:neuron43">
<p>McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5, 115–133.</p>
</div>
<div id="ref-Neal:bayesian94">
<p>Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis). Dept. of Computer Science, University of Toronto.</p>
</div>
<div id="ref-Perdikaris:multifidelity17">
<p>Perdikaris, P., Raissi, M., Damianou, A., Lawrence, N.D., Karniadakis, G.E., 2017. Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences 473. <a href="https://doi.org/10.1098/rspa.2016.0751" class="uri">https://doi.org/10.1098/rspa.2016.0751</a></p>
</div>
<div id="ref-Ranganath-survival16">
<p>Ranganath, R., Perotte, A., Elhadad, N., Blei, D., 2016. Deep survival analysis, in: Doshi-Velez, F., Fackler, J., Kale, D., Wallace, B., Wiens, J. (Eds.), Proceedings of the 1st Machine Learning for Healthcare Conference, Proceedings of Machine Learning Research. PMLR, Children’s Hospital LA, Los Angeles, CA, USA, pp. 101–114.</p>
</div>
<div id="ref-Salakhutdinov:quantitative08">
<p>Salakhutdinov, R., Murray, I., 2008. On the quantitative analysis of deep belief networks, in: Roweis, S., McCallum, A. (Eds.), Proceedings of the International Conference in Machine Learning. Omnipress, pp. 872–879.</p>
</div>
<div id="ref-Salimbeni:doubly2017">
<p>Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational inference for deep gaussian processes, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 4591–4602.</p>
</div>
<div id="ref-Saul:thesis2016">
<p>Saul, A.D., 2016. Gaussian process based approaches for survival analysis (PhD thesis). University of Sheffield.</p>
</div>
<div id="ref-Schulam:counterfactual17">
<p>Schulam, P., Saria, S., 2017. Counterfactual gaussian processes for reliable decision-making and what-if reasoning, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 1696–1706.</p>
</div>
<div id="ref-Steele:predictive12">
<p>Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson, E., Avital, I., Stojadinovic, A., 2012. Using machine-learned Bayesian belief networks to predict perioperative risk of clostridium difficile infection following colon surgery. Interact J Med Res 1, e6. <a href="https://doi.org/10.2196/ijmr.2131" class="uri">https://doi.org/10.2196/ijmr.2131</a></p>
</div>
<div id="ref-Tipping:probpca99">
<p>Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a href="https://doi.org/doi:10.1111/1467-9868.00196" class="uri">https://doi.org/doi:10.1111/1467-9868.00196</a></p>
</div>
<div id="ref-Anqi:gpspike2017">
<p>Wu, A., Roy, N.G., Keeley, S., Pillow, J.W., 2017. Gaussian process based nonlinear latent structure discovery in multivariate spike train data, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 3499–3508.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
