<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Neil D. Lawrence">
  <title>DRAFT SLIDES: Deep Probabilistic Modelling with with Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">DRAFT SLIDES: Deep Probabilistic Modelling with with Gaussian Processes</h1>
  <p class="author">Neil D. Lawrence</p>
</section>

<section id="draft-slides" class="slide level3">
<h3>DRAFT SLIDES</h3>
<!-- Introduction to GPs -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \rightarrow \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{data}\)</span> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{model}\)</span> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><span class="math inline">\(\text{prediction}\)</span> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
</section>
<section id="model" class="slide level3">
<h3>Model</h3>
<ul>
<li><p>Mathematical Abstraction of Problem</p></li>
<li><p>Two functions</p>
<p>Prediction function <span class="math inline">\({f}(\cdot)\)</span></p>
<p>Objective function <span class="math inline">\(\objectiveFunction(\cdot)\)</span></p></li>
</ul>
</section>
<section id="probabilistic-inference" class="slide level3">
<h3>Probabilistic Inference</h3>
<p>Data: <span class="math inline">\({\mathbf{{y}}}\)</span></p>
<p>Model: <span class="math inline">\(p({\mathbf{{y}}}, {\mathbf{{y}}}^*)\)</span></p>
<p>Prediction: <span class="math inline">\(p({\mathbf{{y}}}^*| {\mathbf{{y}}})\)</span></p>
</section>
<section id="gaussian-process" class="slide level3">
<h3>Gaussian Process</h3>
<ul>
<li><p>Probabilistic model for functions</p>
<p><span class="math inline">\(p({\mathbf{{w}}})\)</span></p></li>
<li><p>Relate to data through a likelihood</p>
<p><span class="math inline">\(p({\mathbf{{y}}}| {\mathbf{{w}}})\)</span></p></li>
</ul>
</section>
<section id="section" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_few.svg">
</object>
</section>
<section id="section-1" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples.svg">
</object>
</section>
<section id="section-2" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-3" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_rejection_samples.svg">
</object>
</section>
<section id="key-object" class="slide level3" data-transition="none">
<h3>Key Object</h3>
<ul>
<li><p>Covariance function, <span class="math inline">\({\mathbf{K}}\)</span></p></li>
<li><p>Determines properties of samples.</p></li>
<li><p>Function of <span class="math inline">\({{\bf X}}\)</span>, <span class="math display">\[{k}_{i,j} = {k}({{\bf {x}}}_i, {{\bf {x}}}_j)\]</span></p></li>
</ul>
</section>
<section id="linear-algebra" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[{f}_D({{\bf {x}}}_*) = {\mathbf{{k}}}({{\bf {x}}}_*, {{\bf X}}) {\mathbf{K}}^{-1}
\mathbf{y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = {\mathbf{K}}_{*,*} - {\mathbf{K}}_{*,{\mathbf{{f}}}}
{\mathbf{K}}^{-1} {\mathbf{K}}_{{\mathbf{{f}}}, *}\]</span></p></li>
</ul>
</section>
<section id="linear-algebra-1" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[{f}_D({{\bf {x}}}_*) = {\mathbf{{k}}}({{\bf {x}}}_*, {{\bf X}}) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[{\mathbf{C}}_* = {\mathbf{K}}_{*,*} - {\mathbf{K}}_{*,{\mathbf{{f}}}}
{\mathbf{K}}^{-1} {\mathbf{K}}_{{\mathbf{{f}}}, *}\]</span></p></li>
</ul>
</section>
<section id="section-4" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-5" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_rejection_samples.svg">
</object>
</section>
<section id="section-6" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prediction.svg">
</object>
</section>
<section id="approximate-gaussian-processes" class="slide level3">
<h3>Approximate Gaussian Processes</h3>
</section>
<section id="other-limitations" class="slide level3">
<h3>Other Limitations</h3>
<ul>
<li>Joint Gaussianity is analytic, but not flexible.</li>
</ul>
</section>
<section id="efficient-computation" class="slide level3">
<h3>Efficient Computation</h3>
<ul>
<li>Thang and Turner paper</li>
</ul>
<!--frame start-->
</section>
<section id="variational-compression" class="slide level3">
<h3>Variational Compression</h3>

<ul>
<li><p>Complexity of standard GP:</p>
<ul>
<li><p><span class="math inline">\({\mathcal{O}}({n}^3)\)</span> in computation.</p></li>
<li><p><span class="math inline">\({\mathcal{O}}({n}^2)\)</span> in storage.</p></li>
</ul>
</li>
<li><p>Via low rank representations of covariance:</p>
<ul>
<li><p><span class="math inline">\({\mathcal{O}}({n}{m}^2)\)</span> in computation.</p></li>
<li><p><span class="math inline">\({\mathcal{O}}({n}{m})\)</span> in storage.</p></li>
</ul></li>
<li><p>Where <span class="math inline">\({m}\)</span> is user chosen number of <em>inducing</em> variables. They give the rank of the resulting covariance. </p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="variational-compression" class="slide level3">
<h3>Variational Compression</h3>
<ul>
<li><p>Inducing variables are a compression of the real observations.</p></li>
<li><p>They are like pseudo-data. They can be in space of <span class="math inline">\({\mathbf{{f}}}\)</span> or a space that is related through a linear operator [Alvarez:efficient10] — e.g. a gradient or convolution.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="variational-compression-ii" class="slide level3">
<h3>Variational Compression II</h3>
<ul>
<li><p><strong>Importantly</strong> conditioning on inducing variables renders the likelihood independent across the data.</p></li>
<li><p>It turns out that this allows us to variationally handle uncertainty on the kernel (including the inputs to the kernel).</p></li>
<li><p>It also allows standard scaling approaches: stochastic variational inference Hensman:bigdata13, parallelization Gal:Distributed14 and work by Zhenwen Dai on GPUs to be applied: an <em>engineering</em> challenge?</p></li>
</ul>
<!--frame end-->
<!--Parametric Bottleneck-->
<!--frame start-->
</section>
<section id="nonparametric-gaussian-processes" class="slide level3">
<h3>Nonparametric Gaussian Processes</h3>
<ul>
<li><p>We’ve seen how we go from parametric to non-parametric.</p></li>
<li><p>The limit implies infinite dimensional <span class="math inline">\({\mathbf{{w}}}\)</span>.</p></li>
<li><p>Gaussian processes are generally non-parametric: combine data with covariance function to get model.</p></li>
<li><p>This representation <em>cannot</em> be summarized by a parameter vector of a fixed size.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="the-parametric-bottleneck" class="slide level3">
<h3>The Parametric Bottleneck</h3>
<ul>
<li><p>Parametric models have a representation that does not respond to increasing training set size.</p></li>
<li><p>Bayesian posterior distributions over parameters contain the information about the training data.</p>
<ul>
<li><p>Use Bayes’ rule from training data, <span class="math inline">\(p\left({\mathbf{{w}}}|{\mathbf{{y}}}, {{\bf X}}\right)\)</span>,</p></li>
<li><p>Make predictions on test data <span class="math display">\[p\left({y}_*|{{\bf X}}_*, {\mathbf{{y}}}, {{\bf X}}\right) = \int
      p\left({y}_*|{\mathbf{{w}}},{{\bf X}}_*\right)p\left({\mathbf{{w}}}|{\mathbf{{y}}},
        {{\bf X}})\text{d}{\mathbf{{w}}}\right).\]</span></p></li>
</ul></li>
<li><p><span class="math inline">\({\mathbf{{w}}}\)</span> becomes a bottleneck for information about the training set to pass to the test set.</p></li>
<li><p>Solution: increase <span class="math inline">\({m}\)</span> so that the bottleneck is so large that it no longer presents a problem.</p></li>
<li><p>How big is big enough for <span class="math inline">\({m}\)</span>? Non-parametrics says <span class="math inline">\({m}\rightarrow \infty\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="the-parametric-bottleneck" class="slide level3">
<h3>The Parametric Bottleneck</h3>
<ul>
<li><p>Now no longer possible to manipulate the model through the standard parametric form. </p></li>
<li><p>However, it <em>is</em> possible to express <em>parametric</em> as GPs: <span class="math display">\[{k}\left({{\bf {x}}}_i,{{\bf {x}}}_j\right)={\phi}_:\left({{\bf {x}}}_i\right)^\top{\phi}_:\left({{\bf {x}}}_j\right).\]</span> </p></li>
<li><p>These are known as degenerate covariance matrices. </p></li>
<li><p>Their rank is at most <span class="math inline">\({m}\)</span>, non-parametric models have full rank covariance matrices. </p></li>
<li><p>Most well known is the “linear kernel”, <span class="math inline">\({k}({{\bf {x}}}_i, {{\bf {x}}}_j) = {{\bf {x}}}_i^\top{{\bf {x}}}_j\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="making-predictions" class="slide level3">
<h3>Making Predictions</h3>
<ul>
<li><p>For non-parametrics prediction at new points <span class="math inline">\({\mathbf{{f}}}_*\)</span> is made by conditioning on <span class="math inline">\({\mathbf{{f}}}\)</span> in the joint distribution. </p></li>
<li><p>In GPs this involves combining the training data with the covariance function and the mean function. </p></li>
<li><p>Parametric is a special case when conditional prediction can be summarized in a <em>fixed</em> number of parameters. </p></li>
<li><p>Complexity of parametric model remains fixed regardless of the size of our training data set. </p></li>
<li><p>For a non-parametric model the required number of parameters grows with the size of the training data.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="low-rank-motivation" class="slide level3">
<h3>Low Rank Motivation</h3>
<p><span>Inference in a GP has the following demands:</span></p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\({\mathcal{O}}({n}^3)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\({\mathcal{O}}({n}^2)\)</span></td>
</tr>
</tbody>
</table>
<p><span>Inference in a low rank GP has the following demands:</span></p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\({\mathcal{O}}({n}{m}^2)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\({\mathcal{O}}({n}{m})\)</span></td>
</tr>
</tbody>
</table>

<!--frame end-->
</section>
<section id="information-capture" class="slide level3">
<h3>Information capture</h3>
<p>[Everything we want to do with a GP involves marginalising <span class="math inline">\({\mathbf{{f}}}\)</span>]</p>
<ul>
<li><p>Predictions</p></li>
<li><p>Marginal likelihood</p></li>
<li><p>Estimating covariance parameters</p></li>
</ul>
<p>The posterior of <span class="math inline">\({\mathbf{{f}}}\)</span> is the central object. This means inverting <span class="math inline">\({{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\)</span>.</p>
<!--frame start-->
</section>
<section id="computational-savings" class="slide level3">
<h3>Computational Savings</h3>

<figure>
<img src="../../../gp/tex/diagrams/cov_approx" alt="image" style="width:90.0%" /><figcaption>image</figcaption>
</figure>
<p><span class="math display">\[{{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\approx {{\bf Q}_{{\mathbf{{f}}}{\mathbf{{f}}}}}= {{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{u}}}}}{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}^{-1}{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{f}}}}}\]</span></p>
<p>Instead of inverting <span class="math inline">\({{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\)</span>, we make a low rank (or Nyström) approximation, and invert <span class="math inline">\({{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}\)</span> instead.</p>

<!--frame end-->
<!--frame start-->
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[{{\bf X}},\,{\mathbf{{y}}}\]</span></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
</tbody>
</table>
<!--frame end-->
<!--frame start-->
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span><span class="math display">\[{{\bf X}},\,{\mathbf{{y}}}\]</span></span> <span class="math display">\[{\color{\blueColor} {f}({{\bf {x}}})} \sim {\mathcal GP}\]</span></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
</tbody>
</table>
<!--frame end-->
<!--frame start-->
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span><span class="math display">\[{{\bf X}},\,{\mathbf{{y}}}\]</span> <span class="math display">\[{f}({{\bf {x}}}) \sim {\mathcal GP}\]</span></span> <span class="math display">\[p({\color{\blueColor} {\mathbf{{f}}}}) = {\mathcal{N}\left({\mathbf{0}},{{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\right)}\]</span></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
</tbody>
</table>
<!--frame end-->
<!--frame start-->
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span><span class="math display">\[{{\bf X}},\,{\mathbf{{y}}}\]</span> <span class="math display">\[{f}({{\bf {x}}}) \sim {\mathcal GP}\]</span> <span class="math display">\[p({\mathbf{{f}}}) = {\mathcal{N}\left({\mathbf{0}},{{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\right)}\]</span></span> <span class="math display">\[p({\color{\blueColor} {\mathbf{{f}}}}{|}{\mathbf{{y}}},{{\bf X}})\]</span></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
</tbody>
</table>
<!--frame end-->
<!--frame start-->
</section>
<section id="introducing-mathbfu" class="slide level3">
<h3>Introducing <span class="math inline">\({\mathbf{{u}}}\)</span></h3>
<p>Take an extra <span class="math inline">\({m}\)</span> points on the function, <span class="math inline">\({\mathbf{{u}}}= {f}({\mathbf{\MakeUppercase{{z}}}})\)</span>. <span class="math display">\[p({\mathbf{{y}}},{\mathbf{{f}}},{\mathbf{{u}}}) = p({\mathbf{{y}}}{|}{\mathbf{{f}}}) p({\mathbf{{f}}}{|}{\mathbf{{u}}}) p({\mathbf{{u}}})\]</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="introducing-inducingvector" class="slide level3">
<h3>Introducing <span class="math inline">\({\mathbf{{u}}}\)</span></h3>
<p><img src="../../../gp/tex/diagrams/cov_inducing_withX" alt="image" /></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="introducing-inducingvector" class="slide level3">
<h3>Introducing <span class="math inline">\({\mathbf{{u}}}\)</span></h3>
<p>Take and extra <span class="math inline">\(M\)</span> points on the function, <span class="math inline">\({\mathbf{{u}}}= {f}({\mathbf{\MakeUppercase{{z}}}})\)</span>. <span class="math display">\[p({\mathbf{{y}}},{\mathbf{{f}}},{\mathbf{{u}}}) = p({\mathbf{{y}}}{|}{\mathbf{{f}}}) p({\mathbf{{f}}}{|}{\mathbf{{u}}}) p({\mathbf{{u}}})\]</span> <span class="math display">\[\begin{aligned}
    p({\mathbf{{y}}}{|}{\mathbf{{f}}}) &amp;= {\mathcal{N}\left({\mathbf{{y}}}|{\mathbf{{f}}},{\sigma}^2 {\mathbf{I}}\right)}\\
    p({\mathbf{{f}}}{|}{\mathbf{{u}}}) &amp;= {\mathcal{N}\left({\mathbf{{f}}}| {{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{u}}}}}{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}^{-1}{\mathbf{{u}}}, \Ktilde\right)}\\
    p({\mathbf{{u}}}) &amp;= {\mathcal{N}\left({\mathbf{{u}}}| {\mathbf{0}},{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}\right)}
  \end{aligned}\]</span></p>
<!--frame end-->
<!--frame start-->
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span><span class="math display">\[{{\bf X}},\,{\mathbf{{y}}}\]</span> <span class="math display">\[{f}({{\bf {x}}}) \sim {\mathcal GP}\]</span> <span class="math display">\[p({\mathbf{{f}}}) = {\mathcal{N}\left({\mathbf{0}},{{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\right)}\]</span> <span class="math display">\[p({\mathbf{{f}}}{|}{\mathbf{{y}}},{{\bf X}})\]</span></span> $$\begin{split}</td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: center;">&amp;, \</td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="odd">
<td style="text-align: center;">&amp;p({ }) = </td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="even">
<td style="text-align: center;">\end{split}$$</td>
<td style="text-align: center;"></td>
<td></td>
</tr>
</tbody>
</table>
<!--frame end-->
<!--frame start-->
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span><span class="math display">\[{{\bf X}},\,{\mathbf{{y}}}\]</span> <span class="math display">\[{f}({{\bf {x}}}) \sim {\mathcal GP}\]</span> <span class="math display">\[p({\mathbf{{f}}}) = {\mathcal{N}\left({\mathbf{0}},{{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\right)}\]</span> <span class="math display">\[p({\mathbf{{f}}}{|}{\mathbf{{y}}},{{\bf X}})\]</span> <span class="math display">\[p({\mathbf{{u}}})  = {\mathcal{N}\left({\mathbf{0}},{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}\right)}\]</span></span> <span class="math display">\[\widetilde p({\color{red}{\mathbf{{u}}}}{|}{\mathbf{{y}}},{{\bf X}})\]</span></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
</tbody>
</table>
<!--frame end-->
</section>
<section id="the-alternative-posterior" class="slide level3">
<h3>The alternative posterior</h3>
<p><span>Instead of doing</span> <span class="math display">\[p({\mathbf{{f}}}{|}{\mathbf{{y}}},{{\bf X}}) = \frac{p({\mathbf{{y}}}{|}{\mathbf{{f}}})p({\mathbf{{f}}}{|}{{\bf X}})}{\int p({\mathbf{{y}}}{|}{\mathbf{{f}}})p({\mathbf{{f}}}{|}{{\bf X}}){\text{d}{\mathbf{{f}}}}}\]</span> <span>We’ll do</span> <span class="math display">\[p({\mathbf{{u}}}{|}{\mathbf{{y}}},{\mathbf{\MakeUppercase{{z}}}}) = \frac{p({\mathbf{{y}}}{|}{\mathbf{{u}}})p({\mathbf{{u}}}{|}{\mathbf{\MakeUppercase{{z}}}})}{\int p({\mathbf{{y}}}{|}{\mathbf{{u}}})p({\mathbf{{u}}}{|}{\mathbf{\MakeUppercase{{z}}}}){\text{d}{\mathbf{{u}}}}}\]</span> </p>
<!--Flexible Parametric Approximation-->
<!--frame start-->
</section>
<section id="parametric-but-non-parametric" class="slide level3">
<h3>Parametric but Non-parametric</h3>
<ul>
<li><p>Augment with a vector of <em>inducing</em> variables, <span class="math inline">\({\mathbf{{u}}}\)</span>.</p></li>
<li><p>Form a variational lower bound on true likelihood.</p></li>
<li><p>Bound <em>factorizes</em> given inducing variables.</p></li>
<li><p>Inducing variables appear in bound similar to parameters in a parametric model.</p></li>
<li><p><em>But</em> number of inducing variables can be changed at run time.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="inducing-variable-approximations" class="slide level3">
<h3>Inducing Variable Approximations</h3>
<ul>
<li><p>Date back to <span></span>. See <span></span> for a review.</p></li>
<li><p>We follow variational perspective of <span></span>.</p></li>
<li><p>This is an augmented variable method, followed by a collapsed variational approximation <span></span>.</p></li>
</ul>
<!--frame end-->
<!--frame failure start-->


<!--frame failure end-->
<!--frame start-->
</section>
<section id="variational-bound-on-pmathbfymathbfu" class="slide level3">
<h3>Variational Bound on <span class="math inline">\(p({\mathbf{{y}}}|{\mathbf{{u}}})\)</span></h3>
<p><span class="math display">\[\begin{aligned}
    \log p({\mathbf{{y}}}|{\mathbf{{u}}}) &amp; = \log \int p({\mathbf{{y}}}|{\mathbf{{f}}}) p({\mathbf{{f}}}|{\mathbf{{u}}}) \text{d}{\mathbf{{f}}}\\ &amp; = \int q({\mathbf{{f}}}) \log \frac{p({\mathbf{{y}}}|{\mathbf{{f}}}) p({\mathbf{{f}}}|{\mathbf{{u}}})}{q({\mathbf{{f}}})}\text{d}{\mathbf{{f}}}+ {\text{KL}\left( q({\mathbf{{f}}})\,\|\,p({\mathbf{{f}}}|{\mathbf{{y}}}, {\mathbf{{u}}}) \right)}
  \end{aligned}\]</span></p>
<p></p>
<ul>
<li>Example, set <span class="math inline">\(q({\mathbf{{f}}})=p({\mathbf{{f}}}|{\mathbf{{u}}})\)</span>, <span class="math display">\[\log p({\mathbf{{y}}}|{\mathbf{{u}}}) \geq \log \int p({\mathbf{{f}}}|{\mathbf{{u}}}) \log p({\mathbf{{y}}}|{\mathbf{{f}}})\text{d}{\mathbf{{f}}}.\]</span> <span class="math display">\[p({\mathbf{{y}}}|{\mathbf{{u}}}) \geq \exp \int p({\mathbf{{f}}}|{\mathbf{{u}}}) \log p({\mathbf{{y}}}|{\mathbf{{f}}})\text{d}{\mathbf{{f}}}.\]</span></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="optimal-compression-in-inducing-variables" class="slide level3">
<h3>Optimal Compression in Inducing Variables</h3>
<ul>
<li><p>Maximizing lower bound minimizes the KL divergence (information gain): <span class="math display">\[{\text{KL}\left( p({\mathbf{{f}}}|{\mathbf{{u}}})\,\|\,p({\mathbf{{f}}}|{\mathbf{{y}}}, {\mathbf{{u}}}) \right)} = \int p({\mathbf{{f}}}|{\mathbf{{u}}}) \log \frac{p({\mathbf{{f}}}|{\mathbf{{u}}})}{p({\mathbf{{f}}}|{\mathbf{{y}}}, {\mathbf{{u}}})}\text{d}{\mathbf{{u}}}\]</span></p></li>
<li><p>This is minimized when the information stored about <span class="math inline">\({\mathbf{{y}}}\)</span> is stored already in <span class="math inline">\({\mathbf{{u}}}\)</span>.</p></li>
<li><p>The bound seeks an <em>optimal compression</em> from the <em>information gain</em> perspective.</p></li>
<li><p>If <span class="math inline">\({\mathbf{{u}}}= {\mathbf{{f}}}\)</span> bound is exact (<span class="math inline">\({\mathbf{{f}}}\)</span> <span class="math inline">\(d\)</span>-separates <span class="math inline">\({\mathbf{{y}}}\)</span> from <span class="math inline">\({\mathbf{{u}}}\)</span>).</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="choice-of-inducing-variables" class="slide level3">
<h3>Choice of Inducing Variables</h3>
<ul>
<li><p>Optimizing the bound directly not always practical.</p></li>
<li><p>Free to choose whatever heuristics for the inducing variables.</p></li>
<li><p>Can quantify which heuristics perform better through checking lower bound.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="factorizing-likelihoods" class="slide level3">
<h3>Factorizing Likelihoods</h3>
<ul>
<li><p>If the likelihood, <span class="math inline">\(p({\mathbf{{y}}}|{\mathbf{{f}}})\)</span>, factorizes     </p></li>
<li><p>&lt;8-&gt; Then the bound factorizes.</p></li>
<li><p>&lt;10-&gt; Now need a choice of distributions for <span class="math inline">\({\mathbf{{f}}}\)</span> and <span class="math inline">\({\mathbf{{y}}}|{\mathbf{{f}}}\)</span> …</p></li>
</ul>
<!--frame end-->
</section>
<section id="section-7" class="slide level3">
<h3></h3>
<p><span class="math display">\[{\mathbf{{f}}}, {\mathbf{{u}}}\sim {\mathcal{N}\left(\mathbf{0},\begin{bmatrix}{{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}&amp; {{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{u}}}}}\\{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{f}}}}}&amp; {{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}\end{bmatrix}\right)}\]</span> <span class="math display">\[{\mathbf{{y}}}|{\mathbf{{f}}}= \prod_{i} {\mathcal{N}\left({f},{\sigma}^2\right)}\]</span></p>
<!--Variational Compression-->
<!--frame start-->
</section>
<section id="gaussian-py_if_i" class="slide level3">
<h3>Gaussian <span class="math inline">\(p({y}_i|{f}_i)\)</span></h3>
<p>For Gaussian likelihoods:  </p>
<!--frame end-->
<!--frame start-->
</section>
<section id="gaussian-process-over-mathbff-and-mathbfu" class="slide level3">
<h3>Gaussian Process Over <span class="math inline">\({\mathbf{{f}}}\)</span> and <span class="math inline">\({\mathbf{{u}}}\)</span></h3>
<p>Define: <span class="math display">\[q_{i, i} = {\text{var}_{p({f}_i|{\mathbf{{u}}})}\left( {f}_i \right)} = {\left&lt;{f}_i^2\right&gt;_{p({f}_i|{\mathbf{{u}}})}} - {\left&lt;{f}_i\right&gt;_{p({f}_i|{\mathbf{{u}}})}}^2\]</span> We can write: <span class="math display">\[c_i = \exp\left(-{\frac{q_{i,i}}{2{\sigma}^2}}\right)\]</span> If joint distribution of <span class="math inline">\(p({\mathbf{{f}}}, {\mathbf{{u}}})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} = {k}_{i, i} - {\mathbf{{k}}}_{i, {\mathbf{{u}}}}^\top {\mathbf{K}}_{{\mathbf{{u}}}, {\mathbf{{u}}}}^{-1} {\mathbf{{k}}}_{i, {\mathbf{{u}}}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span class="math inline">\({\mathbf{{u}}}\)</span> but <em>is</em> a function of <span class="math inline">\({{\bf X}}_{\mathbf{{u}}}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="total-conditional-variance" class="slide level3">
<h3>Total Conditional Variance</h3>
<ul>
<li><p>The sum of <span class="math inline">\(q_{i,i}\)</span> is the <em>total conditional variance</em>.</p></li>
<li><p>If conditional density <span class="math inline">\(p({\mathbf{{f}}}|{\mathbf{{u}}})\)</span> is Gaussian then it has covariance <span class="math display">\[\mathbf{Q} = {\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}} - {\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{u}}}}{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}^{-1} {\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{f}}}}\]</span></p></li>
<li><p><span class="math inline">\({\text{tr}\left(\mathbf{Q}\right)} = \sum_{i}q_{i,i}\)</span> is known as total variance.</p></li>
<li><p>Because it is on conditional distribution we call it <em>total conditional variance</em>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="capacity-of-a-density" class="slide level3">
<h3>Capacity of a Density</h3>
<ul>
<li><p>Measure the ’capacity of a density’.</p></li>
<li><p>Determinant of covariance represents ’volume’ of density.</p></li>
<li><p>log determinant is entropy: sum of <em>log</em> eigenvalues of covariance.</p></li>
<li><p>trace of covariance is total variance: sum of eigenvalues of covariance.</p></li>
<li><p><span class="math inline">\(\lambda &gt; \log \lambda\)</span> then total conditional variance upper bounds entropy.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="alternative-view" class="slide level3">
<h3>Alternative View</h3>
<p>Exponentiated total variance bounds determinant. <span class="math display">\[{\left|\mathbf{Q}\right|} &lt; \exp {\text{tr}\left(\mathbf{Q}\right)}\]</span> Because <span class="math display">\[\prod_{i=1}^k \lambda_i &lt; \prod_{i=1}^k \exp(\lambda_i)\]</span> where <span class="math inline">\(\{\lambda_i\}_{i=1}^k\)</span> are the <em>positive</em> eigenvalues of <span class="math inline">\(\mathbf{Q}\)</span> This in turn implies <span class="math display">\[{\left|\mathbf{Q}\right|} &lt; \prod_{i=1}^k \exp\left(q_{i,i}\right)\]</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="communication-channel" class="slide level3">
<h3>Communication Channel</h3>
<ul>
<li><p>Conditional density <span class="math inline">\(p({\mathbf{{f}}}|{\mathbf{{u}}})\)</span> can be seen as a <em>communication channel</em>.</p></li>
<li><p>Normally we have: <span><span class="math display">\[\text{Transmitter} \xrightarrow{{\mathbf{{u}}}} \begin{smallmatrix}p({\mathbf{{f}}}|{\mathbf{{u}}}) \\ \text{Channel}\end{smallmatrix} \xrightarrow{{\mathbf{{f}}}} \text{Receiver}\]</span></span> and we control <span class="math inline">\(p({\mathbf{{u}}})\)</span> (the source density).</p></li>
<li><p><em>Here</em> we can also control the transmission channel <span class="math inline">\(p({\mathbf{{f}}}|{\mathbf{{u}}})\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="lower-bound-on-likelihood" class="slide level3">
<h3>Lower Bound on Likelihood</h3>
<p>Substitute variational bound into marginal likelihood: <span class="math display">\[p({\mathbf{{y}}})\geq \prod_{i=1}^{n}c_i \int {\mathcal{N}\left({\mathbf{{y}}}|{\left&lt;{\mathbf{{f}}}\right&gt;},{\sigma}^2{\mathbf{I}}\right)}p({\mathbf{{u}}}) \text{d}{\mathbf{{u}}}\]</span> Note that: <span class="math display">\[{\left&lt;{\mathbf{{f}}}\right&gt;_{p({\mathbf{{f}}}|{\mathbf{{u}}})}} = {\mathbf{K}}_{{\mathbf{{f}}}, {\mathbf{{u}}}} {\mathbf{K}}_{{\mathbf{{u}}}, {\mathbf{{u}}}}^{-1}{\mathbf{{u}}}\]</span> is <em>linearly</em> dependent on <span class="math inline">\({\mathbf{{u}}}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="deterministic-training-conditional" class="slide level3">
<h3>Deterministic Training Conditional</h3>
<p>Making the marginalization of <span class="math inline">\({\mathbf{{u}}}\)</span> straightforward. In the Gaussian case: <span class="math display">\[p({\mathbf{{u}}}) = {\mathcal{N}\left({\mathbf{{u}}}|{\mathbf{0}},{\mathbf{K}}_{{\mathbf{{u}}},{\mathbf{{u}}}}\right)}\]</span>      </p>
<!--frame end-->
<!--frame start-->
</section>
<section id="variational-marginalisation-of-mathbff" class="slide level3">
<h3>Variational marginalisation of <span class="math inline">\({\mathbf{{f}}}\)</span></h3>
<p><span class="math display">\[\log p({\mathbf{{y}}}{|}{\mathbf{{u}}}) = \log\int p({\mathbf{{y}}}{|}{\mathbf{{f}}})p({\mathbf{{f}}}{|}{\mathbf{{u}}},{{\bf X}})\text{d}{\mathbf{{f}}}\]</span></p>
<p><span class="math display">\[\log p({\mathbf{{y}}}{|}{\mathbf{{u}}}) = \log \mathbb{E}_{p({\mathbf{{f}}}{|}{\mathbf{{u}}},{{\bf X}})}\left[p({\mathbf{{y}}}{|}{\mathbf{{f}}})\right]\]</span> <span class="math display">\[\log p({\mathbf{{y}}}{|}{\mathbf{{u}}}) \geq  \mathbb{E}_{p({\mathbf{{f}}}{|}{\mathbf{{u}}},{{\bf X}})}\left[\log p({\mathbf{{y}}}{|}{\mathbf{{f}}})\right]\triangleq \log\widetilde p({\mathbf{{y}}}{|}{\mathbf{{u}}})\]</span> </p>
<p><span> No inversion of <span class="math inline">\({{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\)</span> required</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="variational-marginalisation-of-mathbff-another-way" class="slide level3">
<h3>Variational marginalisation of <span class="math inline">\({\mathbf{{f}}}\)</span> (another way)</h3>
<p> <span class="math display">\[p({\mathbf{{y}}}{|}{\mathbf{{u}}}) = \frac{p({\mathbf{{y}}}{|}{\mathbf{{f}}})p({\mathbf{{f}}}{|}{\mathbf{{u}}})}{p({\mathbf{{f}}}{|}{\mathbf{{y}}}, {\mathbf{{u}}})}\]</span> <span class="math display">\[\log p({\mathbf{{y}}}{|}{\mathbf{{u}}}) = \log p({\mathbf{{y}}}{|}{\mathbf{{f}}}) + \log \frac{p({\mathbf{{f}}}{|}{\mathbf{{u}}})}{p({\mathbf{{f}}}{|}{\mathbf{{y}}}, {\mathbf{{u}}})}\]</span> <span class="math display">\[\log p({\mathbf{{y}}}{|}{\mathbf{{u}}}) = \bbE_{p({\mathbf{{f}}}{|}{\mathbf{{u}}})}\big[\log p({\mathbf{{y}}}{|}{\mathbf{{f}}})\big] + \bbE_{p({\mathbf{{f}}}{|}{\mathbf{{u}}})}\big[\log \frac{p({\mathbf{{f}}}{|}{\mathbf{{u}}})}{p({\mathbf{{f}}}{|}{\mathbf{{y}}}, {\mathbf{{u}}})}\big]\]</span> <span class="math display">\[\log p({\mathbf{{y}}}{|}{\mathbf{{u}}}) = \widetilde p({\mathbf{{y}}}{|}{\mathbf{{u}}}) + \textsc{KL}[p({\mathbf{{f}}}|{\mathbf{{u}}})||p({\mathbf{{f}}}{|}{\mathbf{{y}}}, {\mathbf{{u}}})]\]</span></p>
<p><span> No inversion of <span class="math inline">\({{\mathbf{K}}_{{\mathbf{{f}}}{\mathbf{{f}}}}}\)</span> required</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="a-lower-bound-on-the-likelihood" class="slide level3">
<h3>A Lower Bound on the Likelihood</h3>
<p><span class="math display">\[\widetilde p({\mathbf{{y}}}{|}{\mathbf{{u}}})  = \prod_{i=1}^{n}\widetilde p({y}_i{|}{\mathbf{{u}}})\]</span> <span class="math display">\[\widetilde p({y}{|}{\mathbf{{u}}}) = {\mathcal{N}\left({y}|{{\mathbf{{k}}}_{{f}{u}}}{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}^{-1}{\mathbf{{u}}},{\sigma}^2\right)} \,{\color{red}\exp\left\{-\tfrac{1}{2{\sigma}^2}\left({{k}_{{f}{f}}}- {{\mathbf{{k}}}_{{f}{u}}}{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}^{-1}{{\mathbf{{k}}}_{{u}{f}}}\right)\right\}}\]</span></p>
<p><span>A straightforward likelihood approximation, and a penalty term</span></p>
<!--frame end-->
<!--frame start-->
</section>
<section id="now-we-can-marginalise-mathbfu" class="slide level3">
<h3>Now we can marginalise <span class="math inline">\({\mathbf{{u}}}\)</span></h3>
<p><span class="math display">\[\widetilde p({\mathbf{{u}}}{|}{\mathbf{{y}}},{\mathbf{\MakeUppercase{{z}}}}) = \frac{\widetilde p({\mathbf{{y}}}{|}{\mathbf{{u}}})p({\mathbf{{u}}}{|}{\mathbf{\MakeUppercase{{z}}}})}{\int \widetilde p({\mathbf{{y}}}{|}{\mathbf{{u}}})p({\mathbf{{u}}}{|}{\mathbf{\MakeUppercase{{z}}}})\dif{{\mathbf{{u}}}}}\]</span></p>
<ul>
<li><p>Computing the posterior costs <span class="math inline">\({\mathcal{O}}({n}{m}^2)\)</span></p></li>
<li><p>We also get a lower bound of the marginal likelihood</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="what-does-the-penalty-term-do" class="slide level3">
<h3>What does the penalty term do?</h3>
<p><span class="math display">\[{\color{red}\sum_{i=1}^{n}-\tfrac{1}{2{\sigma}^2}\left({{k}_{{f}{f}}}- {{\mathbf{{k}}}_{{f}{u}}}{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}^{-1}{{\mathbf{{k}}}_{{u}{f}}}\right)}\]</span></p>

<!--frame end-->
<!--frame start-->
</section>
<section id="what-does-the-penalty-term-do" class="slide level3">
<h3>What does the penalty term do?</h3>
<p><span class="math display">\[{\color{red}\sum_{i=1}^{n}-\tfrac{1}{2{\sigma}^2}\left({{k}_{{f}{f}}}- {{\mathbf{{k}}}_{{f}{u}}}{{\mathbf{K}}_{{\mathbf{{u}}}{\mathbf{{u}}}}}^{-1}{{\mathbf{{k}}}_{{u}{f}}}\right)}\]</span></p>

<!--frame end-->
<!--frame start-->
</section>
<section id="what-does-the-penalty-term-do" class="slide level3">
<h3>What does the penalty term do?</h3>
<figure>
<img src="../../../gp/tex/diagrams/cov_approx" alt="image" style="width:60.0%" /><figcaption>image</figcaption>
</figure>
<figure>
<img src="../../../gp/tex/diagrams/cov_approx_opt" alt="image" style="width:60.0%" /><figcaption>image</figcaption>
</figure>
<!--frame end-->
<!--frame start-->
</section>
<section id="how-good-is-the-inducing-approximation" class="slide level3">
<h3>How good is the inducing approximation?</h3>
<p><span>It’s easy to show that as <span class="math inline">\({\mathbf{\MakeUppercase{{z}}}}\to {{\bf X}}\)</span>:</span></p>
<ul>
<li><p><span class="math inline">\({\mathbf{{u}}}\to {\mathbf{{f}}}\)</span> (and the posterior is exact)</p></li>
<li><p>The penalty term is zero.</p></li>
<li><p>The cost returns to <span class="math inline">\({\mathcal{O}}({n}^3)\)</span></p></li>
</ul>

<ul>
<li><br />
</li>
<li></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="predictions" class="slide level3">
<h3>Predictions</h3>

<!--frame end-->
<!--frame start-->
</section>
<section id="recap" class="slide level3">
<h3>Recap</h3>
<p><span>So far we:</span></p>
<ul>
<li><p>introduced <span class="math inline">\({\mathbf{\MakeUppercase{{z}}}}, {\mathbf{{u}}}\)</span></p></li>
<li><p>approximated the intergral over <span class="math inline">\({\mathbf{{f}}}\)</span> variationally</p></li>
<li><p>captured the information in <span class="math inline">\(\widetilde p({\mathbf{{u}}}{|}{\mathbf{{y}}})\)</span></p></li>
<li><p>obtained a lower bound on the marginal likeihood</p></li>
<li><p>saw the effect of the penalty term</p></li>
<li><p>prediction for new points</p></li>
</ul>
<p><span>Omitted details:</span></p>
<ul>
<li><p>optimization of the covariance parameters using the bound</p></li>
<li><p>optimization of Z (simultaneously)</p></li>
<li><p>the form of <span class="math inline">\(\widetilde p({\mathbf{{u}}}{|}{\mathbf{{y}}})\)</span></p></li>
<li><p>historical approximations</p></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="other-approximations" class="slide level3">
<h3>Other approximations</h3>
<p><span>Subset selection</span> </p>
<ul>
<li><p>Random or systematic</p></li>
<li><p>Set <span class="math inline">\({\mathbf{\MakeUppercase{{z}}}}\)</span> to subset of <span class="math inline">\({{\bf X}}\)</span></p></li>
<li><p>Set <span class="math inline">\({\mathbf{{u}}}\)</span> to subset of <span class="math inline">\({\mathbf{{f}}}\)</span></p></li>
<li><p>Approximation to <span class="math inline">\(p({\mathbf{{y}}}{|}{\mathbf{{u}}})\)</span>:</p>
<ul>
<li><p>$ p(_i) = p(_i_i) i$</p></li>
<li><p>$ p(_i) = 1  i$</p></li>
</ul></li>
</ul>
<!--frame end-->
<!--frame start-->
</section>
<section id="other-approximations" class="slide level3">
<h3>Other approximations</h3>

<ul>
<li><p>Approximation to <span class="math inline">\(p({\mathbf{{y}}}{|}{\mathbf{{u}}})\)</span>:</p>
<ul>
<li>$ p(_i) = (_i, [_i])$</li>
</ul></li>
<li><p>As our variational formulation, but without penalty</p></li>
</ul>
<p>Optimization of <span class="math inline">\({\mathbf{\MakeUppercase{{z}}}}\)</span> is difficult</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="other-approximations" class="slide level3">
<h3>Other approximations</h3>
<p><span>Fully Independent Training Conditional</span> </p>
<ul>
<li><p>Approximation to <span class="math inline">\(p({\mathbf{{y}}}{|}{\mathbf{{u}}})\)</span>:</p></li>
<li><p>$ p() = _ip(_i) $</p></li>
</ul>
<p>Optimization of <span class="math inline">\({\mathbf{\MakeUppercase{{z}}}}\)</span> is still difficult, and there are some weird heteroscedatic effects</p>
<!--frame end-->
<!--frame start-->
<p>[label=bayesGplvm]</p>
</section>
<section id="selecting-data-dimensionality" class="slide level3">
<h3>Selecting Data Dimensionality</h3>
<ul>
<li><p>GP-LVM Provides probabilistic non-linear dimensionality reduction.</p></li>
<li><p>How to select the dimensionality?</p></li>
<li><p>Need to estimate marginal likelihood.</p></li>
<li><p>In standard GP-LVM it increases with increasing <span class="math inline">\({q}\)</span>.</p></li>
</ul>
<!--frame end-->
<!--frame failure start-->


<!--frame failure end-->
</section>
<section id="standard-variational-approach-fails" class="slide level3">
<h3>Standard Variational Approach Fails</h3>
<ul>
<li><p>&lt;1-&gt; Standard variational bound has the form: <span class="math display">\[{\mathcal{L}}= {\left&lt;\log p({\mathbf{{y}}}|{\mathbf{Z}})\right&gt;_{q({\mathbf{Z}})}} + {\text{KL}\left( q({\mathbf{Z}})\,\|\,p({\mathbf{Z}}) \right)}\]</span></p></li>
<li><p>&lt;2-&gt; Requires expectation of <span class="math inline">\(\log p({\mathbf{{y}}}|{\mathbf{Z}})\)</span> under <span class="math inline">\(q({\mathbf{Z}})\)</span>. <span class="math display">\[\log p({\mathbf{{y}}}|{\mathbf{Z}}) = -\frac{1}{2}{\mathbf{{y}}}^\top\left({\mathbf{K}}_{{\mathbf{{f}}}, {\mathbf{{f}}}}+{\sigma}^2{\mathbf{I}}\right)^{-1}{\mathbf{{y}}}-\frac{1}{2}\log {\left|{\mathbf{K}}_{{\mathbf{{f}}}, {\mathbf{{f}}}}+{\sigma}^2 {\mathbf{I}}\right|} -\frac{{n}}{2}\log 2\pi\]</span></p></li>
<li><p>&lt;3-&gt; Extremely difficult to compute because <span class="math inline">\({\mathbf{K}}_{{\mathbf{{f}}}, {\mathbf{{f}}}}\)</span> is dependent on <span class="math inline">\({\mathbf{Z}}\)</span> and appears in the inverse.</p></li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm" class="slide level3">
<h3>Variational Bayesian GP-LVM</h3>
<ul>
<li><p>&lt;1-&gt;Consider collapsed variational bound,   </p></li>
<li><p>&lt;4-&gt; Apply variational lower bound to the inner integral. </p></li>
<li><p>&lt;6-&gt; Which is analytically tractable for Gaussian <span class="math inline">\(q({\mathbf{Z}})\)</span> and some covariance functions.</p></li>
</ul>
</section>
<section id="required-expectations" class="slide level3">
<h3>Required Expectations</h3>
<ul>
<li><p>Need expectations under <span class="math inline">\(q({\mathbf{Z}})\)</span> of: <span class="math display">\[\log c_i = \frac{1}{2{\sigma}^2} \left[{k}_{i, i} - {\mathbf{{k}}}_{i, {\mathbf{{u}}}}^\top {\mathbf{K}}_{{\mathbf{{u}}}, {\mathbf{{u}}}}^{-1} {\mathbf{{k}}}_{i, {\mathbf{{u}}}}\right]\]</span> and <span class="math display">\[\log {\mathcal{N}\left({\mathbf{{y}}}|{\left&lt;{\mathbf{{f}}}\right&gt;_{p({\mathbf{{f}}}|{\mathbf{{u}}},{\mathbf{\MakeUppercase{{y}}}})}},{\sigma}^2{\mathbf{I}}\right)} = -\frac{1}{2}\log 2\pi{\sigma}^2 - \frac{1}{2{\sigma}^2}\left({y}_i - {\mathbf{K}}_{{\mathbf{{f}}}, {\mathbf{{u}}}}{\mathbf{K}}_{{\mathbf{{u}}},{\mathbf{{u}}}}^{-1}{\mathbf{{u}}}\right)^2\]</span></p></li>
<li><p>This requires the expectations <span class="math display">\[{\left&lt;{\mathbf{K}}_{{\mathbf{{f}}},{\mathbf{{u}}}}\right&gt;_{q({\mathbf{Z}})}}\]</span> and <span class="math display">\[{\left&lt;{\mathbf{K}}_{{\mathbf{{f}}},{\mathbf{{u}}}}{\mathbf{K}}_{{\mathbf{{u}}},{\mathbf{{u}}}}^{-1}{\mathbf{K}}_{{\mathbf{{u}}},{\mathbf{{f}}}}\right&gt;_{q({\mathbf{Z}})}}\]</span> which can be computed analytically for some covariance functions.</p></li>
</ul>
</section>
<section id="variational-compression-1" class="slide level3">
<h3>Variational Compression</h3>
<p><br />
<br />
- Augment each layer with inducing variables <span class="math inline">\({\mathbf{{u}}}_i\)</span>.</p>
<ul>
<li>Apply variational compression,
<span class="math display">\[\begin{align}
      p(\dataVector, \{\hiddenVector_i\}_{i=1}^{\numLayers-1}|\{\inducingVector_i\}_{i=1}^{\numLayers}, \inputMatrix) \geq &amp; 
      \tilde p(\dataVector|\inducingVector_{\numLayers}, \hiddenVector_{\numLayers-1})\prod_{i=2}^{\numLayers-1} \tilde p(\hiddenVector_i|\inducingVector_i,\hiddenVector_{i-1}) \tilde p(\hiddenVector_1|\inducingVector_i,\inputMatrix) \nonumber \\
      &amp; \times
      \exp\left(\sum_{i=1}^\numLayers -\frac{1}{2\sigma^2_i}\trace{\conditionalCovariance_{i}}\right)
      \label{eq:deep_structure}
    \end{align}\]</span>
where <span class="math display">\[\tilde p({\mathbf{{h}}}_i|{\mathbf{{u}}}_i,{\mathbf{{h}}}_{i-1})
    = {\mathcal{N}\left({\mathbf{{h}}}_i|{\mathbf{K}}_{{\mathbf{{h}}}_{i}{\mathbf{{u}}}_{i}}{\mathbf{K}}_{{\mathbf{{u}}}_i{\mathbf{{u}}}_i}^{-1}{\mathbf{{u}}}_i,\sigma^2_i{\mathbf{I}}\right)}.\]</span></li>
</ul>
</section>
<section id="nested-variational-compression" class="slide level3">
<h3>Nested Variational Compression</h3>
<p><br />
</p>
<ul>
<li><p>By sustaining explicity distributions over inducing variables James Hensman has developed a nested variatnt of variational compression.</p></li>
<li><p>Exciting thing: it mathematically looks like a deep neural network, but with inducing variables in the place of basis functions.</p></li>
<li><p>Additional complexity control term in the objective function.</p></li>
</ul>

</section>
<section id="nested-bound" class="slide level3">
<h3>Nested Bound</h3>
<span class="math display">\[\begin{align}
    \log p(\dataVector|\inputMatrix )  \geq &amp;
    %
    -\frac{1}{\sigma_1^2} \trace{\conditionalCovariance_1}
    % 
    -\sum_{i=2}^\numLayers \frac{1}{2\sigma_i^2} \left(\psi_{i}
    % 
    - \trace{{\boldsymbol \Phi}_{i}\kernelMatrix_{\inducingVector_{i} \inducingVector_{i}}^{-1}}\right) \nonumber \\
    %
    &amp; - \sum_{i=1}^{\numLayers}\KL{q(\inducingVector_i)}{p(\inducingVector_i)} \nonumber \\
    %
    &amp; - \sum_{i=2}^{\numLayers}\frac{1}{2\sigma^2_{i}}\trace{({\boldsymbol
        \Phi}_i - {\boldsymbol \Psi}_i^\top{\boldsymbol \Psi}_i)
      \kernelMatrix_{\inducingVector_{i}
        \inducingVector_{i}}^{-1}
      \expDist{\inducingVector_{i}\inducingVector_{i}^\top}{q(\inducingVector_{i})}\kernelMatrix_{\inducingVector_{i}\inducingVector_{i}}^{-1}} \nonumber \\
    %
    &amp; + {\only&lt;2&gt;{\color{\redColor}}\log \gaussianDist{\dataVector}{{\boldsymbol
        \Psi}_{\numLayers}\kernelMatrix_{\inducingVector_{\numLayers}
        \inducingVector_{\numLayers}}^{-1}{\mathbf
        m}_\numLayers}{\sigma^2_\numLayers\eye}}
    \label{eq:deep_bound}
  \end{align}\]</span>
</section>
<section id="required-expectations-1" class="slide level3">
<h3>Required Expectations</h3>
<p><span class="math display">\[{\only&lt;1&gt;{\color{\redColor}}\log {\mathcal{N}\left({\mathbf{{y}}}|{\only&lt;2-&gt;{\color{\blueColor}}{\boldsymbol
          \Psi}_{{\ell}}}{\mathbf{K}}_{{\mathbf{{u}}}_{{\ell}}
          {\mathbf{{u}}}_{{\ell}}}^{-1}{\mathbf
          m}_{\ell},\sigma^2_{\ell}{\mathbf{I}}\right)}}\]</span> where   </p>
<!--frame start-->
</section>
<section id="gaussian-py_if_i-1" class="slide level3">
<h3>Gaussian <span class="math inline">\(p({y}_i|{f}_i)\)</span></h3>
<p>For Gaussian likelihoods:  </p>
<!--frame end-->
<!--frame start-->
</section>
<section id="gaussian-process-over-mathbff-and-mathbfu-1" class="slide level3">
<h3>Gaussian Process Over <span class="math inline">\({\mathbf{{f}}}\)</span> and <span class="math inline">\({\mathbf{{u}}}\)</span></h3>
<p>Define: <span class="math display">\[q_{i, i} = {\text{var}_{p({f}_i|{\mathbf{{u}}})}\left( {f}_i \right)} = {\left&lt;{f}_i^2\right&gt;_{p({f}_i|{\mathbf{{u}}})}} - {\left&lt;{f}_i\right&gt;_{p({f}_i|{\mathbf{{u}}})}}^2\]</span> We can write: <span class="math display">\[c_i = \exp\left(-{\frac{q_{i,i}}{2\sigma^2}}\right)\]</span> If joint distribution of <span class="math inline">\(p({\mathbf{{f}}}, {\mathbf{{u}}})\)</span> is Gaussian then: <span class="math display">\[q_{i, i} = {k}_{i, i} - {\mathbf{{k}}}_{i, {\mathbf{{u}}}}^\top {\mathbf{K}}_{{\mathbf{{u}}}, {\mathbf{{u}}}}^{-1} {\mathbf{{k}}}_{i, {\mathbf{{u}}}}\]</span></p>
<p><span class="math inline">\(c_i\)</span> is not a function of <span class="math inline">\({\mathbf{{u}}}\)</span> but <em>is</em> a function of <span class="math inline">\({{\bf X}}_{\mathbf{{u}}}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="lower-bound-on-likelihood-1" class="slide level3">
<h3>Lower Bound on Likelihood</h3>
<p>Substitute variational bound into marginal likelihood: <span class="math display">\[p({\mathbf{{y}}})\geq \prod_{i=1}^{n}c_i \int {\mathcal{N}\left({\mathbf{{y}}}|{\left&lt;{\mathbf{{f}}}\right&gt;},{\sigma}^2{\mathbf{I}}\right)}p({\mathbf{{u}}}) \text{d}{\mathbf{{u}}}\]</span> Note that: <span class="math display">\[{\left&lt;{\mathbf{{f}}}\right&gt;_{p({\mathbf{{f}}}|{\mathbf{{u}}})}} = {\mathbf{K}}_{{\mathbf{{f}}}, {\mathbf{{u}}}} {\mathbf{K}}_{{\mathbf{{u}}}, {\mathbf{{u}}}}^{-1}{\mathbf{{u}}}\]</span> is <em>linearly</em> dependent on <span class="math inline">\({\mathbf{{u}}}\)</span>.</p>
<!--frame end-->
<!--frame start-->
</section>
<section id="deterministic-training-conditional-1" class="slide level3">
<h3>Deterministic Training Conditional</h3>
<p>Making the marginalization of <span class="math inline">\({\mathbf{{u}}}\)</span> straightforward. In the Gaussian case: <span class="math display">\[p({\mathbf{{u}}}) = {\mathcal{N}\left({\mathbf{{u}}}|{\mathbf{0}},{\mathbf{K}}_{{\mathbf{{u}}},{\mathbf{{u}}}}\right)}\]</span>      </p>
<!--frame end-->
<p>&lt;!–### What is a Deep Gaussian Process?</p>
<ul>
<li><p>Function Composition (Introduce as stacked processes … check Oxford Talk fromw ay back.</p></li>
<li><p>Stochastic Process Composition</p></li>
<li><p>Geoff Hinton’s view of Deep Learning</p></li>
</ul>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
              { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
