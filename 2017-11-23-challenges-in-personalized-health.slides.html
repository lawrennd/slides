<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Neil D. Lawrence">
  <title>Personalized Health: Challenges in Data Science</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Personalized Health: Challenges in Data Science</h1>
  <p class="author">Neil D. Lawrence</p>
</section>

<section class="slide level3">


</section>
<section id="section" class="slide level3 slide:" data-transition="none">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>??</p>
</blockquote>
</section>
<section id="section-1" class="slide level3 slide:" data-transition="none">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Benjamin Disraeli</p>
</blockquote>
</section>
<section id="section-2" class="slide level3 slide:" data-transition="none">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Benjamin Disraeli 1804-1881</p>
</blockquote>
</section>
<section id="mathematical-statistics" class="slide level3">
<h3><em>Mathematical</em> Statistics</h3>
<ul>
<li>‘Founded’ by Karl Pearson (1857-1936)</li>
</ul>
<p><img src="../slides/diagrams/Portrait_of_Karl_Pearson.jpg" align="center" width="30%" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-3" class="slide level3 slide:" data-transition="none">
<h3></h3>
<blockquote>
<p>There are three types of lies: lies, damned lies and ‘big data’</p>
<p>Neil Lawrence 1972-?</p>
</blockquote>
</section>
<section id="mathematical-data-science" class="slide level3">
<h3>‘Mathematical Data Science’</h3>
<ul>
<li>‘Founded’ by ? (?-?)</li>
</ul>
<p><img src="./diagrams/Question_mark.png" align="center" width="30%" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="background-big-data" class="slide level3">
<h3>Background: Big Data</h3>
<ul>
<li><p>The pervasiveness of data brings forward particular challenges.</p></li>
<li><p>Those challenges are most sharply in focus for personalized health.</p></li>
<li><p>Particular opportunities, in challenging areas such as <em>mental health</em>.</p></li>
</ul>
</section>
<section id="evolved-relationship" class="slide level3 slide:" data-transition="none">
<h3>Evolved Relationship</h3>
<object type="image/svg+xml" data="../slides/diagrams/data-science-information-flow_neg001.svg">
</object>
</section>
<section id="evolved-relationship-1" class="slide level3 slide:" data-transition="none">
<h3>Evolved Relationship</h3>
<object type="image/svg+xml" data="../slides/diagrams/data-science-information-flow_neg002.svg">
</object>
</section>
<section id="evolved-relationship-2" class="slide level3 slide:" data-transition="none">
<h3>Evolved Relationship</h3>
<object type="image/svg+xml" data="../slides/diagrams/data-science-information-flow_neg003.svg">
</object>
</section>
<section id="embodiment-factors" class="slide level3">
<h3>“Embodiment Factors”</h3>
<table>
<tr>
<td>
</td>
<td align="center">
<img src="../slides/diagrams/IBM_Blue_Gene_P_supercomputer.jpg" width="60%" style="background:none; border:none; box-shadow:none;" align="center">
</td>
<td align="center">
<img src="../slides/diagrams/ClaudeShannon_MFO3807.jpg" width="100%" style="background:none; border:none; box-shadow:none;" align="center">
</td>
</tr>
<tr>
<td>
compute
</td>
<td align="center">
~10 gigaflops
</td>
<td align="center">
~ 1000 teraflops?
</td>
</tr>
<tr>
<td>
communicate
</td>
<td align="center">
~1 gigbit/s
</td>
<td align="center">
~ 100 bit/s
</tr>
<td>
embodiment<br>(compute/communicate)
</td>
<td align="center">
10
</td>
<td align="center">
~ 10<sup>13</sup>
</tr>
</table>
</section>
<section id="evolved-relationship-3" class="slide level3">
<h3>Evolved Relationship</h3>
<object type="image/svg+xml" data="./diagrams/data-science-information-flow_neg003.svg">
</object>
</section>
<section id="effects" class="slide level3">
<h3>Effects</h3>
<ul>
<li><p>This phenomenon has already revolutionised biology.</p></li>
<li><p>Large scale data acquisition and distribution.</p></li>
<li><p>Transcriptomics, genomics, epigenomics, ‘rich phenomics’.</p></li>
<li><p>Great <em>promise</em> for personalized health.</p></li>
</ul>
</section>
<section id="societal-effects" class="slide level3">
<h3>Societal Effects</h3>
<ul>
<li><p>Automated decision making within the computer based only on the data.</p></li>
<li><p>A requirement to better understand our own subjective biases to ensure that the human to computer interface formulates the correct conclusions from the data.</p></li>
<li><p>Particularly important where treatments are being prescribed.</p></li>
<li><p>But what is a treatment in the modern era: interventions could be far more subtle.</p></li>
</ul>
</section>
<section id="societal-effects-1" class="slide level3">
<h3>Societal Effects</h3>
<ul>
<li><p>Shift in dynamic from the direct pathway between human and data to indirect pathway between human and data via the computer</p></li>
<li><p>This change of dynamics gives us the modern and emerging domain of data science</p></li>
</ul>
</section>
<section id="challenges" class="slide level3">
<h3>Challenges</h3>
<ol type="1">
<li><p>Paradoxes of the Data Society</p></li>
<li><p>Quantifying the Value of Data</p></li>
<li><p>Privacy, loss of control, marginalization</p></li>
</ol>
</section>
<section id="breadth-vs-depth-paradox" class="slide level3">
<h3>Breadth vs Depth Paradox</h3>
<ul>
<li><p>Able to quantify to a greater and greater degree the actions of individuals</p></li>
<li><p>But less able to characterize society</p></li>
<li><p>As we measure more, we understand less</p></li>
</ul>
</section>
<section id="what" class="slide level3">
<h3>What?</h3>
<ul>
<li><p>Perhaps greater preponderance of data is making society itself more complex</p></li>
<li><p>Therefore traditional approaches to measurement are failing</p></li>
<li><p>Curate’s egg of a society: it is only ‘measured in parts’</p></li>
</ul>
</section>
<section id="wood-or-tree" class="slide level3">
<h3>Wood or Tree</h3>
<ul>
<li>Can either see a wood or a tree.</li>
</ul>
<p><img src="./diagrams/Grib_skov.jpg" width="50%" style="border:none"> <!-- https://upload.wikimedia.org/wikipedia/commons/5/5b/Grib_skov.jpg--></p>
</section>
<section id="examples" class="slide level3">
<h3>Examples</h3>
<ul>
<li><p>Election polls (UK 2015 elections, EU referendum, US 2016 elections)</p></li>
<li><p>Clinical trials vs personalized medicine: Obtaining statistical power where interventions are subtle. e.g. social media</p></li>
</ul>
</section>
<section id="the-maths" class="slide level3 slide:" data-transition="none">
<h3>The Maths</h3>
<p><span class="math display">\[ \mathbf{\MakeUppercase{y}}= \begin{bmatrix}
y_{1, 1} &amp; y_{1, 2} &amp;\dots &amp; y_{1,p}\\
y_{2, 1} &amp; y_{2, 2} &amp;\dots &amp; y_{2,p}\\
\vdots &amp; \vdots &amp;\dots &amp; \vdots\\
y_{n, 1} &amp; y_{n, 2} &amp;\dots &amp; y_{n,p}
\end{bmatrix} \in \Re^{n\times p}
\]</span></p>
</section>
<section id="the-maths-1" class="slide level3 slide:" data-transition="none">
<h3>The Maths</h3>
<p><span class="math display">\[ \mathbf{\MakeUppercase{y}}= \begin{bmatrix}
\mathbf{y}^\top_{1, :} \\
\mathbf{y}^\top_{2, :} \\
\vdots \\
\mathbf{y}^\top_{n, :}
\end{bmatrix} \in \Re^{n\times p}
\]</span></p>
</section>
<section id="the-maths-2" class="slide level3 slide:" data-transition="none">
<h3>The Maths</h3>
<p><span class="math display">\[ \mathbf{\MakeUppercase{y}}= \begin{bmatrix}
\mathbf{y}_{:, 1} &amp;
\mathbf{y}_{:, 2} &amp;
\dots &amp;
\mathbf{y}_{:, p}
\end{bmatrix} \in \Re^{n\times p}
\]</span></p>
<!-- This is the nature of modern streaming data, what has been called big data, although in the UK it looks like that term will gain a more diffuse meaning now that the government has associated a putative 189 billion pounds of funding to it. But the characteristic of massive missing data is particularly obvious when we look at clinical domains. EMIS, a Yorkshire based provider of software to General Practitioners, has 39 million patient records. When we consider clinical measurements, we need to build models that not only take into account all current clinical tests, but all tests that will be invented in the future. This leads to the idea of massive missing data. The classical statistical table of data is merely the special case where someone has filled in a block of information.  -->
<!-- To deal with massively missing data we need to think about the *Kolmogorov consistency* of a process. Let me introduce Kolmogorov consistency by way of an example heard from Tony O'Hagan, but one that he credits originally to Michael Goldstein. Imagine you are on jury duty. You are asked to adjudicate on the guilt or innocence of Lord Safebury, and you are going to base your judgement on a model that is weighing all the evidence. You are just about to pronounce your decision when a maid comes running in and shouts "He didn't do it! He didn't do it!". The maid wasn't on the witness list and isn't accounted for in your model. How does this effect your inference? The pragmatists answer might be: not at all, because the maid wasn't in the model. But in the interests of justice we might want to include this information in our inference process. If, as a result of the maid's entry, we now think it is less likely that Lord Safebury committed the crime, then necessarily every time that the (unannounced) maid doesn't enter the room we have to assume that it is more likely that Safebury commited the crime (to ensure that the conditional probability of guilt given the maid's evidence normalizes. But we didn't know about the maid, so how can we account for this? Further, how can we account for all possible other surprise evidence, from the announced butlers, gardners, chauffeurs and footmen? Kolmogorov consistency (Kolmogorov:grundbegriffe33) says that the net effect of marginalizing for all these potential bits of new information is null. It is a particular property of the model. Making it (only slightly) more formal, we can consider Kolmogorov consistency as a marginalization property of the model. We take the $n$ dimensional vector, $\dataVector$, to be an (indexed) vector of all our instantiated observations of the world that we have *at the current time*. Then we take the $n^*$ dimensional vector, $\dataVector^*$ to be the observations of the world that we are *yet to see*. -->
</section>
<section id="the-maths-3" class="slide level3 slide:" data-transition="none">
<h3>The Maths</h3>
<p><span class="math display">\[p(\mathbf{\MakeUppercase{y}}|\boldsymbol{\theta}) = \prod_{i=1}^n p(\mathbf{y}_{i, :}|\boldsymbol{\theta})\]</span></p>
</section>
<section id="the-maths-4" class="slide level3 slide:" data-transition="none">
<h3>The Maths</h3>
<p><span class="math display">\[p(\mathbf{\MakeUppercase{y}}|\boldsymbol{\theta}) = \prod_{i=1}^n p(\mathbf{y}_{i, :}|\boldsymbol{\theta})\]</span></p>
<p><span class="math display">\[\log p(\mathbf{\MakeUppercase{y}}|\boldsymbol{\theta}) = \sum_{i=1}^n \log p(\mathbf{y}_{i, :}|\boldsymbol{\theta})\]</span></p>
</section>
<section id="consistency" class="slide level3">
<h3>Consistency</h3>
<ul>
<li><p>Typically <span class="math inline">\(\boldsymbol{\theta} \in \Re^{\mathcal{O}(p)}\)</span></p></li>
<li><p>Consistency reliant on large sample approximation of KL divergence</p></li>
</ul>
<p><span class="math display">\[ \text{KL}(P(\mathbf{\MakeUppercase{y}})|| p(\mathbf{\MakeUppercase{y}}|\boldsymbol{\theta}))\]</span></p>
<ul>
<li><p>Minimization is equivalent to maximization of likelihood.</p></li>
<li><p>A foundation stone of classical statistics.</p></li>
</ul>
</section>
<section id="large-p" class="slide level3">
<h3>Large <span class="math inline">\(p\)</span></h3>
<ul>
<li><p>For large <span class="math inline">\(p\)</span> the parameters are badly determined.</p></li>
<li><p>Large <span class="math inline">\(p\)</span> small <span class="math inline">\(n\)</span> problem.</p></li>
<li><p>Easily dealt with through definition.</p></li>
</ul>
</section>
<section id="the-maths-5" class="slide level3 slide:" data-transition="none">
<h3>The Maths</h3>
<p><span class="math display">\[p(\mathbf{\MakeUppercase{y}}|\boldsymbol{\theta}) = \prod_{j=1}^p p(\mathbf{y}_{:, j}|\boldsymbol{\theta})\]</span></p>
<p><span class="math display">\[\log p(\mathbf{\MakeUppercase{y}}|\boldsymbol{\theta}) = \sum_{j=1}^p \log p(\mathbf{y}_{:, j}|\boldsymbol{\theta})\]</span></p>
</section>
<section id="breadth-vs-depth" class="slide level3">
<h3>Breadth vs Depth</h3>
<ul>
<li><p>Modern Measurement deals with <em>depth</em> (many subjects) … or <em>breadth</em> lots of detail about subject.</p></li>
<li>But what about
<ul>
<li><span class="math inline">\(p\approx n\)</span>?</li>
<li>Stratification of populations: batch effects etc.</li>
</ul></li>
<li><p>Multi-task learning (Natasha Jaques)</p></li>
</ul>
</section>
<section id="does-p-even-exist" class="slide level3">
<h3>Does <span class="math inline">\(p\)</span> Even Exist?</h3>
<ul>
<li><p>Massively missing data.</p></li>
<li><p>Classical bias towards tables.</p></li>
<li><p>Streaming data.</p></li>
</ul>
<p><span class="math display">\[ \mathbf{\MakeUppercase{y}}= \begin{bmatrix}
y_{1, 1} &amp; y_{1, 2} &amp;\dots &amp; y_{1,p}\\
y_{2, 1} &amp; y_{2, 2} &amp;\dots &amp; y_{2,p}\\
\vdots &amp; \vdots &amp;\dots &amp; \vdots\\
y_{n, 1} &amp; y_{n, 2} &amp;\dots &amp; y_{n,p}
\end{bmatrix} \in \Re^{n\times p}
\]</span></p>
</section>
<section id="general-index-on-y" class="slide level3">
<h3>General index on <span class="math inline">\(y\)</span></h3>
<p><span class="math display">\[y_{\bf x}\]</span></p>
<p>where <span class="math inline">\({\bf x}\)</span> might include time, spatial location …</p>
<p>Streaming data. Joint model of past, <span class="math inline">\(\mathbf{y}\)</span> and future <span class="math inline">\(\mathbf{y}_*\)</span></p>
<p><span class="math display">\[p(\mathbf{y}, \mathbf{y}_*)\]</span></p>
<p>Prediction through:</p>
<p><span class="math display">\[p(\mathbf{y}_*|\mathbf{y})\]</span></p>
</section>
<section id="kolmogorov-consistency-exchangeability" class="slide level3">
<h3>Kolmogorov Consistency — Exchangeability</h3>
<ul>
<li><p>From the sum rule of probability we have <span class="math display">\[\begin{align*}
p(\dataVector|n^*) = \int p(\dataVector, \dataVector^*) \text{d}\dataVector^*
\end{align*}\]</span> <span class="math inline">\(n^*\)</span> is length of <span class="math inline">\(\mathbf{y}^*\)</span>.</p></li>
<li><p>Consistent if <span class="math inline">\(p(\mathbf{y}|n^*) = p(\mathbf{y})\)</span></p></li>
<li><p>Prediction then given by product rule <span class="math display">\[\begin{align*}
p(\dataVector^*|\dataVector) = \frac{p(\dataVector, \dataVector^*)}{p(\dataVector)}
\end{align*}\]</span></p></li>
</ul>
</section>
<section id="pmathbfymathbfy" class="slide level3">
<h3><span class="math inline">\(p(\mathbf{y}^*|\mathbf{y})\)</span></h3>
<!-- where the dependence of the marginal distribution for $\dataVector$ aries from the fact that we are forming an $n^*$ dimensional integral over $\dataVector^*$. If our distribution is Kolmogorov consistent, then we know that the distribution over $\dataVector$ is *independent* of the value of $n^*$. So in other words $p(\dataVector|n*)=p(\dataVector)$. So Kolmogorov consistency says that the form of $p(\dataVector)$ remains the same *regardless* of the number of observations of the world that are yet to come.  -->
</section>
<section id="parametric-models" class="slide level3">
<h3>Parametric Models</h3>
<ul>
<li><p>Kolmogorov consistency trivial in parametric model. <span class="math display">\[\begin{align*}
p(\dataVector, \dataVector^*) = \int \prod_{i=1}^n p(\dataScalar_{i} | \boldsymbol{\theta})\prod_{i=1}^{n^*}p(y^*_i|\boldsymbol{\theta}) p(\boldsymbol{\theta}) \text{d}\boldsymbol{\theta}
\end{align*}\]</span></p></li>
<li><p>Marginalizing <span class="math display">\[\begin{align*}
p(\dataVector) = \int \prod_{i=1}^n p(\dataScalar_{i} | \boldsymbol{\theta})\prod_{i=1}^{n^*} \int p(y^*_i|\boldsymbol{\theta}) \text{d}y^*_i p(\boldsymbol{\theta}) \text{d}\boldsymbol{\theta}
\end{align*}\]</span></p></li>
</ul>
</section>
<section id="parametric-bottleneck" class="slide level3">
<h3>Parametric Bottleneck</h3>
<ul>
<li><p>Bayesian methods suggest a prior over <span class="math inline">\(\boldsymbol{\theta}\)</span> and use posterior, <span class="math inline">\(p(\boldsymbol{\theta}|\mathbf{y})\)</span> for making predictions. <span class="math display">\[\begin{align*}
p(\dataVector^*|\dataVector) = \int \prod_i p(\dataScalar_i^* | \boldsymbol{\theta}) p(\boldsymbol{\theta}|\dataVector)\text{d}\boldsymbol{\theta} 
\end{align*}\]</span></p></li>
<li><p>Design time problem: <em>parametric bottleneck</em>. <span class="math display">\[p(\boldsymbol{\theta} | \mathbf{y})\]</span></p></li>
<li><p>Streaming data could turn out to be more complex than we imagine.</p></li>
</ul>
</section>
<section id="finite-storage" class="slide level3">
<h3>Finite Storage</h3>
<ul>
<li><p>Despite our large interconnected brains, we only have finite storage.</p></li>
<li><p>Similar for digital computers. So we need to assume that we can only store a finite number of things about the data <span class="math inline">\(\mathbf{y}\)</span>.</p></li>
<li><p>This pushes us back towards <em>parametric</em> models.</p></li>
</ul>
<!-- ### Inducing Variables -->
<!-- * Choose to go a different way.  -->
<!-- * Introduce a set of auxiliary variables, $\inducingVector$, which are $m$ in length.  -->
<!-- * They are like "artificial data". -->
<!-- * Used to *induce* a distribution: $q(\inducingVector|\dataVector)$  -->
<!-- ### Making Parameters non-Parametric -->
<!-- * Introduce variable set which is *finite* dimensional.  -->
<!-- $$ -->
<!-- p(\dataVector^*|\dataVector) \approx \int p(\dataVector^*|\inducingVector) q(\inducingVector|\dataVector) \text{d}\inducingVector -->
<!-- $$ -->
<!-- * But dimensionality of $\inducingVector$ can be changed to improve approximation. -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- * Model for our data, $\dataVector$ -->
<!-- $$p(\dataVector)$$ -->
<!-- <br><object type="image/svg+xml" data="./diagrams/py.svg"> -->
<!-- </object> -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- * Prior density over $\mappingFunctionVector$. Likelihood relates data, $\dataVector$, to $\mappingFunctionVector$. -->
<!-- $$p(\dataVector)=\int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector)\text{d}\mappingFunctionVector$$<br> -->
<!-- <object type="image/svg+xml" data="./diagrams/pygfpf.svg"> -->
<!-- </object> -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- * Prior density over $\mappingFunctionVector$. Likelihood relates data, $\dataVector$, to $\mappingFunctionVector$. -->
<!-- $$p(\dataVector)=\int p(\dataVector|\mappingFunctionVector)p(\inducingVector|\mappingFunctionVector)p(\mappingFunctionVector)\text{d}\mappingFunctionVector\text{d}\inducingVector$$<br> -->
<!-- <object type="image/svg+xml" data="./diagrams/pygfpugfpf.svg"> -->
<!-- </object></td></tr> -->
<!-- </table> -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- $$p(\dataVector)=\int \int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector|\inducingVector)\text{d}\mappingFunctionVectorp(\inducingVector)\text{d}\inducingVector$$ -->
<!-- <br><object type="image/svg+xml" data="./diagrams/pygfpfgupu.svg"> -->
<!-- </object> -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- $$p(\dataVector)=\int \int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector|\inducingVector)\text{d}\mappingFunctionVectorp(\inducingVector)\text{d}\inducingVector$$<br> -->
<!-- <object type="image/svg+xml" data="./diagrams/pygfpfgupu2.svg"> -->
<!-- </object> -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- $$p(\dataVector|\inducingVector)=\int p(\dataVector|\mappingFunctionVector)p(\mappingFunctionVector|\inducingVector)\text{d}\mappingFunctionVector$$<br> -->
<!-- <object type="image/svg+xml" data="./diagrams/pygfpfgu.svg"> -->
<!-- </object> -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- $$p(\dataVector|\inducingVector)$$<br> -->
<!-- <object type="image/svg+xml" data="./diagrams/pygu.svg"> -->
<!-- </object> -->
<!-- ### Variational Compression {.slide: data-transition="none"} -->
<!-- $$p(\dataVector|\boldsymbol{\theta})$$<br> -->
<!-- <object type="image/svg+xml" data="./diagrams/pygtheta.svg"> -->
<!-- </object> -->
<!-- ### Compression -->
<!-- * Replace true $p(\inducingVector|\dataVector)$ with approximation $q(\inducingVector|\dataVector)$. -->
<!-- * Minimize KL divergence between approximation and truth. -->
<!-- * This is similar to the Bayesian posterior distribution. -->
<!-- * But it's placed over a set of 'pseudo-observations'. -->
</section>
<section id="also-need" class="slide level3">
<h3>Also need</h3>
<ul>
<li>More classical statistics!
<ul>
<li>Like the ‘paperless office’</li>
</ul></li>
<li><p>A better characterization of human (see later)</p></li>
<li>Larger studies (100,000 genome)
<ul>
<li>Combined with complex models: algorithmic challenges</li>
</ul></li>
</ul>
</section>
<section id="quantifying-the-value-of-data" class="slide level3">
<h3>Quantifying the Value of Data</h3>
<p>There’s a sea of data, but most of it is undrinkable</p>
<p><img src="./diagrams/sea-water-ocean-waves.jpg" width="50%"></p>
<p>We require data-desalination before it can be consumed!</p>
</section>
<section id="data" class="slide level3">
<h3>Data</h3>
<ul>
<li>90% of our time is spent on validation and integration (Leo Anthony Celi)</li>
<li>“The Dirty Work We Don’t Want to Think About” (Eric Xing)</li>
<li>“Voodoo to get it decompressed” (Francisco Giminez?)</li>
<li>In health care clinicians collect the data and often control the direction of research through guardianship of data.</li>
</ul>
</section>
<section id="value" class="slide level3">
<h3>Value</h3>
<ul>
<li>How do we measure value in the data economy?</li>
<li>How do we encourage data workers: curation and management
<ul>
<li>Incentivization for sharing and production.</li>
<li>Quantifying the value in the contribution of <em>each actor</em>.</li>
</ul></li>
</ul>
</section>
<section id="section-4" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object type="image/svg+xml" data="./diagrams/pomdp001.svg">
</object>
</section>
<section id="section-5" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object type="image/svg+xml" data="./diagrams/pomdp002.svg">
</object>
</section>
<section id="section-6" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object type="image/svg+xml" data="./diagrams/pomdp003.svg">
</object>
</section>
<section id="section-7" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object type="image/svg+xml" data="./diagrams/pomdp004.svg">
</object>
</section>
<section id="section-8" class="slide level3 slide:" data-transition="none">
<h3></h3>
<object type="image/svg+xml" data="./diagrams/pomdp005.svg">
</object>
</section>
<section id="credit-allocation" class="slide level3">
<h3>Credit Allocation</h3>
<ul>
<li><p>Direct work on data generates an enormous amount of ‘value’ in the data economy but this is unaccounted in the economy</p></li>
<li><p>Hard because data is difficult to ‘embody’</p></li>
<li><p>Value of shared data: <a href="https://wellcome.ac.uk/what-we-do/our-work/sharing-research-data-improve-public-health-full-joint-statement-funders-health">Wellcome Trust 2010 Joint Statement</a> (from the “Foggy Bottom” meeting)</p></li>
</ul>
</section>
<section id="solutions" class="slide level3">
<h3>Solutions</h3>
<ul>
<li><p>Encourage greater interaction between application domains and data scientists</p></li>
<li><p>Encourage visualization of data</p></li>
<li><p>Adoption of ‘data readiness levels’</p></li>
<li><p>Implications for incentivization schemes</p></li>
</ul>
</section>
<section id="privacy-loss-of-control-and-marginalization" class="slide level3">
<h3>Privacy, Loss of Control and Marginalization</h3>
<ul>
<li><p>Society is becoming harder to monitor</p></li>
<li><p>Individual is becoming easier to monitor</p></li>
</ul>
</section>
<section id="conversation" class="slide level3 slide:" data-transition="none">
<h3>Conversation</h3>
<object type="image/svg+xml" data="../slides/diagrams/anne_bob001.svg">
</object>
</section>
<section id="conversation-1" class="slide level3 slide:" data-transition="none">
<h3>Conversation</h3>
<object type="image/svg+xml" data="../slides/diagrams/anne_bob002.svg">
</object>
</section>
<section id="conversation-2" class="slide level3 slide:" data-transition="none">
<h3>Conversation</h3>
<object type="image/svg+xml" data="../slides/diagrams/anne_bob003.svg">
</object>
</section>
<section id="modelling" class="slide level3">
<h3>Modelling</h3>
<object type="image/svg+xml" data="../slides/diagrams/anne.svg">
</object>
</section>
<section id="modelling-1" class="slide level3">
<h3>Modelling</h3>
<object type="image/svg+xml" data="../slides/diagrams/bob.svg">
</object>
</section>
<section id="hate-speech-or-political-dissent" class="slide level3">
<h3>Hate Speech or Political Dissent?</h3>
<ul>
<li>social media monitoring for ‘hate speech’ can be easily turned to political dissent monitoring</li>
</ul>
</section>
<section id="marketing" class="slide level3">
<h3>Marketing</h3>
<ul>
<li>can become more sinister when the target of the marketing is well understood and the (digital) environment of the target is also so well controlled</li>
</ul>
</section>
<section id="free-will" class="slide level3 slide:" data-transition="none">
<h3>Free Will</h3>
<ul>
<li>What does it mean if a computer can predict our individual behavior better than we ourselves can?</li>
</ul>
</section>
<section id="discrimination" class="slide level3 slide:" data-transition="none">
<h3>Discrimination</h3>
<ul>
<li><p>Potential for explicit and implicit discrimination on the basis of race, religion, sexuality, health status</p></li>
<li><p>All prohibited under European law, but can pass unawares, or be implicit</p></li>
<li><p>GDPR: General Data Protection Regulation</p></li>
</ul>
</section>
<section id="discrimination-1" class="slide level3 slide:" data-transition="none">
<h3>Discrimination</h3>
<ul>
<li><p>Potential for explicit and implicit discrimination on the basis of race, religion, sexuality, health status</p></li>
<li><p>All prohibited under European law, but can pass unawares, or be implicit</p></li>
<li><p>GDPR: Good Data Practice Rules</p></li>
</ul>
</section>
<section id="marginalization" class="slide level3">
<h3>Marginalization</h3>
<ul>
<li>Credit scoring, insurance, medical treatment</li>
<li>What if certain sectors of society are under-represented in our aanalysis?</li>
<li>What if Silicon Valley develops everything for us?</li>
</ul>
</section>
<section id="digital-revolution-and-inequality" class="slide level3">
<h3>Digital Revolution and Inequality?</h3>
<p><img src="./diagrams/woman-tends-house-in-village-of-uganda-africa.jpg" width="50%" style="border:none"></p>
</section>
<section id="amelioration" class="slide level3">
<h3>Amelioration</h3>
<ul>
<li>Work to ensure individual retains control of their own data</li>
<li>We accept privacy in our real lives, need to accept it in our digital</li>
<li>Control of persona and ability to project</li>
<li>Need better technological solutions: trust and algorithms.</li>
</ul>
</section>
<section id="section-9" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_few.svg">
</object>
</section>
<section id="section-10" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples.svg">
</object>
</section>
<section id="section-11" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-12" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_rejection_samples.svg">
</object>
</section>
<section id="key-object" class="slide level3" data-transition="none">
<h3>Key Object</h3>
<ul>
<li><p>Covariance function, <span class="math inline">\(\mathbf{K}\)</span></p></li>
<li><p>Determines properties of samples.</p></li>
<li><p>Function of <span class="math inline">\({\bf X}\)</span>, <span class="math display">\[k_{i,j} = k({\bf x}_i, {\bf x}_j)\]</span></p></li>
</ul>
</section>
<section id="linear-algebra" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D({\bf x}_*) = \mathbf{k}({\bf x}_*, {\bf X}) \mathbf{K}^{-1}
\mathbf{y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{f}, *}\]</span></p></li>
</ul>
</section>
<section id="linear-algebra-1" class="slide level3" data-transition="none">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D({\bf x}_*) = \mathbf{k}({\bf x}_*, {\bf X}) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{f}, *}\]</span></p></li>
</ul>
</section>
<section id="section-13" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prior_samples_data.svg">
</object>
</section>
<section id="section-14" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_rejection_samples.svg">
</object>
</section>
<section id="section-15" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot" data="../_gp/diagrams/gp_prediction.svg">
</object>
</section>
<section id="differential-privacy-summary" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>Differential Privacy, summary</h3>
<ul>
<li><p>We want to protect a user from a linkage attack…</p>
<p>…while still performing inference over the whole group.</p></li>
<li><p>Making a dataset private is more than just erasing names.</p></li>
<li><p>To achieve a level of privacy one needs to add <strong>randomness</strong> to the data.</p></li>
<li><p>This is a fundamental feature of differential privacy.</p></li>
</ul>
<p>See <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf">The Algorithmic Foundations of Differential Privacy</a> by Dwork and Roth for a rigorous introduction to the framework.</p>
</section>
<section id="differential-privacy-for-gaussian-processes" class="slide level3" data-transition="None">
<h3>Differential Privacy for Gaussian Processes</h3>
<p>We have a dataset in which the inputs, <span class="math inline">\(\mathbf{X}\)</span>, are <strong>public</strong>. The outputs, <span class="math inline">\(\mathbf{y}\)</span>, we want to keep <strong>private</strong>.</p>
<figure>
<img data-src="../slides/diagrams/kung_pseudo_pert_neg.png" alt="Data consists of the heights and weights of 287 women from a census of the !Kung" style="border:none" data-align="center" style="width:65.0%" /><figcaption>Data consists of the heights and weights of 287 women from a census of the !Kung</figcaption>
</figure>
<p><strong>Data consists of the heights and weights of 287 women from a census of the !Kung</strong></p>
</section>
<section id="vectors-and-functions" class="slide level3" data-transition="None">
<h3>Vectors and Functions</h3>
<p>Hall et al. (2013) showed that one can ensure that a version of <span class="math inline">\(f\)</span>, function <span class="math inline">\(\tilde{f}\)</span> is <span class="math inline">\((\varepsilon, \delta)\)</span>-differentially private by adding a scaled sample from a GP prior.</p>
<p><img data-src="../slides/diagrams/hall1_neg.png" style="border:none" data-align="center" style="width:30.0%" /></p>
<p>3 pages of maths ahead!</p>
</section>
<section id="applied-to-gaussian-processes" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>Applied to Gaussian Processes</h3>
<ul>
<li><p>We applied this method to the GP posterior.</p></li>
<li><p>The covariance of the posterior only depends on the inputs, <span class="math inline">\(X\)</span>. So we can compute this without applying DP.</p></li>
<li><p>The mean function, <span class="math inline">\(f_D(\mathbf{x_*})\)</span>, does depend on <span class="math inline">\(\mathbf{y}\)</span>. <span class="math display">\[f_D(\mathbf{x_*}) = \mathbf{k}(x_*, \mathbf{X})
\mathbf{K}^{-1} \mathbf{y}\]</span></p></li>
<li><p>We are interested in finding</p>
<p><span class="math display">\[|| f_D(\mathbf{x_*}) -
f_{D^\prime}(\mathbf{x_*}) ||_H^2\]</span></p>
<p>…how much the mean function (in RKHS) can change due to a change in <span class="math inline">\(\mathbf{y}\)</span>.</p></li>
</ul>
</section>
<section id="applied-to-gaussian-processes-1" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>Applied to Gaussian Processes</h3>
<ul>
<li><p>Using the representer theorem, we can write <span class="math display">\[|| f_D(\mathbf{x_*}) -
  f_{D^\prime}(\mathbf{x_*}) ||_H^2\]</span></p>
<p>as:</p>
<p><span class="math display">\[\Big|\Big|\sum_{i=1}^n k(\mathbf{x_*},\mathbf{x}_i)
\left(\alpha_i - \alpha^\prime_i\right)\Big|\Big|_H^2\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\alpha} - \boldsymbol{\alpha}^\prime = \mathbf{K}^{-1} \left(\mathbf{y} - \mathbf{y}^\prime \right)\)</span></p></li>
</ul>
</section>
<section id="section-16" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3></h3>
<ul>
<li><p>L2 Norm</p>
<p><span class="math display">\[\Big|\Big|\sum_{i=1}^n k(\mathbf{x_*},\mathbf{x}_i)
\left(\alpha_i - \alpha^\prime_i\right)\Big|\Big|_H^2\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\alpha} - \boldsymbol{\alpha}^\prime = \mathbf{K}^{-1} \left(\mathbf{y} - \mathbf{y}^\prime \right)\)</span></p></li>
<li><p>We constrain the kernel: <span class="math inline">\(-1\leq k(\cdot,\cdot) \leq 1\)</span> and we only allow one element of <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{y}&#39;\)</span> to differ (by at most <span class="math inline">\(d\)</span>).</p></li>
<li><p>So only one column of <span class="math inline">\(\mathbf{K}^{-1}\)</span> will be involved in the change of mean (which we are summing over).</p></li>
<li><p>The distance above can then be shown to be no greater than <span class="math inline">\(d\;||\mathbf{K}^{-1}||_\infty\)</span></p></li>
</ul>
</section>
<section id="applied-to-gaussian-processes-2" class="slide level3" data-transition="None">
<h3>Applied to Gaussian Processes</h3>
<p>This ‘works’ in that it allows DP predictions…but to avoid too much noise, the value of <span class="math inline">\(\varepsilon\)</span> is too large (here it is 100)</p>
<p><img data-src="../slides/diagrams/kung_standard_simple_neg.png" style="border:none" data-align="center" style="width:50.0%" /></p>
<p>EQ kernel, <span class="math inline">\(\ell = 25\)</span> years, <span class="math inline">\(\Delta=100\)</span>cm</p>
</section>
<section id="inducing-inputs" class="slide level3" data-transition="None">
<h3>Inducing Inputs</h3>
<p>Using sparse methods (i.e. inducing inputs) can help reduce the sensitivity a little. We’ll see more on this later.</p>
<p><img data-src="../slides/diagrams/kung_inducing_simple_neg.png" style="border:none" data-align="center" style="width:70.0%" /></p>
</section>
<section id="cloaking" class="slide level3" data-transition="None">
<h3>Cloaking</h3>
<ul>
<li><p>So far we’ve made the whole posterior mean function private…</p>
<p>…what if we just concentrate on making particular predictions private?</p></li>
</ul>
</section>
<section id="effect-of-perturbation" class="slide level3" data-transition="None">
<h3>Effect of perturbation</h3>
<ul>
<li><p>Standard approach: sample the noise is from the GP’s <strong>prior</strong>.</p></li>
<li><p>Not necessarily the most ‘efficient’ covariance to use.</p></li>
</ul>
</section>
<section id="cloaking-1" class="slide level3" data-transition="None">
<h3>Cloaking</h3>
<object type="image/svg+xml" data="../slides/diagrams/dp_firstpoint0_neg.svg">
</object>
<p><em>Left</em>: Function change. <em>Right</em>: test point change</p>
</section>
<section id="cloaking-1" class="slide level3" data-transition="None">
<h3>Cloaking</h3>
<object type="image/svg+xml" data="../slides/diagrams/dp_firstpoint2_neg.svg">
</object>
<p><em>Left</em>: Function change. <em>Right</em>: test point change</p>
</section>
<section id="cloaking-1" class="slide level3" data-transition="None">
<h3>Cloaking</h3>
<object type="image/svg+xml" data="../slides/diagrams/dp_secondpoint0_neg.svg">
</object>
<p><em>Left</em>: Function change. <em>Right</em>: test point change</p>
</section>
<section id="cloaking-1" class="slide level3" data-transition="None">
<h3>Cloaking</h3>
<object type="image/svg+xml" data="../slides/diagrams/dp_secondpoint2_neg.svg">
</object>
<p><em>Left</em>: Function change. <em>Right</em>: test point change</p>
</section>
<section id="cloaking-1" class="slide level3" data-transition="None">
<h3>Cloaking</h3>
<object type="image/svg+xml" data="../slides/diagrams/dp_with_ellipse1_neg.svg">
</object>
<p><em>Left</em>: Function change. <em>Right</em>: test point change</p>
</section>
<section id="cloaking-1" class="slide level3" data-transition="None">
<h3>Cloaking</h3>
<object type="image/svg+xml" data="../slides/diagrams/dp_with_ellipse2_neg.svg">
</object>
<p><em>Left</em>: Function change. <em>Right</em>: test point change</p>
</section>
<section id="dp-vectors" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>DP Vectors</h3>
<ul>
<li><p>Hall et al. (2013) also presented a bound on vectors.</p></li>
<li><p>Find a bound (<span class="math inline">\(\Delta\)</span>) on the scale of the output change, in term of its Mahalanobis distance (wrt the added noise covariance).</p>
<p><span class="math display">\[\sup_{D \sim {D&#39;}} ||\mathbf{M}^{-1/2} (\mathbf{y}_* - \mathbf{y}_{*}&#39;)||_2 \leq \Delta\]</span></p></li>
<li><p>We use this to scale the noise we add:</p>
<p><span class="math display">\[\frac{\text{c}(\delta)\Delta}{\varepsilon} \mathcal{N}_d(0,\mathbf{M})\]</span></p>
<p>We get to pick <span class="math inline">\(\mathbf{M}\)</span></p></li>
</ul>
</section>
<section id="cloaking-2" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>Cloaking</h3>
<ul>
<li><p>Intuitively we want to construct <span class="math inline">\(\mathbf{M}\)</span> so that it has greatest covariance in those directions most affected by changes in training points, so that it will be most able to mask those changes.</p></li>
<li><p>The change in posterior mean predictions is,</p>
<p><span class="math display">\[\mathbf{y}_* - \mathbf{y}&#39;_* = \mathbf{K}_{*f} \mathbf{K}^{-1} (\mathbf{y}-\mathbf{y}&#39;)\]</span></p></li>
<li><p>Effect of perturbing each training point on each test point is represented in the cloaking matrix,</p>
<p><span class="math display">\[\mathbf{C} = \mathbf{K}_{*f} \mathbf{K}^{-1}\]</span></p></li>
</ul>
</section>
<section id="cloaking-3" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>Cloaking</h3>
<ul>
<li><p>We assume we are protecting only one training input’s change, by at most <span class="math inline">\(d\)</span>.</p></li>
<li><p>So <span class="math inline">\(\mathbf{y}-\mathbf{y}&#39;\)</span> will be all zeros except for one element, <span class="math inline">\(i\)</span>.<br />
</p></li>
<li><p>So the change in test points will be (at most)</p>
<p><span class="math display">\[\mathbf{y}_*&#39; - \mathbf{y}_* = d \mathbf{C}_{:i}\]</span></p></li>
<li><p>We’re able to write the earlier bound as,</p>
<p><span class="math display">\[d^2 \sup_{i} \mathbf{c}_i^\top \mathbf{M}^{-1} \mathbf{c}_i \leq\Delta\]</span></p>
<p>where <span class="math inline">\(\mathbf{c}_i \triangleq \mathbf{C}_{:i}\)</span></p></li>
</ul>
</section>
<section id="cloaking-4" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>Cloaking</h3>
<ul>
<li><p>Dealing with <span class="math inline">\(d\)</span> elsewhere and setting <span class="math inline">\(\Delta = 1\)</span> (thus <span class="math inline">\(0 \leq \mathbf{c}_i^\top \mathbf{M}^{-1} \mathbf{c}_i \leq 1\)</span>) and minimise <span class="math inline">\(\log |\mathbf{M}|\)</span> (minimises the partial entropy).</p></li>
<li><p>Using Lagrange multipliers and gradient descent, we find</p>
<p><span class="math display">\[\mathbf{M} = \sum_i{\lambda_i \mathbf{c}_i \mathbf{c}_i^\top}\]</span></p></li>
</ul>
</section>
<section id="cloaking-results" class="slide level3" data-transition="None">
<h3>Cloaking: Results</h3>
<p>The noise added by this method is now practical.</p>
<p><img data-src="../slides/diagrams/kung_cloaking_simple_neg.png" style="border:none" data-align="center" style="width:100.0%" /></p>
<p>EQ kernel, <span class="math inline">\(l = 25\)</span> years, <span class="math inline">\(\Delta=100\)</span>cm, <span class="math inline">\(\varepsilon=1\)</span></p>
</section>
<section id="cloaking-results-1" class="slide level3" data-background="../slides/diagrams/pres_bg.png">
<h3>Cloaking: Results</h3>
<p>It also has some interesting features;</p>
<ul>
<li>Less noise where data is concentrated</li>
<li>Least noise far from any data</li>
<li>Most noise just outside data</li>
</ul>
</section>
<section id="cloaking-results-2" class="slide level3" data-transition="None">
<h3>Cloaking: Results</h3>
<p><img data-src="../slides/diagrams/kung_cloaking_simple_neg.png" style="border:none" data-align="center" style="width:100.0%" /></p>
</section>
<section id="house-prices-around-london" class="slide level3" data-transition="None">
<h3>House Prices Around London</h3>
<p><img src="../slides/diagrams/houseprices_bigcirc_15km_0_labels_neg.png" width="60%" style="border:none"></p>
</section>
<section id="citibike" class="slide level3" data-transition="None">
<h3>Citibike</h3>
<ul>
<li><p>Tested on 4D citibike dataset (predicting journey durations from start/finish station locations).</p></li>
<li><p>The method appears to achieve lower noise than binning alternatives (for reasonable <span class="math inline">\(\varepsilon\)</span>).</p></li>
</ul>
</section>
<section id="citibike-1" class="slide level3" data-transition="None">
<h3>Citibike</h3>
<p><img data-src="../slides/diagrams/newtable2_neg.png" style="border:none" data-align="center" style="width:80.0%" /> lengthscale in degrees, values above, journey duration (in seconds)</p>
</section>
<section id="cloaking-and-inducing-inputs" class="slide level3">
<h3>Cloaking and Inducing Inputs</h3>
<ul>
<li><p>Outliers poorly predicted.</p></li>
<li><p>Too much noise around data ‘edges’.</p></li>
<li><p>Use inducing inputs to reduce the sensitivity to these outliers.</p></li>
</ul>
</section>
<section id="cloaking-no-inducing-inputs" class="slide level3" data-transition="None">
<h3>Cloaking (no) Inducing Inputs</h3>
<p><img data-src="../slides/diagrams/cloaking-no-inducing_neg.png" style="border:none" data-align="center" style="width:100.0%" /></p>
</section>
<section id="cloaking-and-inducing-inputs-1" class="slide level3" data-transition="None">
<h3>Cloaking and Inducing Inputs</h3>
<p><img data-src="../slides/diagrams/cloaking-inducing_neg.png" style="border:none" data-align="center" style="width:80.0%" /></p>
</section>
<section id="results" class="slide level3">
<h3>Results</h3>
<ul>
<li><p>For 1D !Kung, RMSE improved from <span class="math inline">\(15.0 \pm 2.0 \text{cm}\)</span> to <span class="math inline">\(11.1 \pm 0.8 \text{cm}\)</span></p>
<p>Use Age and Weight to predict Height</p></li>
<li><p>For 2D !Kung, RMSE improved from <span class="math inline">\(22.8 \pm 1.9 \text{cm}\)</span> to <span class="math inline">\(8.8 \pm 0.6 \text{cm}\)</span></p>
<p>Note that the uncertainty across cross-validation runs smaller. 2D version benefits from data’s 1D manifold.</p></li>
</ul>
</section>
<section id="cloaking-no-inducing-inputs-1" class="slide level3" data-transition="none">
<h3>Cloaking (no) Inducing Inputs</h3>
<p><img data-src="../slides/diagrams/housing-no-inducing_neg.png" style="border:none" data-align="center" style="width:80.0%" /></p>
</section>
<section id="cloaking-and-inducing-inputs-2" class="slide level3" data-transition="none">
<h3>Cloaking and Inducing Inputs</h3>
<p><img data-src="../slides/diagrams/housing-inducing_neg.png" style="border:none" data-align="center" style="width:80.0%" /></p>
</section>
<section id="awareness" class="slide level3">
<h3>Awareness</h3>
<ul>
<li>Need to increase awareness of the pitfalls among researchers</li>
<li>Need to ensure that technological solutions are being delivered not merely for few (#FirstWorldProblems)</li>
<li>Address a wider set of challenges that the greater part of the world’s population is facing</li>
</ul>
</section>
<section id="conclusion" class="slide level3">
<h3>Conclusion</h3>
<ul>
<li>Data science offers a great deal of promise for personalized health</li>
<li>There are challenges and pitfalls</li>
<li>It is incumbent on us to avoid them</li>
<li>Need new ways of thinking!</li>
<li><em>Mathematical</em> Data Science</li>
</ul>
<p><strong>Many solutions rely on education and awareness</strong></p>
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: lawrennd</li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
