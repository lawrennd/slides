<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2018-12-10">
  <title>Machine Learning and the Physical World</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["color.js"]
          }
        });
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
    document.setElementById("range-".concat(group)) = slideIndex
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
\[\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Machine Learning and the Physical World</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2018-12-10</time></p>
  <p class="venue" style="text-align:center">Center for Statistics and Machine Learning, Princeton</p>
</section>

<section class="slide level3">

<!-- Enables links to pages-->
<p><!--% not ipynb--></p>
<p><!--% not notes--></p>
</section>
<section id="section" class="slide level3">
<h3></h3>
<div style="text-align:center">
<img class="" src="../slides/diagrams/science-holborn-viaduct.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none;">
</div>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<div style="text-align:center">
<img class="negate" src="../slides/diagrams/SteamEngine_Boulton&Watt_1784.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none;">
</div>
<center>
<em>Watt’s Steam Engine which made Steam Power Efficient and Practical </em>
</center>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<div style="text-align:center">
<img class="negate" src="../slides/diagrams/Centrifugal_governor.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none;">
</div>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</section>
<section id="efficiency" class="slide level3">
<h3>Efficiency</h3>
<ul>
<li>Economies driven by ‘production’.</li>
<li>Greater production comes with better efficiency.
<ul>
<li>E.g. moving from gathering food to settled agriculture.</li>
</ul></li>
<li>In the modern era one approach to becoming more efficient is automation of processes.
<ul>
<li>E.g. manufacturing production lines</li>
</ul></li>
</ul>
</section>
<section id="physical-processes" class="slide level3">
<h3>Physical Processes</h3>
<ul>
<li>Manufacturing processes consist of production lines and robotic automation.</li>
<li>Logistics can also be decomposed into the supply chain processes.</li>
<li>Efficiency can be improved by automation.</li>
</ul>
</section>
<section id="goods-and-information" class="slide level3">
<h3>Goods and Information</h3>
<ul>
<li>For modern society: management of flow of goods and information.</li>
<li>Flow of information is highly automated.</li>
<li>Processing of data is decomposed into stages in computer code.</li>
</ul>
</section>
<section id="intervention" class="slide level3">
<h3>Intervention</h3>
<ul>
<li>For all cases: manufacturing, logistics, data management</li>
<li>Pipeline requires human intervention from an operator.</li>
<li>Interventions create bottlenecks, slow the process.</li>
<li>Machine learning is a key technology in automating these manual stages.</li>
</ul>
</section>
<section id="long-grass" class="slide level3">
<h3>Long Grass</h3>
<ul>
<li>Easy to replicate interventions have already been dealt with.</li>
<li>Components that still require human intervention are the knottier problems.</li>
<li>Difficult decompose into stages which could then be further automated.</li>
<li>These components are ‘process-atoms’.</li>
<li>These are the “long grass” regions of technology.</li>
</ul>
</section>
<section id="nature-of-challenge" class="slide level3">
<h3>Nature of Challenge</h3>
<ul>
<li>In manufacturing or logistics settings atoms are flexible manual skills.
<ul>
<li>Requires emulation of a human’s motor skills.</li>
</ul></li>
<li>In information processing: our flexible cognitive skills.
<ul>
<li>Our ability to mentally process an image or some text.</li>
</ul></li>
</ul>
</section>
<section id="worked-example-delivery-drones" class="slide level3">
<h3>Worked Example: Delivery Drones</h3>
<iframe width="800" height="600" src="https://www.youtube.com/embed/vNySOrI2Ny8?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</section>
<section id="artificial-intelligence-and-data-science" class="slide level3">
<h3>Artificial Intelligence and Data Science</h3>
<ul>
<li>AI aims to equip computers with human capabilities
<ul>
<li>Image understanding</li>
<li>Computer vision</li>
<li>Speech recognition</li>
<li>Natural language understanding</li>
<li>Machine translation</li>
</ul></li>
</ul>
</section>
<section id="supervised-learning-for-ai" class="slide level3">
<h3>Supervised Learning for AI</h3>
<ul>
<li>Dominant approach today:
<ul>
<li>Generate large labelled data set from humans.</li>
<li>Use <em>supervised learning</em> to emulate that data.
<ul>
<li><em>E.g.</em> <a href="www.image-net.org">ImageNet</a> <span class="citation" data-cites="Russakovsky-imagenet15">Russakovsky et al. (2015)</span></li>
</ul></li>
</ul></li>
<li>Significant advances due to <em>deep learning</em>
<ul>
<li><em>E.g.</em> Alexa, Amazon Go</li>
</ul></li>
</ul>
</section>
<section id="data-science" class="slide level3">
<h3>Data Science</h3>
<ul>
<li>Arises from <em>happenstance data</em>.</li>
<li>Differs from statistics in that the question comes <em>after</em> data collection.</li>
</ul>
</section>
<section id="the-gap" class="slide level3">
<h3>The Gap</h3>
<ul>
<li>There is a gap between the world of data science and AI.</li>
<li>The mapping of the virtual onto the physical world.</li>
<li>E.g. Causal understanding.</li>
</ul>
</section>
<section id="machine-learning-in-supply-chain" class="slide level3">
<h3>Machine Learning in Supply Chain</h3>
<ul>
<li><em>Supply chain</em>: Large Automated Decision Making Network</li>
<li>Amazon’s supply chain: Possibly the world’s largest ‘AI’</li>
<li>Major Challenge:
<ul>
<li>We have a <em>mechanistic</em> understanding of supply chain.</li>
<li>Machine learning is a <em>data driven</em> technology.</li>
</ul></li>
</ul>
<!--include{_ml/includes/or-control-econometrics-statistics-ml.md}-->
</section>
<section id="uncertainty-quantification" class="slide level3">
<h3>UNCERTAINTY QUANTIFICATION</h3>
</section>
<section id="data-driven" class="slide level3">
<h3>Data Driven</h3>
<ul>
<li>Machine Learning: Replicate Processes through <em>direct use of data</em>.</li>
<li>Aim to emulate cognitive processes through the use of data.</li>
<li>Use data to provide new approaches in control and optimization that should allow for emulation of human motor skills.</li>
</ul>
</section>
<section id="process-emulation" class="slide level3">
<h3>Process Emulation</h3>
<ul>
<li>Key idea: emulate the process as a mathematical function.</li>
<li>Each function has a set of <em>parameters</em> which control its behavior.</li>
<li><em>Learning</em> is the process of changing these parameters to change the shape of the function</li>
<li>Choice of which class of mathematical functions we use is a vital component of our <em>model</em>.</li>
</ul>
</section>
<section id="emukit-playground" class="slide level3">
<h3>Emukit Playground</h3>
<ul>
<li><p>Work <a href="https://twitter.com/_AdamHirst">Adam Hirst</a>, Software Engineering Intern and Cliff McCollum.</p></li>
<li><p>Tutorial on emulation.</p></li>
</ul>
</section>
<section id="emukit-playground-1" class="slide level3">
<h3>Emukit Playground</h3>
<div style="text-align:center">
<a href="https://amzn.github.io/emukit-playground/"><img class="" src="../slides/diagrams/uq/emukit-playground.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none;"></a>
</div>
</section>
<section id="emukit-playground-2" class="slide level3">
<h3>Emukit Playground</h3>
<div style="text-align:center">
<a href="https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization"><img class="negate" src="../slides/diagrams/uq/emukit-playground-bayes-opt.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none;"></a>
</div>
</section>
<section id="uncertainty-quantification-1" class="slide level3">
<h3>Uncertainty Quantification</h3>
<ul>
<li>Deep nets are powerful approach to images, speech, language.</li>
<li>Proposal: Deep GPs may also be a great approach, but better to deploy according to natural strengths.</li>
</ul>
</section>
<section id="uncertainty-quantification-2" class="slide level3">
<h3>Uncertainty Quantification</h3>
<ul>
<li>Probabilistic numerics, surrogate modelling, emulation, and UQ.</li>
<li>Not a fan of AI as a term.</li>
<li>But we are faced with increasing amounts of <em>algorithmic decision making</em>.</li>
</ul>
</section>
<section id="ml-and-decision-making" class="slide level3">
<h3>ML and Decision Making</h3>
<ul>
<li>When trading off decisions: compute or acquire data?</li>
<li>There is a critical need for uncertainty.</li>
</ul>
</section>
<section id="uncertainty-quantification-3" class="slide level3">
<h3>Uncertainty Quantification</h3>
<blockquote>
<p>Uncertainty quantification (UQ) is the science of quantitative characterization and reduction of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known.</p>
</blockquote>
<ul>
<li>Interaction between physical and virtual worlds of major interest.</li>
</ul>
</section>
<section id="contrast" class="slide level3">
<h3>Contrast</h3>
<ul>
<li>Simulation in <em>reinforcement learning</em>.</li>
<li>Known as <em>data augmentation</em>.</li>
<li>Newer, similar in spirit, but typically ignores uncertainty.</li>
</ul>
</section>
<section id="example-formula-one-racing" class="slide level3">
<h3>Example: Formula One Racing</h3>
<ul>
<li><p>Designing an F1 Car requires CFD, Wind Tunnel, Track Testing etc.</p></li>
<li><p>How to combine them?</p></li>
</ul>
</section>
<section id="mountain-car-simulator" class="slide level3">
<h3>Mountain Car Simulator</h3>
<div style="text-align:center">
<img class="negate" src="../slides/diagrams/uq/mountaincar.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none;">
</div>
</section>
<section id="car-dynamics" class="slide level3">
<h3>Car Dynamics</h3>
<p><span class="math display">\[\inputVector_{t+1} = \mappingFunction(\inputVector_{t},\textbf{u}_{t})\]</span></p>
<p>where <span class="math inline">\(\textbf{u}_t\)</span> is the action force, <span class="math inline">\(\inputVector_t = (p_t, v_t)\)</span> is the vehicle state</p>
</section>
<section id="policy" class="slide level3">
<h3>Policy</h3>
<ul>
<li>Assume policy is linear with parameters <span class="math inline">\(\boldsymbol{\theta}\)</span></li>
</ul>
<p><span class="math display">\[\pi(\inputVector,\theta)= \theta_0 + \theta_p p + \theta_vv.\]</span></p>
</section>
<section id="emulate-the-mountain-car" class="slide level3">
<h3>Emulate the Mountain Car</h3>
<ul>
<li>Goal is find <span class="math inline">\(\theta\)</span> such that</li>
</ul>
<p><span class="math display">\[\theta^* = arg \max_{\theta} R_T(\theta).\]</span></p>
<ul>
<li>Reward is computed as 100 for target, minus squared sum of actions</li>
</ul>
</section>
<section id="random-linear-controller" class="slide level3">
<h3>Random Linear Controller</h3>
<iframe src="../slides/diagrams/uq/mountain_car_random.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="best-controller-after-50-iterations-of-bayesian-optimization" class="slide level3">
<h3>Best Controller after 50 Iterations of Bayesian Optimization</h3>
<iframe src="../slides/diagrams/uq/mountain_car_simulated.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="data-efficient-emulation" class="slide level3">
<h3>Data Efficient Emulation</h3>
<ul>
<li><p>For standard Bayesian Optimization ignored <em>dynamics</em> of the car.</p></li>
<li><p>For more data efficiency, first <em>emulate</em> the dynamics.</p></li>
<li><p>Then do Bayesian optimization of the <em>emulator</em>.</p></li>
<li><p>Use a Gaussian process to model <span class="math display">\[\Delta v_{t+1} = v_{t+1} - v_{t}\]</span> and <span class="math display">\[\Delta x_{t+1} = p_{t+1} - p_{t}\]</span></p></li>
<li><p>Two processes, one with mean <span class="math inline">\(v_{t}\)</span> one with mean <span class="math inline">\(p_{t}\)</span></p></li>
</ul>
</section>
<section id="emulator-training" class="slide level3">
<h3>Emulator Training</h3>
<ul>
<li><p>Used 500 randomly selected points to train emulators.</p></li>
<li><p>Can make proces smore efficient through <em>experimental design</em>.</p></li>
</ul>
</section>
<section id="comparison-of-emulation-and-simulation" class="slide level3">
<h3>Comparison of Emulation and Simulation</h3>
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/uq/emu_sim_comparison.svg" style>
</object>
</div>
</section>
<section id="data-efficiency" class="slide level3">
<h3>Data Efficiency</h3>
<ul>
<li><p>Our emulator used only 500 calls to the simulator.</p></li>
<li><p>Optimizing the simulator directly required 37,500 calls to the simulator.</p></li>
</ul>
</section>
<section id="best-controller-using-emulator-of-dynamics" class="slide level3">
<h3>Best Controller using Emulator of Dynamics</h3>
<iframe src="../slides/diagrams/uq/mountain_car_emulated.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
<p>500 calls to the simulator vs 37,500 calls to the simulator</p>
<p><span class="math display">\[\mappingFunction_i\left(\inputVector\right) = \rho\mappingFunction_{i-1}\left(\inputVector\right) + \delta_i\left(\inputVector \right)\]</span></p>
</section>
<section id="multi-fidelity-emulation" class="slide level3">
<h3>Multi-Fidelity Emulation</h3>
<p><span class="math display">\[\mappingFunction_i\left(\inputVector\right) = \mappingFunctionTwo_{i}\left(\mappingFunction_{i-1}\left(\inputVector\right)\right) + \delta_i\left(\inputVector \right),\]</span></p>
</section>
<section id="best-controller-with-multi-fidelity-emulator" class="slide level3">
<h3>Best Controller with Multi-Fidelity Emulator</h3>
<iframe src="../slides/diagrams/uq/mountain_car_multi_fidelity.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
<p>250 observations of high fidelity simulator and 250 of the low fidelity simulator</p>
</section>
<section id="emukit" class="slide level3">
<h3>Emukit</h3>
<ul>
<li>Work by Javier Gonzalez, Andrei Paleyes, Mark Pullin, Maren Mahsereci.</li>
<li>Available on <a href="https://github.com/amzn/emukit">Github</a></li>
<li>Example <a href="https://github.com/amzn/emukit/blob/develop/notebooks/Emukit-sensitivity-montecarlo.ipynb">sensitivity notebook</a>.</li>
</ul>
</section>
<section id="emukit-software" class="slide level3">
<h3>Emukit Software</h3>
<ul>
<li><em>Multi-fidelity emulation</em>: build surrogate models for multiple sources of information;</li>
<li><em>Bayesian optimisation</em>: optimise physical experiments and tune parameters ML algorithms;</li>
<li><em>Experimental design/Active learning</em>: design experiments and perform active learning with ML models;</li>
<li><em>Sensitivity analysis</em>: analyse the influence of inputs on the outputs</li>
<li><em>Bayesian quadrature</em>: compute integrals of functions that are expensive to evaluate.</li>
</ul>
</section>
<section id="mxfusion" class="slide level3">
<h3>MxFusion</h3>
<table>
<tr>
<td width="70%">
<ul>
<li>Work by Eric Meissner and Zhenwen Dai.</li>
<li>Probabilistic programming.</li>
<li>Available on <a href="https://github.com/amzn/mxfusion">Github</a>
</td>
<td width="30%">
<img class="" src="../slides/diagrams/mxfusion-logo.png" width="" height="auto" align="" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table></li>
</ul>
</section>
<section id="mxfusion-1" class="slide level3">
<h3>MxFusion</h3>
<ul>
<li>Targeted at challenges we face in emulation.</li>
<li>Composition of Gaussian processes (Deep GPs)</li>
<li>Combining GPs with neural networks.</li>
<li>Example <a href="https://github.com/amzn/MXFusion/blob/master/examples/notebooks/ppca_tutorial.ipynb">PPCA Tutorial</a>.</li>
</ul>
</section>
<section id="why-another-framework" class="slide level3">
<h3>Why another framework?</h3>
<p>Existing libraries had either: * Probabilistic modelling with rich, flexible models and universal inference or * Specialized, efficient inference over a subset of models</p>
<p><strong>We needed both</strong></p>
</section>
<section id="key-requirements" class="slide level3">
<h3>Key Requirements</h3>
<ul>
<li>Integration with deep learning</li>
<li>Flexiblility</li>
<li>Scalability</li>
<li>Specialized inference and models support
<ul>
<li>Bayesian Deep Learning methods</li>
<li>Rapid prototyping and software re-use</li>
<li>GPUs, specialized inference methods</li>
</ul></li>
</ul>
</section>
<section id="modularity" class="slide level3">
<h3>Modularity</h3>
<ul>
<li>Specialized Inference</li>
<li>Composability (tinkerability)
<ul>
<li>Better leveraging of expert expertise</li>
</ul></li>
</ul>
</section>
<section id="what-does-it-look-like" class="slide level3">
<h3>What does it look like?</h3>
<p><strong>Modelling</strong></p>
<p><strong>Inference</strong></p>
</section>
<section id="modelling" class="slide level3">
<h3>Modelling</h3>
</section>
<section id="directed-graphs" class="slide level3">
<h3>Directed Graphs</h3>
<ul>
<li>Variable</li>
<li>Function</li>
<li>Distribution</li>
</ul>
</section>
<section id="example" class="slide level3">
<h3>Example</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">m <span class="op">=</span> Model()
m.mu <span class="op">=</span> Variable()
m.s <span class="op">=</span> Variable(transformation<span class="op">=</span>PositiveTransformation())
m.Y <span class="op">=</span> Normal.define_variable(mean<span class="op">=</span>m.mu, variance<span class="op">=</span>m.s)</code></pre></div>
</section>
<section id="primary-components-in-modeling" class="slide level3">
<h3>3 primary components in modeling</h3>
<ul>
<li>Variable</li>
<li>Distribution</li>
<li>Function</li>
</ul>
</section>
<section id="primary-methods-for-models" class="slide level3">
<h3>2 primary methods for models</h3>
<ul>
<li><code>log_pdf</code></li>
<li><code>draw_samples</code></li>
</ul>
</section>
<section id="inference-two-classes" class="slide level3">
<h3>Inference: Two Classes</h3>
<ul>
<li>Variational Inference</li>
<li>MCMC Sampling (<em>soon</em>) Built on MXNet Gluon (imperative code, not static graph)</li>
</ul>
</section>
<section id="example-1" class="slide level3">
<h3>Example</h3>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">infr <span class="op">=</span> GradBasedInference(inference_algorithm<span class="op">=</span>MAP(model<span class="op">=</span>m, observed<span class="op">=</span>[m.Y]))
infr.run(Y<span class="op">=</span>data)</code></pre></div>
</section>
<section id="modules" class="slide level3">
<h3>Modules</h3>
<ul>
<li>Model + Inference together form building blocks.
<ul>
<li>Just doing modular modeling with universal inference doesn’t really scale, need specialized inference methods for specialized modelling objects like non-parametrics.</li>
</ul></li>
</ul>
</section>
<section id="pilco-a-model-based-policy-search" class="slide level3">
<h3>PILCO: A Model-based Policy Search</h3>
<p>PILCO <span class="citation" data-cites="Deisenroth:pilco11">(Deisenroth and Rasmussen, n.d.)</span> is a model-based data-efficient algorithm that solves the RL problem by the following two step iterative process:</p>
<ol type="1">
<li>Fit a Gaussian Process that models the state dynamics, using calls to a simulator</li>
<li>Optimize a parametric policy using our GP instead of calling the simulator.</li>
</ol>
</section>
<section id="enhancements-mxfusion-brings" class="slide level3">
<h3>Enhancements MXFusion Brings</h3>
<ul>
<li>Use Monte Carlo integration instead of moment estimation</li>
<li>Use automatic differentiation</li>
<li>A flexible interface for Gaussian processes, trivial to switch to sparse or stochastic variational</li>
</ul>
</section>
<section id="preparation" class="slide level3">
<h3>Preparation</h3>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">pip</span> install mxnet mxfusion gym</code></pre></div>
<p>Set the global configuration.</p>
</section>
<section id="example-pendulum" class="slide level3">
<h3>Example: Pendulum</h3>
</section>
<section id="pendulum" class="slide level3">
<h3>Pendulum</h3>
<iframe src="../slides/diagrams/ml/animation_random_policy.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="fit-the-dynamics-model" class="slide level3">
<h3>Fit the Dynamics Model</h3>
<ul>
<li>Dynamics:</li>
</ul>
<p><span class="math display">\[
p(\dataScalar_{t+1}|\dataScalar_t, a_t)
\]</span></p>
</section>
<section id="define-and-fit-the-model" class="slide level3">
<h3>Define and fit the model</h3>
</section>
<section id="policy-1" class="slide level3">
<h3>Policy</h3>
<ul>
<li>Make use of neural network with one hidden layer</li>
</ul>
</section>
<section id="obtaining-the-policy-gradients" class="slide level3">
<h3>Obtaining the policy gradients</h3>
</section>
<section id="the-loop" class="slide level3">
<h3>The Loop</h3>
</section>
<section id="after-first-epsiode" class="slide level3">
<h3>After First Epsiode</h3>
<p>Policy after the first episode (random exploration):</p>
<iframe src="../slides/diagrams/ml/animation_policy_iter_0.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="after-fifth-episode" class="slide level3">
<h3>After Fifth Episode</h3>
<p>Policy after the 5th episode:</p>
<iframe src="../slides/diagrams/ml/animation_policy_iter_4.html" width="1024" height="768" allowtransparency="true" frameborder="0">
</iframe>
</section>
<section id="contribute" class="slide level3">
<h3>Contribute!</h3>
<p><a href="https://github.com/amzn/mxfusion" class="uri">https://github.com/amzn/mxfusion</a></p>
</section>
<section id="future-plans" class="slide level3">
<h3>Future plans</h3>
<ul>
<li>Deep GPs (implemented, not yet merged)</li>
<li>MCMC Methods</li>
<li>Time series models (RGPs)</li>
</ul>
</section>
<section id="long-term-aim" class="slide level3">
<h3>Long term Aim</h3>
<ul>
<li>Simulate/Emulate the components of the system.
<ul>
<li>Validate with real world using multifidelity.</li>
<li>Interpret system using e.g. sensitivity analysis.</li>
</ul></li>
<li>Perform end to end learning to optimize.
<ul>
<li>Maintain interpretability.</li>
</ul></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Deisenroth:pilco11">
<p>Deisenroth, M.P., Rasmussen, C.E., n.d. PILCO: A model-based and data-efficient approach to policy search, in:.</p>
</div>
<div id="ref-Russakovsky-imagenet15">
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L., 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115, 211–252. <a href="https://doi.org/10.1007/s11263-015-0816-y" class="uri">https://doi.org/10.1007/s11263-015-0816-y</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
