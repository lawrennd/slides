<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2019-01-09">
  <title>Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["color.js"]
          }
        });
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
    document.setElementById("range-".concat(group)) = slideIndex
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
\[\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2019-01-09</time></p>
  <p class="venue" style="text-align:center">MLSS, Stellenbosch, South Africa</p>
</section>

<section class="slide level3">

<!-- Enables links to pages-->
<p><!--% not ipynb--></p>
<p><!--% slides and not ipynb












 <!--% not notes--></p>
</section>
<section id="section" class="slide level3">
<h3></h3>
<span style="text-align:right"><span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span></span>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/gp/rasmussen-williams-book.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span class="math inline">\(\errorFunction(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</section>
<section id="artificial-intelligence" class="slide level3">
<h3>Artificial Intelligence</h3>
<ul>
<li>Machine learning is a mainstay because of importance of prediction.</li>
</ul>
</section>
<section id="uncertainty" class="slide level3">
<h3>Uncertainty</h3>
<ul>
<li>Uncertainty in prediction arises from:</li>
<li>scarcity of training data and</li>
<li>mismatch between the set of prediction functions we choose and all possible prediction functions.</li>
<li>Also uncertainties in objective, leave those for another day.</li>
</ul>
</section>
<section id="neural-networks-and-prediction-functions" class="slide level3">
<h3>Neural Networks and Prediction Functions</h3>
<ul>
<li>adaptive non-linear function models inspired by simple neuron models <span class="citation" data-cites="McCulloch:neuron43">(McCulloch and Pitts, 1943)</span></li>
<li>have become popular because of their ability to model data.</li>
<li>can be composed to form highly complex functions</li>
<li>start by focussing on one hidden layer</li>
</ul>
</section>
<section id="prediction-function-of-one-hidden-layer" class="slide level3">
<h3>Prediction Function of One Hidden Layer</h3>
<p><span class="math display">\[
\mappingFunction(\inputVector) = \left.\mappingVector^{(2)}\right.^\top \activationVector(\mappingMatrix_{1}, \inputVector)
\]</span></p>
<p><span class="math inline">\(\mappingFunction(\cdot)\)</span> is a scalar function with vector inputs,</p>
<p><span class="math inline">\(\activationVector(\cdot)\)</span> is a vector function with vector inputs.</p>
<ul>
<li><p>dimensionality of the vector function is known as the number of hidden units, or the number of neurons.</p></li>
<li><p>elements of <span class="math inline">\(\activationVector(\cdot)\)</span> are the <em>activation</em> function of the neural network</p></li>
<li><p>elements of <span class="math inline">\(\mappingMatrix_{1}\)</span> are the parameters of the activation functions.</p></li>
</ul>
</section>
<section id="relations-with-classical-statistics" class="slide level3">
<h3>Relations with Classical Statistics</h3>
<ul>
<li><p>In statistics activation functions are known as <em>basis functions</em>.</p></li>
<li><p>would think of this as a <em>linear model</em>: not linear predictions, linear in the parameters</p></li>
<li><p><span class="math inline">\(\mappingVector_{1}\)</span> are <em>static</em> parameters.</p></li>
</ul>
</section>
<section id="adaptive-basis-functions" class="slide level3">
<h3>Adaptive Basis Functions</h3>
<ul>
<li><p>In machine learning we optimize <span class="math inline">\(\mappingMatrix_{1}\)</span> as well as <span class="math inline">\(\mappingMatrix_{2}\)</span> (which would normally be denoted in statistics by <span class="math inline">\(\boldsymbol{\beta}\)</span>).</p></li>
<li><p>Revisit that decision: follow the path of <span class="citation" data-cites="Neal:bayesian94">Neal (1994)</span> and <span class="citation" data-cites="MacKay:bayesian92">MacKay (1992)</span>.</p></li>
<li><p>Consider the probabilistic approach.</p></li>
</ul>
</section>
<section id="probabilistic-modelling" class="slide level3">
<h3>Probabilistic Modelling</h3>
<ul>
<li>Probabilistically we want, <span class="math display">\[
p(\dataScalar_*|\dataVector, \inputMatrix, \inputVector_*),
\]</span> <span class="math inline">\(\dataScalar_*\)</span> is a test output <span class="math inline">\(\inputVector_*\)</span> is a test input <span class="math inline">\(\inputMatrix\)</span> is a training input matrix <span class="math inline">\(\dataVector\)</span> is training outputs</li>
</ul>
</section>
<section id="joint-model-of-world" class="slide level3">
<h3>Joint Model of World</h3>
<p><span class="math display">\[
p(\dataScalar_*|\dataVector, \inputMatrix, \inputVector_*) = \int p(\dataScalar_*|\inputVector_*, \mappingMatrix) p(\mappingMatrix | \dataVector, \inputMatrix) \text{d} \mappingMatrix
\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\mappingMatrix\)</span> contains <span class="math inline">\(\mappingMatrix_1\)</span> and <span class="math inline">\(\mappingMatrix_2\)</span></p>
<p><span class="math inline">\(p(\mappingMatrix | \dataVector, \inputMatrix)\)</span> is posterior density</p>
</div>
</section>
<section id="likelihood" class="slide level3">
<h3>Likelihood</h3>
<p><span class="math inline">\(p(\dataScalar|\inputVector, \mappingMatrix)\)</span> is the <em>likelihood</em> of data point</p>
<div class="fragment">
<p>Normally assume independence: <span class="math display">\[
p(\dataVector|\inputMatrix, \mappingMatrix) = \prod_{i=1}^\numData p(\dataScalar_i|\inputVector_i, \mappingMatrix),\]</span></p>
</div>
</section>
<section id="likelihood-and-prediction-function" class="slide level3">
<h3>Likelihood and Prediction Function</h3>
<p><span class="math display">\[
p(\dataScalar_i | \mappingFunction(\inputVector_i)) = \frac{1}{\sqrt{2\pi \dataStd^2}} \exp\left(-\frac{\left(\dataScalar_i - \mappingFunction(\inputVector_i)\right)^2}{2\dataStd^2}\right)
\]</span></p>
</section>
<section id="unsupervised-learning" class="slide level3">
<h3>Unsupervised Learning</h3>
<ul>
<li><p>Can also consider priors over latents <span class="math display">\[
p(\dataVector_*|\dataVector) = \int p(\dataVector_*|\inputMatrix_*, \mappingMatrix) p(\mappingMatrix | \dataVector, \inputMatrix) p(\inputMatrix) p(\inputMatrix_*) \text{d} \mappingMatrix \text{d} \inputMatrix \text{d}\inputMatrix_*
\]</span></p></li>
<li><p>This gives <em>unsupervised learning</em>.</p></li>
</ul>
</section>
<section id="probabilistic-inference" class="slide level3">
<h3>Probabilistic Inference</h3>
<ul>
<li><p>Data: <span class="math inline">\(\dataVector\)</span></p></li>
<li><p>Model: <span class="math inline">\(p(\dataVector, \dataVector^*)\)</span></p></li>
<li><p>Prediction: <span class="math inline">\(p(\dataVector^*| \dataVector)\)</span></p></li>
</ul>
</section>
<section id="graphical-models" class="slide level3">
<h3>Graphical Models</h3>
<ul>
<li>Represent joint distribution through <em>conditional dependencies</em>.</li>
<li>E.g. Markov chain</li>
</ul>
<p><span class="math display">\[p(\dataVector) = p(\dataScalar_\numData | \dataScalar_{\numData-1}) p(\dataScalar_{\numData-1}|\dataScalar_{\numData-2}) \dots p(\dataScalar_{2} | \dataScalar_{1})\]</span></p>
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/ml/markov.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<p>Predict Perioperative Risk of Clostridium Difficile Infection Following Colon Surgery <span class="citation" data-cites="Steele:predictive12">(Steele et al., 2012)</span></p>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/bayes-net-diagnosis.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="performing-inference" class="slide level3">
<h3>Performing Inference</h3>
<ul>
<li><p>Easy to write in probabilities</p></li>
<li><p>But underlying this is a wealth of computational challenges.</p></li>
<li><p>High dimensional integrals typically require approximation.</p></li>
</ul>
</section>
<section id="linear-models" class="slide level3">
<h3>Linear Models</h3>
<ul>
<li><p>In statistics, focussed more on <em>linear</em> model implied by <span class="math display">\[
  \mappingFunction(\inputVector) = \left.\mappingVector^{(2)}\right.^\top \activationVector(\mappingMatrix_1, \inputVector)
  \]</span></p></li>
<li><p>Hold <span class="math inline">\(\mappingMatrix_1\)</span> fixed for given analysis.</p></li>
<li><p>Gaussian prior for <span class="math inline">\(\mappingMatrix\)</span>, <span class="math display">\[
  \mappingVector^{(2)} \sim \gaussianSamp{\zerosVector}{\covarianceMatrix}.
  \]</span> <span class="math display">\[
  \dataScalar_i = \mappingFunction(\inputVector_i) + \noiseScalar_i,
  \]</span> where <span class="math display">\[
  \noiseScalar_i \sim \gaussianSamp{0}{\dataStd^2}
  \]</span></p></li>
</ul>
</section>
<section id="linear-gaussian-models" class="slide level3">
<h3>Linear Gaussian Models</h3>
<ul>
<li>Normally integrals are complex but for this Gaussian linear case they are trivial.</li>
</ul>
</section>
<section id="multivariate-gaussian-properties" class="slide level3">
<h3>Multivariate Gaussian Properties</h3>
</section>
<section id="recall-univariate-gaussian-properties" class="slide level3">
<h3>Recall Univariate Gaussian Properties</h3>
<div class="fragment">
<ol type="1">
<li>Sum of Gaussian variables is also Gaussian.</li>
</ol>
<p><span class="math display">\[\dataScalar_i \sim \gaussianSamp{\meanScalar_i}{\dataStd_i^2}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\sum_{i=1}^{\numData} \dataScalar_i \sim \gaussianSamp{\sum_{i=1}^\numData \meanScalar_i}{\sum_{i=1}^\numData\dataStd_i^2}\]</span></p>
</div>
</section>
<section id="recall-univariate-gaussian-properties-1" class="slide level3">
<h3>Recall Univariate Gaussian Properties</h3>
<ol start="2" type="1">
<li>Scaling a Gaussian leads to a Gaussian.</li>
</ol>
<div class="fragment">
<p><span class="math display">\[\dataScalar \sim \gaussianSamp{\meanScalar}{\dataStd^2}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\mappingScalar\dataScalar\sim \gaussianSamp{\mappingScalar\meanScalar}{\mappingScalar^2 \dataStd^2}\]</span></p>
</div>
</section>
<section id="multivariate-consequence" class="slide level3">
<h3>Multivariate Consequence</h3>
<p><span style="text-align:left">If</span> <span class="math display">\[\inputVector \sim \gaussianSamp{\meanVector}{\covarianceMatrix}\]</span></p>
<div class="fragment">
<p><span style="text-align:left">And</span> <span class="math display">\[\dataVector= \mappingMatrix\inputVector\]</span></p>
</div>
<div class="fragment">
<p><span style="text-align:left">Then</span> <span class="math display">\[\dataVector \sim \gaussianSamp{\mappingMatrix\meanVector}{\mappingMatrix\covarianceMatrix\mappingMatrix^\top}\]</span></p>
</div>
</section>
<section id="linear-gaussian-models-1" class="slide level3">
<h3>Linear Gaussian Models</h3>
<ol type="1">
<li>linear Gaussian models are easier to deal with</li>
<li>Even the parameters <em>within</em> the process can be handled, by considering a particular limit.</li>
</ol>
</section>
<section id="multivariate-gaussian-properties-1" class="slide level3">
<h3>Multivariate Gaussian Properties</h3>
<ul>
<li><p>If <span class="math display">\[
\dataVector = \mappingMatrix \inputVector + \noiseVector,
\]</span></p></li>
<li><p>Assume <span class="math display">\[\begin{align}
\inputVector &amp; \sim \gaussianSamp{\meanVector}{\covarianceMatrix}\\
\noiseVector &amp; \sim \gaussianSamp{\zerosVector}{\covarianceMatrixTwo}
\end{align}\]</span></p></li>
<li><p>Then <span class="math display">\[
\dataVector \sim \gaussianSamp{\mappingMatrix\meanVector}{\mappingMatrix\covarianceMatrix\mappingMatrix^\top + \covarianceMatrixTwo}.
\]</span> If <span class="math inline">\(\covarianceMatrixTwo=\dataStd^2\eye\)</span>, this is Probabilistic Principal Component Analysis <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>, because we integrated out the inputs (or <em>latent</em> variables they would be called in that case).</p></li>
</ul>
</section>
<section id="non-linear-on-inputs" class="slide level3">
<h3>Non linear on Inputs</h3>
<ul>
<li>Set each activation function computed at each data point to be</li>
</ul>
<p><span class="math display">\[
\activationScalar_{i,j} = \activationScalar(\mappingVector^{(1)}_{j}, \inputVector_{i})
\]</span> Define <em>design matrix</em> <span class="math display">\[
\activationMatrix = 
\begin{bmatrix}
\activationScalar_{1, 1} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numHidden} \\
\activationScalar_{1, 2} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numData} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\activationScalar_{\numData, 1} &amp; \activationScalar_{\numData, 2} &amp; \dots &amp; \activationScalar_{\numData, \numHidden}
\end{bmatrix}.
\]</span></p>
</section>
<section id="matrix-representation-of-a-neural-network" class="slide level3">
<h3>Matrix Representation of a Neural Network</h3>
<p><span class="math display">\[\dataScalar\left(\inputVector\right) = \activationVector\left(\inputVector\right)^\top \mappingVector + \noiseScalar\]</span></p>
<div class="fragment">
<p><span class="math display">\[\dataVector = \activationMatrix\mappingVector + \noiseVector\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}\]</span></p>
</div>
</section>
<section id="prior-density" class="slide level3">
<h3>Prior Density</h3>
<ul>
<li>Define</li>
</ul>
<p><span class="math display">\[
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha\eye},
\]</span></p>
<ul>
<li>Rules of multivariate Gaussians to see that,</li>
</ul>
<p><span class="math display">\[
\dataVector \sim \gaussianSamp{\zerosVector}{\alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye}.
\]</span></p>
<p><span class="math display">\[
\kernelMatrix = \alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye.
\]</span></p>
</section>
<section id="joint-gaussian-density" class="slide level3">
<h3>Joint Gaussian Density</h3>
<ul>
<li>Elements are a function <span class="math inline">\(\kernel_{i,j} = \kernel\left(\inputVector_i, \inputVector_j\right)\)</span></li>
</ul>
<p><span class="math display">\[
\kernelMatrix = \alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye.
\]</span></p>
</section>
<section id="covariance-function" class="slide level3">
<h3>Covariance Function</h3>
<p><span class="math display">\[
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)
\]</span></p>
<ul>
<li>formed by inner products of the rows of the <em>design matrix</em>.</li>
</ul>
</section>
<section id="gaussian-process" class="slide level3">
<h3>Gaussian Process</h3>
<ul>
<li><p>Instead of making assumptions about our density over each data point, <span class="math inline">\(\dataScalar_i\)</span> as i.i.d.</p></li>
<li><p>make a joint Gaussian assumption over our data.</p></li>
<li><p>covariance matrix is now a function of both the parameters of the activation function, <span class="math inline">\(\mappingMatrix_1\)</span>, and the input variables, <span class="math inline">\(\inputMatrix\)</span>.</p></li>
<li><p>Arises from integrating out <span class="math inline">\(\mappingVector^{(2)}\)</span>.</p></li>
</ul>
</section>
<section id="basis-functions" class="slide level3">
<h3>Basis Functions</h3>
<ul>
<li>Can be very complex, such as deep kernels, <span class="citation" data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or could even put a convolutional neural network inside.</li>
<li>Viewing a neural network in this way is also what allows us to beform sensible <em>batch</em> normalizations <span class="citation" data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</li>
</ul>
</section>
<section id="non-degenerate-gaussian-processes" class="slide level3">
<h3>Non-degenerate Gaussian Processes</h3>
<ul>
<li>This process is <em>degenerate</em>.</li>
<li>Covariance function is of rank at most <span class="math inline">\(\numHidden\)</span>.</li>
<li>As <span class="math inline">\(\numData \rightarrow \infty\)</span>, covariance matrix is not full rank.</li>
<li>Leading to <span class="math inline">\(\det{\kernelMatrix} = 0\)</span></li>
</ul>
</section>
<section id="infinite-networks" class="slide level3">
<h3>Infinite Networks</h3>
<ul>
<li>In ML Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> asked “what would happen if you took <span class="math inline">\(\numHidden \rightarrow \infty\)</span>?”</li>
</ul>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<center>
<em>Page 37 of <a href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s 1994 thesis</a> </em>
</center>
</section>
<section id="roughly-speaking" class="slide level3">
<h3>Roughly Speaking</h3>
<ul>
<li>Instead of</li>
</ul>
<p><span class="math display">\[
  \begin{align*}
  \kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) &amp; = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)\\
  &amp; = \alpha \sum_k \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_j\right)
  \end{align*}
  \]</span></p>
<ul>
<li>Sample infinitely many from a prior density, <span class="math inline">\(p(\mappingVector^{(1)})\)</span>,</li>
</ul>
<p><span class="math display">\[
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \int \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}, \inputVector_j\right) p(\mappingVector^{(1)}) \text{d}\mappingVector^{(1)}
\]</span></p>
<ul>
<li>Also applies for non-Gaussian <span class="math inline">\(p(\mappingVector^{(1)})\)</span> because of the <em>central limit theorem</em>.</li>
</ul>
</section>
<section id="simple-probabilistic-program" class="slide level3">
<h3>Simple Probabilistic Program</h3>
<ul>
<li><p>If <span class="math display">\[
  \begin{align*} 
  \mappingVector^{(1)} &amp; \sim p(\cdot)\\ \phi_i &amp; = \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right), 
  \end{align*}
  \]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a Gaussian process.</p></li>
</ul>
</section>
<section id="further-reading" class="slide level3">
<h3>Further Reading</h3>
<ul>
<li><p>Chapter 2 of Neal’s thesis <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>Rest of Neal’s thesis. <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>David MacKay’s PhD thesis <span class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span></p></li>
</ul>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-6" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<!-- ### Two Dimensional Gaussian Distribution -->
<!-- include{_ml/includes/two-d-gaussian.md} -->
</section>
<section id="distributions-over-functions" class="slide level3">
<h3>Distributions over Functions</h3>
</section>
<section id="sampling-a-function" class="slide level3">
<h3>Sampling a Function</h3>
<p><strong>Multi-variate Gaussians</strong></p>
<ul>
<li>We will consider a Gaussian with a particular structure of covariance matrix.</li>
<li>Generate a single sample from this 25 dimensional Gaussian density, <span class="math display">\[
\mappingFunctionVector=\left[\mappingFunction_{1},\mappingFunction_{2}\dots \mappingFunction_{25}\right].
\]</span></li>
<li>We will plot these points against their index.</li>
</ul>
</section>
<section id="gaussian-distribution-sample" class="slide level3">
<h3>Gaussian Distribution Sample</h3>
<script>
showDivs(0, 'two_point_sample');
</script>
<small></small> <input id="range-two_point_sample" type="range" min="0" max="8" value="0" onchange="setDivs('two_point_sample')" oninput="setDivs('two_point_sample')">
<button onclick="plusDivs(-1, 'two_point_sample')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample')">
❯
</button>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample000.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample001.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample002.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample003.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample004.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample005.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample006.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample007.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample008.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="prediction-of-mappingfunction_2-from-mappingfunction_1" class="slide level3">
<h3>Prediction of <span class="math inline">\(\mappingFunction_{2}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h3>
<script>
showDivs(9, 'two_point_sample2');
</script>
<small></small> <input id="range-two_point_sample2" type="range" min="9" max="12" value="9" onchange="setDivs('two_point_sample2')" oninput="setDivs('two_point_sample2')">
<button onclick="plusDivs(-1, 'two_point_sample2')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample2')">
❯
</button>
<div class="two_point_sample2" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample009.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample2" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample010.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample2" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample011.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample2" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample012.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="uluru" class="slide level3">
<h3>Uluru</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/gp/799px-Uluru_Panorama.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="prediction-with-correlated-gaussians" class="slide level3">
<h3>Prediction with Correlated Gaussians</h3>
<ul>
<li>Prediction of <span class="math inline">\(\mappingFunction_2\)</span> from <span class="math inline">\(\mappingFunction_1\)</span> requires <em>conditional density</em>.</li>
<li>Conditional density is <em>also</em> Gaussian. <span class="math display">\[
p(\mappingFunction_2|\mappingFunction_1) = \gaussianDist{\mappingFunction_2}{\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1}{ \kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}}
\]</span> where covariance of joint density is given by <span class="math display">\[
\kernelMatrix = \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}.\end{bmatrix}
\]</span></li>
</ul>
</section>
<section id="prediction-of-mappingfunction_8-from-mappingfunction_1" class="slide level3">
<h3>Prediction of <span class="math inline">\(\mappingFunction_{8}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h3>
<script>
showDivs(13, 'two_point_sample3');
</script>
<small></small> <input id="range-two_point_sample3" type="range" min="13" max="17" value="13" onchange="setDivs('two_point_sample3')" oninput="setDivs('two_point_sample3')">
<button onclick="plusDivs(-1, 'two_point_sample3')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample3')">
❯
</button>
<div class="two_point_sample3" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample013.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample3" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample014.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample3" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample015.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample3" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample016.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<div class="two_point_sample3" style="text-align:center;">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/two_point_sample017.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="key-object" class="slide level3">
<h3>Key Object</h3>
<ul>
<li>Covariance function, <span class="math inline">\(\kernelMatrix\)</span></li>
<li>Determines properties of samples.</li>
<li>Function of <span class="math inline">\(\inputMatrix\)</span>, <span class="math display">\[\kernelScalar_{i,j} = \kernelScalar(\inputVector_i, \inputVector_j)\]</span></li>
</ul>
</section>
<section id="linear-algebra" class="slide level3">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean <span class="math display">\[\mappingFunction_D(\inputVector_*) = \kernelVector(\inputVector_*, \inputMatrix) \kernelMatrix^{-1}
\mathbf{y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* = \kernelMatrix_{*,*} - \kernelMatrix_{*,\mappingFunctionVector}
\kernelMatrix^{-1} \kernelMatrix_{\mappingFunctionVector, *}\]</span></p></li>
</ul>
</section>
<section id="linear-algebra-1" class="slide level3">
<h3>Linear Algebra</h3>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[\mappingFunction_D(\inputVector_*) = \kernelVector(\inputVector_*, \inputMatrix) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\covarianceMatrix_* = \kernelMatrix_{*,*} - \kernelMatrix_{*,\mappingFunctionVector}
\kernelMatrix^{-1} \kernelMatrix_{\mappingFunctionVector, *}\]</span></p></li>
</ul>
</section>
<section id="section-7" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-8" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-9" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-10" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="section-11" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level3">
<h3>Exponentiated Quadratic Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)\]</span>
</center>
<table>
<tr>
<td width="40%">
<div style="text-align:">
<object class align data="../slides/diagrams/kern/eq_covariance.svg" style="vertical-align:middle">
</object>
</div>
</td>
<td width="40%">
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data" class="slide level3">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3">
<h3>Olympic Marathon Data</h3>
<div style="text-align:center">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/datasets/olympic-marathon.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="section-12" class="slide level3">
<h3></h3>
<table>
<tr>
<td width="40%">
<img class="" src="../slides/diagrams/turing-run.jpg" width="" height="auto" align="" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</td>
<td width="50%">
<img class="" src="../slides/diagrams/turing-times.gif" width="" height="auto" align="" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-gp" class="slide level3">
<h3>Olympic Marathon Data GP</h3>
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/olympic-marathon-gp.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="learning-covariance-parameters" class="slide level3">
<h3>Learning Covariance Parameters</h3>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-13" class="slide level3">
<h3></h3>
<p><span class="math display">\[\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}\]</span></p>
</section>
<section id="section-14" class="slide level3">
<h3></h3>
<p><span class="math display">\[\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\color{yellow} \det{\kernelMatrix}^{\frac{1}{2}}}}{\color{cyan}\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-15" class="slide level3">
<h3></h3>
<p><span class="math display">\[
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;{\color{yellow}-\frac{1}{2}\log\det{\kernelMatrix}}{\color{cyan}-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\errorFunction(\parameterVector) = {\color{yellow} \frac{1}{2}\log\det{\kernelMatrix}} + {\color{cyan} \frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
\]</span></p>
</section>
<section id="section-16" class="slide level3">
<h3></h3>
<p>The parameters are <em>inside</em> the covariance function (matrix). <span class="math display">\[\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)\]</span></p>
</section>
<section id="eigendecomposition-of-covariance" class="slide level3">
<h3>Eigendecomposition of Covariance</h3>
<p><span><span class="math display">\[\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\eigenvalueMatrix\)</span> represents distance on axes. <span class="math inline">\(\rotationMatrix\)</span> gives rotation.
</td>
</tr>
</table>
</section>
<section id="eigendecomposition-of-covariance-1" class="slide level3">
<h3>Eigendecomposition of Covariance</h3>
<ul>
<li><span class="math inline">\(\eigenvalueMatrix\)</span> is <em>diagonal</em>, <span class="math inline">\(\rotationMatrix^\top\rotationMatrix = \eye\)</span>.</li>
<li>Useful representation since <span class="math inline">\(\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2\)</span>.</li>
</ul>
</section>
<section id="capacity-control-coloryellow-log-detkernelmatrix" class="slide level3">
<h3>Capacity control: <span class="math inline">\({\color{yellow} \log \det{\kernelMatrix}}\)</span></h3>
<!--
\only<1>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant1}}\only<2>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant2}}\only<3>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant3}}\only<4>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant4}}\only<5>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant5}}\only<6>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant6}}\only<7>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant7}}\only<8>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant8}}\only<9>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant9}}\only<10>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant10}}-->
</section>
<section id="data-fit-colorcyan-fracdatavectortopkernelmatrix-1datavector2" class="slide level3">
<h3>Data Fit: <span class="math inline">\({\color{cyan} \frac{\dataVector^\top\kernelMatrix^{-1}\dataVector}{2}}\)</span></h3>
<!--

\only<1>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic1}}\only<2>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic2}}\only<3>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic3}}-->
</section>
<section id="errorfunctionparametervector-coloryellowfrac12logdetkernelmatrixcolorcyanfracdatavectortopkernelmatrix-1datavector2" class="slide level3">
<h3><span class="math display">\[\errorFunction(\parameterVector) = {\color{yellow}\frac{1}{2}\log\det{\kernelMatrix}}+{\color{cyan}\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}\]</span></h3>
<script>
showDivs(0, 'gp-optimise');
</script>
<small></small> <input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')">
<button onclick="plusDivs(-1, 'gp-optimise')">
❮
</button>
<button onclick="plusDivs(1, 'gp-optimise')">
❯
</button>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise000.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise001.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise002.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise003.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise004.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise005.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise006.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise007.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise008.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise009.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise010.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise011.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise012.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise013.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise014.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise015.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise016.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise017.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise018.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise019.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise020.svg" style="vertical-align:middle;">
</object>
</div>
</td>
<td width="50%">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/gp-optimise021.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</div>
</section>
<section id="della-gatta-gene-data" class="slide level3">
<h3>Della Gatta Gene Data</h3>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level3">
<h3>Della Gatta Gene Data</h3>
<div style="text-align:center">
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/datasets/della-gatta-gene.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="gene-expression-example" class="slide level3">
<h3>Gene Expression Example</h3>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-17" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/health/1471-2105-12-180_1.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-18" class="slide level3">
<h3></h3>
</section>
<section id="tp53-gene-data-gp" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/della-gatta-gene-gp.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="tp53-gene-data-gp-1" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/della-gatta-gene-gp2.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="tp53-gene-data-gp-2" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/della-gatta-gene-gp3.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="multiple-optima" class="slide level3">
<h3>Multiple Optima</h3>
<div style="text-align:">
<object class="svgplot " align data="../slides/diagrams/gp/multiple-optima000.svg" style="vertical-align:middle;">
</object>
</div>
<!--### Multiple Optima


<div style="text-align:" class=""><object class="svgplot " align="" data="../slides/diagrams/gp/multiple-optima001.svg" style="vertical-align:middle;"></object></div>-->
</section>
<section id="example-prediction-of-malaria-incidence-in-uganda" class="slide level3">
<h3>Example: Prediction of Malaria Incidence in Uganda</h3>
<p><span style="text-align:right"><img class="" src="../slides/diagrams/people/2013_03_28_180606.JPG" width="1.5cm" align="" style="background:none; border:none; box-shadow:none; position:absolute; clip:rect(2662px,1780px,1110px,600px);vertical-align:middle"></span></p>
<ul>
<li>Work with Ricardo Andrade Pacheco, John Quinn and Martin Mubaganzi (Makerere University, Uganda)</li>
<li>See <a href="http://air.ug/research.html">AI-DEV Group</a>.</li>
</ul>
</section>
<section id="malaria-prediction-in-uganda" class="slide level3">
<h3>Malaria Prediction in Uganda</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<center>
<em>Data SRTM/NASA from <a href="https://dds.cr.usgs.gov/srtm/version2_1" class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a> </em>
</center>
<p><span style="text-align:right"><span class="citation" data-cites="Andrade:consistent14">(Andrade-Pacheco et al., 2014,<span class="citation" data-cites="Mubangizi:malaria14">Mubangizi et al. (2014)</span>)</span></span></p>
</section>
<section id="kapchorwa-district" class="slide level3">
<h3>Kapchorwa District</h3>
<div style="text-align:">
<object class align data="../slides/diagrams/health/Kapchorwa_District_in_Uganda.svg" style="vertical-align:middle">
</object>
</div>
</section>
<section id="malaria-prediction-in-nagongera-sentinel-site" class="slide level3">
<h3>Malaria Prediction in Nagongera (Sentinel Site)</h3>
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="mubende-district" class="slide level3">
<h3>Mubende District</h3>
<div class="center" style="text-align:svgplot_normal">
<object class="center" align="svgplot_normal" data="../slides/diagrams/health/Mubende_District_in_Uganda.svg" style="vertical-align:middle">
</object>
</div>
</section>
<section id="malaria-prediction-in-uganda-1" class="slide level3">
<h3>Malaria Prediction in Uganda</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/health/mubende.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="gp-school-at-makerere" class="slide level3">
<h3>GP School at Makerere</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="kabarole-district" class="slide level3">
<h3>Kabarole District</h3>
<div style="text-align:">
<object class align data="../slides/diagrams/health/Kabarole_District_in_Uganda.svg" style="vertical-align:middle">
</object>
</div>
</section>
<section id="early-warning-systems" class="slide level3">
<h3>Early Warning Systems</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="early-warning-systems-1" class="slide level3">
<h3>Early Warning Systems</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="additive-covariance" class="slide level3">
<h3>Additive Covariance</h3>
<center>
<span class="math display">\[\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)\]</span>
</center>
<table>
<tr>
<td width="40%">
<div style="text-align:">
<object class align data="../slides/diagrams/kern/add_covariance.svg" style="vertical-align:middle">
</object>
</div>
</td>
<td width="40%">
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="section-19" class="slide level3">
<h3></h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</section>
<section id="gelman-book" class="slide level3">
<h3>Gelman Book</h3>
<table>
<tr>
<td width="50%">
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</td>
<td width="50%">
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</td>
</tr>
</table>
<p><span class="citation" data-cites="Gelman:bayesian13">Gelman et al. (2013)</span></p>
</section>
<section id="basis-function-covariance" class="slide level3">
<h3>Basis Function Covariance</h3>
<center>
<span class="math display">\[\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)\]</span>
</center>
<table>
<tr>
<td width="40%">
<div style="text-align:">
<object class align data="../slides/diagrams/kern/basis_covariance.svg" style="vertical-align:middle">
</object>
</div>
</td>
<td width="40%">
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</td>
</tr>
</table>
</section>
<section id="brownian-covariance" class="slide level3">
<h3>Brownian Covariance</h3>
<p><span class="math display">\[
\kernelScalar(t, t^\prime) = \alpha \min(t, t^\prime)
\]</span></p>
<!--<table><tr><td width="50%">
<div style="text-align:" class=""><object class="svgplot " align="" data="../slides/diagrams/kern/brownian_covariance.svg" style="vertical-align:middle;"></object></div>
</td><td width="50%">
<iframe src="../slides/diagrams/kern/brownian_covariance.html" width="512" height="384" allowtransparency="true" frameborder="0">
</iframe>
</td></tr></table>
-->
<table>
<tr>
<td width="45%">
<div style="text-align:">
<object class align data="../slides/diagrams/kern/brownian_covariance.svg" style="vertical-align:middle">
</object>
</div>
</td>
<td width="45%">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</td>
</tr>
</table>
</section>
<section id="mlp-covariance" class="slide level3">
<h3>MLP Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)\]</span>
</center>
<table>
<tr>
<td width="40%">
<div style="text-align:">
<object class align data="../slides/diagrams/kern/mlp_covariance.svg" style="vertical-align:middle">
</object>
</div>
</td>
<td width="40%">
<div style="text-align:center;vertical-align:middle">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
</td>
</tr>
</table>
<!--include{_gp/includes/gp-optimize.md}-->
</section>
<section id="gpss-gaussian-process-summer-school" class="slide level3">
<h3>GPSS: Gaussian Process Summer School</h3>
<table>
<tr>
<td width="60%">
<ul>
<li><a href="http://gpss.cc" class="uri">http://gpss.cc</a></li>
<li>Next one is in Sheffield in <em>September 2019</em>.</li>
<li>Many lectures from past meetings available online</li>
</ul>
</td>
<td width="40%">
<div style="text-align:1.5cm">
<object class="svgplot " align="1.5cm" data="../slides/diagrams/logo/gpss-logo.svg" style="vertical-align:middle;">
</object>
</div>
</td>
</tr>
</table>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level3">
<h3>GPy: A Gaussian Process Framework in Python</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level3">
<h3>GPy: A Gaussian Process Framework in Python</h3>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level3">
<h3>Features</h3>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="other-software" class="slide level3">
<h3>Other Software</h3>
<ul>
<li><a href="https://github.com/GPflow/GPflow">GPflow</a></li>
<li><a href="https://github.com/cornellius-gp/gpytorch">GPyTorch</a></li>
</ul>
</section>
<section id="mxfusion-modular-probabilistic-programming-on-mxnet" class="slide level3">
<h3>MXFusion: Modular Probabilistic Programming on MXNet</h3>
<div style="text-align:center;vertical-align:middle">
<img class="" src="../slides/diagrams/ml/mxfusion.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</div>
<center>
<a href="https://github.com/amzn/MXFusion" class="uri">https://github.com/amzn/MXFusion</a>
</center>
</section>
<section id="mxfusion" class="slide level3">
<h3>MxFusion</h3>
<table>
<tr>
<td width="70%">
<ul>
<li>Work by Eric Meissner and Zhenwen Dai.</li>
<li>Probabilistic programming.</li>
<li>Available on <a href="https://github.com/amzn/mxfusion">Github</a>
</td>
<td width="30%">
<img class="" src="../slides/diagrams/mxfusion-logo.png" width="" height="auto" align="" style="background:none; border:none; box-shadow:none;vertical-align:middle">
</td>
</tr>
</table></li>
</ul>
</section>
<section id="acknowledgments" class="slide level3">
<h3>Acknowledgments</h3>
<p>Stefanos Eleftheriadis, John Bronskill, Hugh Salimbeni, Rich Turner, Zhenwen Dai, Javier Gonzalez, Andreas Damianou, Mark Pullin, Michael Smith, James Hensman, John Quinn, Martin Mubangizi.</p>
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: @lawrennd</li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Andrade:consistent14">
<p>Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014. Consistent mapping of government malaria records across a changing territory delimitation. Malaria Journal 13. <a href="https://doi.org/10.1186/1475-2875-13-S1-P5" class="uri">https://doi.org/10.1186/1475-2875-13-S1-P5</a></p>
</div>
<div id="ref-Cho:deep09">
<p>Cho, Y., Saul, L.K., 2009. Kernel methods for deep learning, in: Bengio, Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.</p>
</div>
<div id="ref-DellaGatta:direct08">
<p>Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D., Missero, C., Bernardo, D. di, 2008. Direct targets of the trp63 transcription factor revealed by a combination of gene expression profiling and reverse engineering. Genome Research 18, 939–948. <a href="https://doi.org/10.1101/gr.073601.107" class="uri">https://doi.org/10.1101/gr.073601.107</a></p>
</div>
<div id="ref-Gelman:bayesian13">
<p>Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.</p>
</div>
<div id="ref-Ioffe:batch15">
<p>Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: Bach, F., Blei, D. (Eds.), Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, Lille, France, pp. 448–456.</p>
</div>
<div id="ref-Kalaitzis:simple11">
<p>Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking differentially expressed gene expression time courses through Gaussian process regression. BMC Bioinformatics 12. <a href="https://doi.org/10.1186/1471-2105-12-180" class="uri">https://doi.org/10.1186/1471-2105-12-180</a></p>
</div>
<div id="ref-MacKay:bayesian92">
<p>MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis). California Institute of Technology.</p>
</div>
<div id="ref-McCulloch:neuron43">
<p>McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5, 115–133.</p>
</div>
<div id="ref-Mubangizi:malaria14">
<p>Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence, N.D., 2014. Malaria surveillance with multiple data sources using Gaussian process models, in: 1st International Conference on the Use of Mobile Ict in Africa.</p>
</div>
<div id="ref-Neal:bayesian94">
<p>Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis). Dept. of Computer Science, University of Toronto.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine learning. mit, Cambridge, MA.</p>
</div>
<div id="ref-Steele:predictive12">
<p>Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson, E., Avital, I., Stojadinovic, A., 2012. Using machine-learned Bayesian belief networks to predict perioperative risk of clostridium difficile infection following colon surgery. Interact J Med Res 1, e6. <a href="https://doi.org/10.2196/ijmr.2131" class="uri">https://doi.org/10.2196/ijmr.2131</a></p>
</div>
<div id="ref-Tipping:probpca99">
<p>Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a href="https://doi.org/doi:10.1111/1467-9868.00196" class="uri">https://doi.org/doi:10.1111/1467-9868.00196</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
