<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2019-06-03">
  <title>What is Machine Learning?</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="figure-animate.js"></script>
</head>
<body>
\[\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">What is Machine Learning?</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2019-06-03</time></p>
  <p class="venue" style="text-align:center">Data Science Africa Summer School, Addis Ababa, Ethiopia</p>
</section>

<section class="slide level3">

<!-- Front matter -->
<!---->
<!--Back matter-->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="introduction" class="slide level3">
<h3>Introduction</h3>
<ul>
<li><p>General introduction to machine learning.</p></li>
<li><p>Highlight technical challenges and current solutions.</p></li>
<li><p>What is machine learning? And why is it important?</p></li>
</ul>
</section>
<section id="rise-of-machine-learning" class="slide level3">
<h3>Rise of Machine Learning</h3>
<ul>
<li><p>Driven by data and computation</p></li>
<li><p>Fundamentally dependent on models</p></li>
</ul>
<p><span class="math display">\[
\text{data} + \text{model} + \text{compute} \rightarrow \text{prediction}
\]</span></p>
</section>
<section id="data-revolution" class="slide level3">
<h3>Data Revolution</h3>
<div class="figure">
<div id="data-science-information-flow-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/data-science/new-flow-of-information.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="efficiency" class="slide level3">
<h3>Efficiency</h3>
<ul>
<li>Economies driven by ‘production’.</li>
<li>Greater production comes with better efficiency.
<ul>
<li>E.g. moving from gathering food to settled agriculture.</li>
</ul></li>
<li>In the modern era one approach to becoming more efficient is automation of processes.
<ul>
<li>E.g. manufacturing production lines</li>
</ul></li>
</ul>
</section>
<section id="physical-processes" class="slide level3">
<h3>Physical Processes</h3>
<ul>
<li>Manufacturing processes consist of production lines and robotic automation.</li>
<li>Logistics can also be decomposed into the supply chain processes.</li>
<li>Efficiency can be improved by automation.</li>
</ul>
</section>
<section id="goods-and-information" class="slide level3">
<h3>Goods and Information</h3>
<ul>
<li>For modern society: management of flow of goods and information.</li>
<li>Flow of information is highly automated.</li>
<li>Processing of data is decomposed into stages in computer code.</li>
</ul>
</section>
<section id="intervention" class="slide level3">
<h3>Intervention</h3>
<ul>
<li>For all cases: manufacturing, logistics, data management</li>
<li>Pipeline requires human intervention from an operator.</li>
<li>Interventions create bottlenecks, slow the process.</li>
<li>Machine learning is a key technology in automating these manual stages.</li>
</ul>
</section>
<section id="long-grass" class="slide level3">
<h3>Long Grass</h3>
<ul>
<li>Easy to replicate interventions have already been dealt with.</li>
<li>Components that still require human intervention are the knottier problems.</li>
<li>Difficult decompose into stages which could then be further automated.</li>
<li>These components are ‘process-atoms’.</li>
<li>These are the “long grass” regions of technology.</li>
</ul>
</section>
<section id="nature-of-challenge" class="slide level3">
<h3>Nature of Challenge</h3>
<ul>
<li>In manufacturing or logistics settings atoms are flexible manual skills.
<ul>
<li>Requires emulation of a human’s motor skills.</li>
</ul></li>
<li>In information processing: our flexible cognitive skills.
<ul>
<li>Our ability to mentally process an image or some text.</li>
</ul></li>
</ul>
</section>
<section id="worked-example-delivery-drones" class="slide level3">
<h3>Worked Example: Delivery Drones</h3>
<div class="figure">
<div id="amazon-drone-delivery-figure" class="figure-frame">
<iframe width="100%" height="auto" src="https://www.youtube.com/embed/vNySOrI2Ny8?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
</section>
<section id="data-driven" class="slide level3">
<h3>Data Driven</h3>
<ul>
<li>Machine Learning: Replicate Processes through <em>direct use of data</em>.</li>
<li>Aim to emulate cognitive processes through the use of data.</li>
<li>Use data to provide new approaches in control and optimization that should allow for emulation of human motor skills.</li>
</ul>
</section>
<section id="process-emulation" class="slide level3">
<h3>Process Emulation</h3>
<ul>
<li>Key idea: emulate the process as a mathematical function.</li>
<li>Each function has a set of <em>parameters</em> which control its behavior.</li>
<li><em>Learning</em> is the process of changing these parameters to change the shape of the function</li>
<li>Choice of which class of mathematical functions we use is a vital component of our <em>model</em>.</li>
</ul>
</section>
<section id="polynomial-fit" class="slide level3">
<h3>Polynomial Fit</h3>
<div class="figure">
<div id="olympic-loo000-lm-polynomial-num-basis-001-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_loo000_LM_polynomial_number001.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="polynomial-fit-1" class="slide level3">
<h3>Polynomial Fit</h3>
<div class="figure">
<div id="olympic-loo000-lm-polynomial-num-basis-002-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_loo000_LM_polynomial_number002.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="polynomial-fit-2" class="slide level3">
<h3>Polynomial Fit</h3>
<div class="figure">
<div id="olympic-loo000-lm-polynomial-num-basis-003-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_loo000_LM_polynomial_number003.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="polynomial-fit-3" class="slide level3">
<h3>Polynomial Fit</h3>
<div class="figure">
<div id="olympic-loo000-lm-polynomial-num-basis-004-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_loo000_LM_polynomial_number004.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="polynomial-fit-4" class="slide level3">
<h3>Polynomial Fit</h3>
<div class="figure">
<div id="olympic-loo000-lm-polynomial-num-basis-005-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_loo000_LM_polynomial_number005.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="polynomial-fit-5" class="slide level3">
<h3>Polynomial Fit</h3>
<div class="figure">
<div id="olympic-loo000-lm-polynomial-num-basis-006-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_loo000_LM_polynomial_number006.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="polynomial-fit-6" class="slide level3">
<h3>Polynomial Fit</h3>
<div class="figure">
<div id="olympic-loo000-lm-polynomial-num-basis-007-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/olympic_loo000_LM_polynomial_number007.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="artificial-intelligence" class="slide level3">
<h3>Artificial Intelligence</h3>
<ul>
<li>ML is the principal technology underlying the recent advances in artificial intelligence techniques.</li>
<li>Different approach to that developed in classical artificial intelligence (sometimes referred to as “good old fashioned AI” or GOFAI).</li>
<li>GOFAI relied on symbolic logic as its mathematical engine.</li>
</ul>
</section>
<section id="artificial-intelligence-1" class="slide level3">
<h3>Artificial Intelligence</h3>
<ul>
<li>Early AI used expert systems: a set of logical rules implemented to reconstruct expertise. For example, rules to decide whether or not someone has cancer.</li>
<li>Such rules prove hard to specify for very complex processes.</li>
</ul>
</section>
<section id="data-science" class="slide level3">
<h3>Data Science</h3>
<ul>
<li>Can split applications of machine learning broadly into <em>data science</em> and <em>artificial intelligence</em>.</li>
<li><em>Data science</em>: making sense of ‘new data’, the large volumes of data from sensors and increased interconnectivity (big data, IoT)</li>
<li>Classical statistics: the question is formed first, and data is later.</li>
</ul>
</section>
<section id="data-science-1" class="slide level3">
<h3>Data Science</h3>
<ul>
<li>Data Science: data is first, questions come later.</li>
<li>Overlap through <em>exploratory data analysis</em></li>
</ul>
</section>
<section id="artificial-intelligence-2" class="slide level3">
<h3>Artificial Intelligence</h3>
<ul>
<li><em>Artificial intelligence</em> originates in <em>cybernetics</em></li>
<li>Challenge to recreate ‘intelligent’ behaviour.</li>
<li>Either <em>general intelligence</em> or emulate <em>human</em> capabilities.</li>
<li>Machine learning is important because of success of data-driven artificial intelligence.</li>
<li>Data-driven artificial intelligence: instead of solving from first principles, collect data.</li>
</ul>
</section>
<section id="machine-learning" class="slide level3">
<h3>Machine Learning</h3>
<ol type="1">
<li>observe a system in practice</li>
<li>emulate its behavior with mathematics.</li>
</ol>
<ul>
<li><p>Design challenge: where to put mathematical function.</p></li>
<li><p>Where it’s placed leads to different ML domains.</p></li>
</ul>
</section>
<section id="supervised-learning" class="slide level3">
<h3>Supervised Learning</h3>
<ul>
<li>Most widely deployed machine learning technology.
<ul>
<li>Particular domain of success has been <em>classification</em>.</li>
</ul></li>
<li>Take input to function (e.g. image)</li>
<li>Use output to place it in a class (e.g. dog or cat).</li>
<li>Simple idea underpins a lot of machine learning.</li>
</ul>
</section>
<section id="perceptron" class="slide level3">
<h3>Perceptron</h3>
<script>
showDivs(1, 'perceptron-algorithm');
</script>
<p><small></small> <input id="range-perceptron-algorithm" type="range" min="1" max="44" value="1" onchange="setDivs('perceptron-algorithm')" oninput="setDivs('perceptron-algorithm')"> <button onclick="plusDivs(-1, 'perceptron-algorithm')">❮</button> <button onclick="plusDivs(1, 'perceptron-algorithm')">❯</button></p>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron001.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron002.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron003.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron004.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron005.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron006.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron007.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron008.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron009.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron010.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron011.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron012.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron013.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron014.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron015.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron016.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron017.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron018.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron019.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron020.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron021.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron022.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron023.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron024.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron025.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron026.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron027.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron028.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron029.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron030.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron031.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron032.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron033.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron034.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron035.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron036.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron037.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron038.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron039.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron040.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron041.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron042.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron043.svg" width="80%" style=" ">
</object>
</div>
<div class="peceptron-algorithm" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/perceptron044.svg" width="80%" style=" ">
</object>
</div>
<p><em>Simple classification with the perceptron algorithm.</em> }</p>
</section>
<section id="classification" class="slide level3">
<h3>Classification</h3>
<ul>
<li>Examples in a speech based intelligent agent: <em>wake word</em> classification.</li>
<li>Major breakthrough for images was in 2012 with the ImageNet result of <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-">Alex Krizhevsky, Ilya Sutskever and Geoff Hinton</a></li>
<li>ImageNet is a large data base of 14 million images with many thousands of classes.</li>
<li>Data is used in a community-wide challenge for object categorization.</li>
<li>Krizhevsky et al used <em>convolutional neural networks</em>.</li>
</ul>
</section>
<section id="supervised-learning-1" class="slide level3">
<h3>Supervised Learning</h3>
<ul>
<li>Inputs, <span class="math inline">\(\inputVector\)</span>, mapped to a label, <span class="math inline">\(\dataScalar\)</span>, through a function <span class="math inline">\(\mappingFunction(\cdot)\)</span> dependent on parameters, <span class="math inline">\(\weightVector\)</span>, <span class="math display">\[
\dataScalar = \mappingFunction(\inputVector; \weightVector).
\]</span></li>
<li><span class="math inline">\(\mappingFunction(\cdot)\)</span> is known as the <em>prediction function</em>.</li>
</ul>
</section>
<section id="supervised-learning-challenges" class="slide level3">
<h3>Supervised Learning Challenges</h3>
<ol type="1">
<li>choosing which features, <span class="math inline">\(\inputVector\)</span>, are relevant in the prediction</li>
<li>defining the appropriate <em>class of function</em>, <span class="math inline">\(\mappingFunction(\cdot)\)</span>.</li>
<li>selecting the right parameters, <span class="math inline">\(\weightVector\)</span>.</li>
</ol>
</section>
<section id="feature-selection" class="slide level3">
<h3>Feature Selection</h3>
<ul>
<li>Olympic prediction example only using year to predict pace.</li>
<li>Could also use <em>course characteristics</em> (e.g. how hilly) <em>temperature</em>.</li>
<li>Can use <em>feature selection algorithms</em></li>
<li>Automate the process of finding the features that we need.</li>
</ul>
</section>
<section id="applications" class="slide level3">
<h3>Applications</h3>
<ul>
<li>rank search results, decide which adverts to serve or, what appears at the top of your newsfeed.</li>
<li>Facebook features include number of likes, whether it’s got an image, whether this is a friend you interact with.</li>
<li>Newsfeed ranking algorithm is critical to Facebook’s success.</li>
<li>Heavy investments in machine learning pipelines for evaluation of the feature utility.</li>
</ul>
</section>
<section id="class-of-function-mappingfunctioncdot" class="slide level3">
<h3>Class of Function, <span class="math inline">\(\mappingFunction(\cdot)\)</span></h3>
<ul>
<li>What should be the characteristics of the mapping between <span class="math inline">\(\inputVector\)</span> and <span class="math inline">\(\dataScalar\)</span>?</li>
<li>Often we choose it to be <em>smooth</em> (similar inputs lead to similar outputs).</li>
<li>Often choose it to be a linear function.</li>
<li>In <em>forecasting</em> we might want a <em>periodic</em> function (weekly or seasonal effects).</li>
</ul>
</section>
<section id="class-of-function-neural-networks" class="slide level3">
<h3>Class of Function: Neural Networks</h3>
<ul>
<li>For imagenet prediction function was a <em>convolutional neural network</em></li>
<li>A convolutional neural network introduces <em>invariances</em> into the function that are particular to image classification.</li>
</ul>
</section>
<section id="class-of-function-invariances" class="slide level3">
<h3>Class of Function: Invariances</h3>
<ul>
<li>An invariance is a transformation of the input that we don’t want to effect the output.
<ul>
<li>e.g. a cat remains a cat regardless of location (translation), size (scale) or upside-down (rotation and reflection).</li>
<li>Convolutional neural networks encode invariances in the mathematical function.</li>
</ul></li>
</ul>
</section>
<section id="encoding-knowledge" class="slide level3">
<h3>Encoding Knowledge</h3>
<ul>
<li>Encoding invariance is like encoding knowledge in the model.</li>
<li>If we don’t specify these invariances then the model must learn them.</li>
<li>Learning invariances requires a <em>lot</em> more data.</li>
<li>The amount of data required to achieve a certain performance is the <em>data efficiency</em>.</li>
</ul>
</section>
<section id="choosing-prediction-function" class="slide level3">
<h3>Choosing Prediction Function</h3>
<ul>
<li>Prediction function can be any set of parameterized functions.</li>
<li>In the Olympic marathon example above we used a polynomial fit, <span class="math display">\[
\mappingFunction(\inputScalar) = \weightScalar_0 + \weightScalar_1 \inputScalar+ \weightScalar_2 \inputScalar^2 + \weightScalar_3 \inputScalar^3 + \weightScalar_4 \inputScalar^4.
\]</span></li>
<li>Olympic example is a supervised learning challenge. But it is a <em>regression</em> problem.</li>
<li>A regression problem is one where the output is a continuous value (such as the pace in the marathon).</li>
</ul>
</section>
<section id="regression-problems" class="slide level3">
<h3>Regression Problems</h3>
<ul>
<li>In classification the output of prediction function is constrained to be discrete.</li>
<li>An early example of a regression data set used in machine learning was <a href="http://lib.stat.cmu.edu/datasets/tecator">the Tecator data</a>
<ul>
<li>Fat, water and protein content of meat samples was predicted as a function of the absorption of infrared light.</li>
</ul></li>
</ul>
</section>
<section id="parameter-estimation-objective-functions" class="slide level3">
<h3>Parameter Estimation: Objective Functions</h3>
<ul>
<li>After choosing <em>features</em> and <em>function class</em> we need <em>parameters</em>.</li>
<li>Estimate <span class="math inline">\(\weightVector\)</span> by specifying an <em>objective function</em>.</li>
<li>The objective function specifies the quality of the match between the prediction function and the <em>training data</em>.</li>
<li>In supervised learning the objective function has input data (for ImageNet, the image, for Olympic marathon the year) and a <em>label</em>.</li>
</ul>
</section>
<section id="labels-and-squared-error" class="slide level3">
<h3>Labels and Squared Error</h3>
<ul>
<li><em>Label</em> is where the term supervised learning comes from.</li>
<li>Idea is that a supervisor, or annotator, has already given labels.</li>
<li>For regression problem, a typical objective function is the <em>squared error</em>, <span class="math display">\[
\errorFunction(\weightVector) = \sum_{i=1}^\numData (\dataScalar_i - \mappingFunction(\inputVector_i))^2
\]</span></li>
</ul>
</section>
<section id="data-provision" class="slide level3">
<h3>Data Provision</h3>
<ul>
<li>Here the data is provided to us as a set of <span class="math inline">\(n\)</span> inputs, <span class="math inline">\(\inputVector_1\)</span>, <span class="math inline">\(\inputVector_2\)</span>, <span class="math inline">\(\inputVector_3\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\inputVector_n\)</span></li>
<li>Each data point with an associated label, <span class="math inline">\(\dataScalar_1\)</span>, <span class="math inline">\(\dataScalar_2\)</span>, <span class="math inline">\(\dataScalar_3\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(\dataScalar_n\)</span>.</li>
<li>Sometimes the label is cheap to acquire.
<ul>
<li>E.g. in Newsfeed ranking Facebook are acquiring a label each time a user clicks on a post in their Newsfeed. (Data Science?)</li>
<li>In ad-click prediction labels are obtained whenever an advert is clicked.</li>
</ul></li>
</ul>
</section>
<section id="annotation" class="slide level3">
<h3>Annotation</h3>
<ul>
<li>Often we have to employ human annotators to label the data.
<ul>
<li>E.g. in ImageNet, the breakthrough deep learning result was annotated using Amazon’s Mechanical Turk. (AI?)</li>
</ul></li>
<li>Without large scale human input, would not have the breakthrough results in AI we have today.</li>
</ul>
</section>
<section id="annotation-1" class="slide level3">
<h3>Annotation</h3>
<ul>
<li>Some tasks <em>easier</em> to annotate than *others.
<ul>
<li>For Tecator data, need to acquire the actual values of water, protein and fat content in the meat samples.</li>
<li>Each data point requires real experiments</li>
</ul></li>
</ul>
</section>
<section id="annotation-2" class="slide level3">
<h3>Annotation</h3>
<ul>
<li>Even for <em>easy</em> tasks there will be problems.
<ul>
<li>E.g. humans extrapolate the context of an image.</li>
</ul></li>
<li>Quality of any machine learning solution is very sensitive to the quality of annotated data we have.</li>
<li>Investing in processes and tools to improve annotation of data is therefore a strong priority.</li>
</ul>
</section>
<section id="misrepresentation" class="slide level3">
<h3>Misrepresentation</h3>
<ul>
<li>Can be significant problems with misrepresentation in the data sets.</li>
<li>If data isn’t collected carefully, it can reflect biases about the population that we don’t want our models to have.</li>
<li>A face detector using Californians may not perform well when deployed in Kampala, Uganda.</li>
</ul>
</section>
<section id="automation" class="slide level3">
<h3>Automation</h3>
<ul>
<li>Train supervised learning system, place it in production</li>
<li>Supervised learning is probably the dominant approaches to learning.</li>
<li>Cost and time associated with labeling data is a major bottleneck for deploying machine learning systems.</li>
<li>Creating training data requires significant human effort.</li>
</ul>
</section>
<section id="training-and-test" class="slide level3">
<h3>Training and Test</h3>
<ul>
<li>Very important distinction.
<ul>
<li>Separation between training data and test data (or production data).</li>
</ul></li>
<li>Training data is the data that was used to find the model parameters.</li>
<li>Test data (or production data) is the data that is used with the live system.</li>
<li>Ability of a machine learning system to predict well in production is known as its <em>generalization</em> ability.</li>
<li>System’s ability to predict in areas where it hasn’t previously seen data.</li>
</ul>
</section>
<section id="generalization-error" class="slide level3">
<h3>Generalization Error</h3>
<ul>
<li>Easy to develop a prediction function that reconstructs the training data exactly: a look up table. you can just use a look up table.</li>
<li>How would the lookup table predict <em>between</em> the training data, where examples haven’t been seen before?</li>
<li>Choice of the class of prediction functions is critical in ensuring that the model generalizes well.</li>
<li>Generalization error is normally estimated by applying the objective function to a set of data that the model <em>wasn’t</em> trained on: the <em>test data</em>.</li>
</ul>
</section>
<section id="performance" class="slide level3">
<h3>Performance</h3>
<ul>
<li>To ensure good performance we normally want a model that gives us a low generalization error.</li>
<li>If we weren’t sure of the right prediction function to use then we could try 1,000 different prediction functions.</li>
<li>We could use the one that gives us the lowest error on the test data.</li>
<li>But you have to be careful.</li>
</ul>
</section>
<section id="validation-and-model-selection" class="slide level3">
<h3>Validation and Model Selection</h3>
<ul>
<li>Selecting a model in this way is like a further stage of training where you are using the test data in the training.<a href="#/fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li>
<li>It is known as <em>validation</em>.</li>
<li>The error is known as <em>validation error</em>.</li>
<li>Using the validation error for model selection is a standard machine learning technique.</li>
</ul>
</section>
<section id="difficult-trap" class="slide level3">
<h3>Difficult Trap</h3>
<ul>
<li>All machine learning practitioners should know not to use the test data in your training procedure.</li>
<li>But because validation data is used for model selection, it is not an unbiased estimate of the generalization performance.</li>
</ul>
</section>
<section id="overfitting" class="slide level3">
<h3>Overfitting</h3>
<div class="figure">
<div id="alex-ihler-overfitting-figure" class="figure-frame">
<iframe width="100%" height="auto" src="https://www.youtube.com/embed/py8QrZPT48s?start=4m0s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<p><em>Alex Ihler on Polynomials and Overfitting</em></p>
</section>
<section id="unsupervised-learning" class="slide level3">
<h3>Unsupervised Learning</h3>
<ul>
<li>When you have data, <span class="math inline">\(\inputVector\)</span>, but no labels <span class="math inline">\(\dataVector\)</span>.</li>
<li>Extract <em>structure</em> from data.</li>
<li>Type of structure you are interested dependent on the broader context of the task.</li>
</ul>
</section>
<section id="context" class="slide level3">
<h3>Context</h3>
<ul>
<li>Supervised learning: context is very much driven by the labels.</li>
<li>Humans easily sort objects e.g. animals and vehicles</li>
<li>For large data sets or data we are not used to (high dimensional data) would like to automate.
<ul>
<li>E.g., an e-commerce company algorithm for sorting products into groups.</li>
</ul></li>
</ul>
</section>
<section id="discrete-vs-continuous" class="slide level3">
<h3>Discrete vs Continuous</h3>
<ul>
<li>Supervised learning is broadly divided into classification and regression</li>
<li>Unsupervised learning can be split too:</li>
</ul>
<ol type="1">
<li>methods that cluster the data</li>
<li>methods that represent the data as (lower dimensional) continuous values.</li>
</ol>
</section>
<section id="clustering" class="slide level3">
<h3>Clustering</h3>
<ul>
<li><em>Task</em>: associate each data point with a different label.</li>
<li>Label is <em>not</em> provided.</li>
<li>Quite intuitive for humans, we do it naturally.</li>
</ul>
</section>
<section id="platonic-ideals" class="slide level3">
<h3>Platonic Ideals</h3>
<ul>
<li>Names for animals originally invented by humans through ‘clustering’</li>
<li>Can we have the computer to recreate that process of inventing the label?</li>
<li>Greek philosopher, Plato, thought about ideas, he considered the concept of the Platonic ideal.</li>
<li>Platonic ideal bird is the bird that is most bird-like or the chair that is most chair-like.</li>
</ul>
</section>
<section id="cluster-center" class="slide level3">
<h3>Cluster Center</h3>
<ul>
<li>Can define different clusters, by finding their Platonic ideal (known as the cluster center)</li>
<li>Allocate each data point to the relevant nearest cluster center.</li>
<li>Allocate each animal to the class defined by its nearest cluster center.</li>
</ul>
</section>
<section id="similarity-and-distance-measures" class="slide level3">
<h3>Similarity and Distance Measures</h3>
<ul>
<li>Define a notion of either similarity or distance between the objects and their Platonic ideal.</li>
<li>If objects are vectors of data, <span class="math inline">\(\inputVector_i\)</span>.</li>
<li>Represent cluster center for category <span class="math inline">\(j\)</span> by a vector <span class="math inline">\(\meanVector_j\)</span>.</li>
<li>This vector contains the ideal features of a bird, a chair, or whatever category <span class="math inline">\(j\)</span> is.</li>
</ul>
</section>
<section id="similarity-or-distance" class="slide level3">
<h3>Similarity or Distance</h3>
<ul>
<li>Can either think in terms of similarity of the objects, or distances.</li>
<li>We want objects that are similar to each other to cluster together. We want objects that are distant from each other to cluster apart.</li>
<li>Use mathematical function to formalize this notion, e.g. for distance <span class="math display">\[
\distanceScalar_{ij} = \mappingFunction(\inputVector_i, \meanVector_j).
\]</span></li>
</ul>
</section>
<section id="squared-distance" class="slide level3">
<h3>Squared Distance</h3>
<ul>
<li>Find cluster centers that are close to as many data points as possible.</li>
<li>A commonly used distance is the squared distance, <span class="math display">\[
\distanceScalar_{ij} = (\inputVector_i - \meanVector_j)^2.
\]</span></li>
<li>Already seen for regression.</li>
</ul>
</section>
<section id="objective-function" class="slide level3">
<h3>Objective Function</h3>
<ul>
<li>Given similarity measure, need number of cluster centers, <span class="math inline">\(\numComps\)</span>.</li>
<li>Find their location by allocating each center to a sub-set of the points and minimizing the sum of the squared errors, <span class="math display">\[
\errorFunction(\meanMatrix) = \sum_{i \in \mathbf{i}_j} (\inputVector_i - \meanVector_j)^2
\]</span> here <span class="math inline">\(\mathbf{i}_j\)</span> is all indices of data points allocated to the <span class="math inline">\(j\)</span>th center.</li>
</ul>
</section>
<section id="k-means-clustering" class="slide level3">
<h3><span class="math inline">\(k\)</span>-Means Clustering</h3>
<ul>
<li><em><span class="math inline">\(k\)</span>-means clustering</em> is simple and quick to implement.</li>
<li>Very <em>initialisation</em> sensitive.</li>
</ul>
</section>
<section id="initialisation" class="slide level3">
<h3>Initialisation</h3>
<ul>
<li>Initialisation is the process of selecting a starting set of parameters.</li>
<li>Optimisation result can depend on the starting point.</li>
<li>For <span class="math inline">\(k\)</span>-means clustering you need to choose an initial set of centers.</li>
<li>Optimisation surface has many local optima, algorithm gets stuck in ones near initialisation.</li>
</ul>
</section>
<section id="k-means-clustering-1" class="slide level3">
<h3><span class="math inline">\(k\)</span>-Means Clustering</h3>
<script>
showDivs(1, 'kmeans_clustering');
</script>
<p><small></small> <input id="range-kmeans_clustering" type="range" min="1" max="26" value="1" onchange="setDivs('kmeans_clustering')" oninput="setDivs('kmeans_clustering')"> <button onclick="plusDivs(-1, 'kmeans_clustering')">❮</button> <button onclick="plusDivs(1, 'kmeans_clustering')">❯</button></p>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_001.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_002.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_003.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_004.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_005.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_006.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_007.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_008.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_009.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_010.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_011.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_012.svg" width="80%" style=" ">
</object>
</div>
<div class="kmeans_clustering" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/kmeans-clustering/kmeans_clustering_013.svg" width="80%" style=" ">
</object>
</div>
<p><em>Clustering with the <span class="math inline">\(k\)</span>-means clustering algorithm.</em></p>
</section>
<section id="k-means-clustering-2" class="slide level3">
<h3><span class="math inline">\(k\)</span>-Means Clustering</h3>
<div class="figure">
<div id="k-means-clustering-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/mfqmoUN-Cuw?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<p><em><span class="math inline">\(k\)</span>-means clustering by Alex Ihler</em></p>
</section>
<section id="hierarchical-clustering" class="slide level3">
<h3>Hierarchical Clustering</h3>
<ul>
<li>Form taxonomies of the cluster centers</li>
<li>Like humans apply to animals, to form <em>phylogenies</em></li>
</ul>
<div class="figure">
<div id="-figure" class="figure-frame">

</div>
</div>
<p>{<iframe width="Hierarchical Clustering by Alex Ihler." height="alex-ihler-hierarchical-clustering" src="https://www.youtube.com/embed/OcoE7JlbXvY){800}{600}?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
</section>
<section id="phylogenetic-trees" class="slide level3">
<h3>Phylogenetic Trees</h3>
<ul>
<li>Perform a hierarchical clustering based on genetic data, i.e. the actual contents of the genome.</li>
<li>Perform across a number of species and produce a <em>phylogenetic tree</em>.</li>
<li>Represents a guess at actual evolution of the species.</li>
<li>Used to estimate the origin of viruses like AIDS or Bird flu</li>
</ul>
</section>
<section id="product-clustering" class="slide level3">
<h3>Product Clustering</h3>
<ul>
<li>Could apply hierarchical clustering to Amazon’s products.</li>
<li>Would give us a phylogeny of products.</li>
<li>Each cluster of products would be split into sub-clusters of products until we got down to individual products.
<ul>
<li>E.g. at high level Electronics/Clothing</li>
</ul></li>
</ul>
</section>
<section id="hierarchical-clustering-challenge" class="slide level3">
<h3>Hierarchical Clustering Challenge</h3>
<ul>
<li>Many products belong in more than one cluster: e.g. running shoes are ‘sporting goods’ and they are ‘clothing’.</li>
<li>Tree structure doesn’t allow this allocation.</li>
<li>Our own psychological grouping capabilities are in cognitive science.
<ul>
<li>E.g. Josh Tenenbaum and collaborators cluster data in more complex ways.</li>
</ul></li>
</ul>
</section>
<section id="dimensionality-reduction" class="slide level3">
<h3>Dimensionality Reduction</h3>
<ul>
<li>Compress the data by replacing the original data with reduced number of continuous variables.</li>
</ul>
<div class="figure">
<div id="marionette-figure" class="figure-frame">
<object class width="40%" data="../slides/diagrams/ml/marionette.svg">
</object>
</div>
</div>
</section>
<section id="dimensionality-reduction-1" class="slide level3">
<h3>Dimensionality Reduction</h3>
<ul>
<li>Position of each body part of a marionette could be thought of as our data, <span class="math inline">\(\inputVector_i\)</span>.</li>
<li>Each data point is the 3-D co-ordinates of all the different body parts</li>
<li>Movement of parts determined by puppeteer via strings.</li>
<li>For a simple puppet with one stick can move the stick up and down, left and right and twist.</li>
</ul>
</section>
<section id="dimensionality-reduction-2" class="slide level3">
<h3>Dimensionality Reduction</h3>
<ul>
<li>This gives three parameters in the puppeteers control.</li>
<li>Implies that the puppet we see moving is controlled by only 3 variables.</li>
<li>These 3 variables are often called the hidden or <em>latent variables</em>.</li>
<li>Assume similar for real world data, observations are derived from lower dimensional underlying process</li>
</ul>
</section>
<section id="examples-in-social-sciences" class="slide level3">
<h3>Examples in Social Sciences</h3>
<ul>
<li>Underpins <em>psychological scoring</em> such as <em>IQ</em> or <em>personality tests</em></li>
<li>Myers-Briggs assumes personality is four dimensional.</li>
<li>Political belief (left/right wing).</li>
<li>Also language modelling has taken similar approaches: <a href="https://arxiv.org/abs/1301.3781">word2vec</a></li>
</ul>
</section>
<section id="principal-component-analysis" class="slide level3">
<h3>Principal Component Analysis</h3>
<ul>
<li>Principal component analysis (PCA) a linear dimensionality reduction technique</li>
<li>In Hotelling’s formulation of PCA: a assume <span class="math inline">\(\inputVector\)</span> is a linear weighted sum of the latent factors of interest.</li>
<li>E.g. IQ test we would try and predict subject <span class="math inline">\(i\)</span>’s answer to the <span class="math inline">\(j\)</span>th question with the following function <span class="math display">\[
\dataScalar_{ij} = \mappingFunction_j(\latentScalar_i; \weightVector).
\]</span> <span class="math inline">\(\latentScalar_i\)</span> would be the IQ of subject <span class="math inline">\(i\)</span> and <span class="math inline">\(\mappingFunction_j(\cdot)\)</span> would function relating IQ and question answer.</li>
</ul>
</section>
<section id="hotellings-pca" class="slide level3">
<h3>Hotelling’s PCA</h3>
<ul>
<li>Assume function is linear function. This idea is taken from a wider field known as <em>factor analysis</em>, so Hotelling described the challenge as <span class="math display">\[
\mappingFunction_j(\latentScalar_i; \weightVector) = \weightScalar_j \latentScalar_i
\]</span></li>
<li>Answer to the <span class="math inline">\(j\)</span>th question is predicted to be a scaling of the subject’s IQ.</li>
<li>Scale factor is given by <span class="math inline">\(\weightScalar_j\)</span>.</li>
</ul>
</section>
<section id="higher-latent-dimensions" class="slide level3">
<h3>Higher Latent Dimensions</h3>
<ul>
<li>For more latent dimensions matrix of scales, <span class="math inline">\(\weightVector\)</span> <span class="math display">\[
\mappingFunction_j(\latentVector_i; \weightVector) = \weightScalar_{1j} \latentScalar_{1i} + \weightScalar_{2j} \latentScalar_{2i}
\]</span></li>
<li><span class="math inline">\(\latentScalar_{1i}\)</span> might be extrovert/introvert and <span class="math inline">\(\latentScalar_{2i}\)</span> might rational/perceptual</li>
</ul>
</section>
<section id="parameters" class="slide level3">
<h3>Parameters</h3>
<ul>
<li>Parameters <span class="math inline">\(\weightVector\)</span> are known as the factor <em>loadings</em> in FA.</li>
<li>In PCA they are known as the principal components.</li>
<li>To fit the model need <em>loadings</em>, <span class="math inline">\(\weightVector\)</span>, and latent variables, <span class="math inline">\(\latentMatrix\)</span>.</li>
<li>Can use least squares (leads to <em>matrix factorization</em> and recommender systems).</li>
<li>Recommender systems most elements of <span class="math inline">\(\inputVector_i\)</span> are missing.</li>
</ul>
</section>
<section id="probability" class="slide level3">
<h3>Probability</h3>
<ul>
<li>PCA and factor analysis the unknown latent factors are dealt with through a probability distribution.</li>
<li>Assume these “unknowns” are drawn from a zero mean, unit variance normal distribution.</li>
<li>That implies a particular <em>probability</em> density for data (PDF).</li>
<li>The PDF has parameters depending on factor loadings to be estimated.</li>
</ul>
</section>
<section id="maximum-likelihood" class="slide level3">
<h3>Maximum Likelihood</h3>
<ul>
<li>Fit model by “maximising likelihood of data” under the PDF.</li>
<li>Maxium likelihood for PCA is the <em>eigenvalue decomposition</em> of the data covariance matrix.</li>
<li>Algorithmically simple and convenient, but slow to compute for very large data sets with many features and many subjects.</li>
</ul>
</section>
<section id="principal-component-analysis-1" class="slide level3">
<h3>Principal Component Analysis</h3>
<div class="figure">
<div id="dem-manifold-print-all-1-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/demManifoldPrint_all_1_2.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="reinforcement-learning" class="slide level3">
<h3>Reinforcement Learning</h3>
<ul>
<li><p>The final domain of learning we will review is known as reinforcement learning.</p></li>
<li><p>Many researchers seem to believe is offering a route to <em>general intelligence</em>.</p></li>
<li><p>Idea of general intelligence is develop algorithms that are adaptable to many different circumstances.</p></li>
</ul>
</section>
<section id="reinforcement-learning-1" class="slide level3">
<h3>Reinforcement Learning</h3>
<ul>
<li><p>Supervised learning algorithms are designed to resolve particular challenges.</p></li>
<li><p>Data is annotated with those challenges in mind.</p></li>
<li><p>Unsupervised attempts to build representations without any context.</p></li>
</ul>
</section>
<section id="reward" class="slide level3">
<h3>“Reward”</h3>
<ul>
<li><p>In reinforcement learning some context is given, in the form of a reward. But it is often <em>delayed</em></p></li>
<li><p>Credit allocation problem: many actions that affected the outcome, but which actions had a positive effect and which a negative effect?</p></li>
</ul>
</section>
<section id="ab-testing-and-reward" class="slide level3">
<h3>A/B Testing and Reward</h3>
<ul>
<li><p>Advert clicks can be seen as a reward.</p></li>
<li><p>Testing the customer experience, A/B testing, prioritises short term reward.</p></li>
<li><p>The internet is currently being driven by short term rewards which make it distracting in the short term, but perhaps less useful in the long term.</p></li>
<li><p>Click-bait is an example, but there are more subtle effects.</p></li>
<li><p>Success of Facebook is driven by its ability to draw us in when likely we should be doing something else. This is driven by large scale A/B testing.</p></li>
</ul>
</section>
<section id="longer-term" class="slide level3">
<h3>Longer Term</h3>
<ul>
<li><p>One open question is how to drive non-visual interfaces through equivalents to A/B testing.</p></li>
<li><p>Speech interfaces, such as those used in intelligent agents, are less amenable to A/B testing when determining the quality of the interface.</p></li>
<li><p>Improving interaction with them is therefore less exact science than the visual interface.</p></li>
</ul>
</section>
<section id="data-efficiency" class="slide level3">
<h3>Data Efficiency</h3>
<ul>
<li><p>Data efficient reinforcement learning methods are likely to be key to improving these agent’s ability to interact.</p></li>
<li><p>However, they are not yet mature enough to be deployed yet.</p></li>
</ul>
</section>
<section id="game-play" class="slide level3">
<h3>Game Play</h3>
<ul>
<li><p>Reinforcement learning methods have been deployed with high profile success is game play.</p></li>
<li><p>Reward is delayed to the end of the game, victory or defeat.</p></li>
<li><p>Can acquire lots of data through simulation.</p></li>
<li><p>Many of the recent advances in reinforcement learning have occurred with methods that are not data efficient.</p></li>
</ul>
</section>
<section id="deepmind" class="slide level3">
<h3>DeepMind</h3>
<ul>
<li><p>The company DeepMind is set up around reinforcement learning as an approach to general intelligence.</p></li>
<li><p>Best known achievements are centered around artificial intelligence in game play.</p></li>
<li><p>For example, Atari game play and AlphaGo.</p></li>
</ul>
</section>
<section id="deep-q-learning" class="slide level3">
<h3>Deep Q Learning</h3>
<ul>
<li><p>DeepMind uses an approach to Machine Learning where there are two mathematical functions at work.</p></li>
<li><p>The <em>policy function</em> determines the action to be taken at any given moment,</p></li>
<li><p>The <em>value function</em> estimates the quality of a board position at any given time.</p></li>
<li><p>In AlphaGo make use of convolutional neural networks for both these models.</p></li>
</ul>
</section>
<section id="alphago" class="slide level3">
<h3>AlphaGo</h3>
<ul>
<li>Go was considered a challenge for artificial intelligence for two reasons.</li>
</ul>
<ol type="1">
<li><p>Game tree has a very high `branching factor’.</p>
<p>In Go, there are so many legal moves that the game tree increases exponentially.</p></li>
<li><p>Evaluating the quality of any given board position was deemed to be very hard.[^chess].</p></li>
</ol>
<ul>
<li>AlphaGo played more than 30,000,000 games to learn value and policy.</li>
</ul>
</section>
<section id="model-based-approach" class="slide level3">
<h3>Model Based Approach</h3>
<ul>
<li><p>An alternative approach to reinforcement learning is to use a prediction function.</p></li>
<li><p>This is similar to <em>classical control</em> and field of <em>system identification</em>.</p></li>
<li><p>Known as <em>model based</em> reinforcement learning.</p></li>
</ul>
</section>
<section id="optimization-methods" class="slide level3">
<h3>Optimization Methods</h3>
<ul>
<li><p>Reinforcement learning can also used to improve user experience.</p></li>
<li><p>Reward is gained when the user buys a product from us.</p></li>
<li><p>This makes it closely allied to the area of optimization.</p></li>
<li><p>Optimization of our user interfaces is like reinforcement learning task, but normally approached through <em>Bayesian optimization</em> or <em>bandit learning</em>.</p></li>
</ul>
</section>
<section id="optimization" class="slide level3">
<h3>Optimization</h3>
<ul>
<li><p>Normal optimization we have a mathematical representation of our objective function as a direct function of the parameters.</p></li>
<li><p>In Bayesian Optimization we don’t.</p></li>
<li><p>E.g. Examples in this form of optimization include:</p></li>
</ul>
<ol type="1">
<li><p>What is the best user interface for presenting adverts?</p></li>
<li><p>What is the best design of wing for an F1 car?</p></li>
<li><p>Which product should I return top of the list in response to this user’s search?</p></li>
</ol>
</section>
<section id="bayesian-optimization" class="slide level3">
<h3>Bayesian Optimization</h3>
<ul>
<li><p>Can’t directly relate the parameters in the system of interest to our objective through a mathematical function.</p></li>
<li><p>E.g. What is the mathematical function that relates a user’s experience to the probability that they will buy a product?</p></li>
</ul>
</section>
<section id="bayesian-optimization-1" class="slide level3">
<h3>Bayesian Optimization</h3>
<ul>
<li><p>Use machine learning to develop a <em>surrogate model</em> for the optimization task.</p></li>
<li><p>Surrogate model is a prediction function that attempts to recreate the process we are finding hard to model.</p></li>
<li><p>Try to simultaneously fit the surrogate model and optimize the process.</p></li>
</ul>
</section>
<section id="surrogate-models" class="slide level3">
<h3>Surrogate Models</h3>
<ul>
<li><p>Bayesian optimization methods use a <em>surrogate model</em> (normally a specific form of regression model).</p></li>
<li><p>Use this to predict how the real system will perform.</p></li>
<li><p>Optimize in the surrogate model.</p></li>
</ul>
</section>
<section id="conclusion" class="slide level3">
<h3>Conclusion</h3>
<ul>
<li><p>Introduce range of ML approaches.</p></li>
<li><p>Focussed on where they use mathematical functions as a general overview.</p></li>
</ul>
</section>
<section id="deployment" class="slide level3">
<h3>Deployment</h3>
<ul>
<li><p>Introduced almost in order of difficulty of deployment.</p></li>
<li><p>Supervised learning requires more data annotation, but more straightforward to deploy.</p></li>
<li><p>Therefore major focus with supervised learning should always be on maintaining data quality.</p></li>
</ul>
</section>
<section id="where-to-deploy" class="slide level3">
<h3>Where to Deploy?</h3>
<blockquote>
<p>If a typical person can do a mental task with less than one second of thought, we can probably automate it using AI either now or in the near future.</p>
<p>Andrew Ng</p>
</blockquote>
</section>
<section id="is-this-right" class="slide level3">
<h3>Is this Right?</h3>
<ul>
<li><p>Broadly agree with this quote but only in the context of supervised learning.</p></li>
<li><p>The picture with regard to unsupervised learning and reinforcement learning is more clouded.</p></li>
</ul>
</section>
<section id="getting-easier" class="slide level3">
<h3>Getting Easier</h3>
<ul>
<li><p>We seem to be moving beyond the era where very deep machine learning expertise is required to deploy methods.</p></li>
<li><p>Just require a solid understanding of machine learning (say to Masters level)</p></li>
</ul>
</section>
<section id="streaming-data" class="slide level3">
<h3>Streaming Data</h3>
<ul>
<li><p>Supervised machine learning solutions are normally trained offline.</p></li>
<li><p>They do not adapt when deployed because this makes them less verifiable.</p></li>
<li><p>This compounds the brittleness of our solutions.</p></li>
</ul>
</section>
<section id="model-choice" class="slide level3">
<h3>Model Choice</h3>
<ul>
<li><p>For all machine learning methods is the initial choice of useful classes of functions.</p></li>
<li><p>The deep learning revolution is associated with a particular class of mathematical functions.</p></li>
</ul>
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li><p>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></p></li>
<li><p>Blog post on <a href="http://inverseprobability.com/2018/11/05/the-3ds-of-machine-learning-systems-design">The 3Ds of Machine Learning Systems Design</a></p></li>
</ul>

</section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Using the test data in your training procedure is a major error in any machine learning procedure. It is extremely dangerous as it gives a misleading assessment of the model performance. The <a href="http://inverseprobability.com/2015/06/04/baidu-on-imagenet">Baidu ImageNet scandal</a> was an example of a team competing in the ImageNet challenge which did this. The team had announced via the publication pre-print server Arxiv that they had a world-leading performance on the ImageNet challenge. This was reported in the mainstream media. Two weeks later the challenge organizers revealed that the team had created multiple accounts for checking their test performance more times than was permitted by the challenge rules. This was then reported as “AI’s first doping scandal”. The team lead was fired by Baidu.<a href="#/fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
