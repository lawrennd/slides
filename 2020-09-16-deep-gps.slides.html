<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2020-09-16">
  <title>Deep GPs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="figure-animate.js"></script>
</head>
<body>
\[\newcommand{\tk}[1]{}
%\newcommand{\tk}[1]{\textbf{TK}: #1}
\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Deep GPs</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2020-09-16</time></p>
  <p class="venue" style="text-align:center">Virtual Gaussian Process Summer School</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<!-- SECTION Deep Gaussian Processes -->
</section>
<section id="deep-gaussian-processes" class="slide level2">
<h2>Deep Gaussian Processes</h2>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="cosmic-microwave-background-figure" class="figure-frame">
<div class="centered" style="">
<img class="vertical-align:middle" src="../slides/diagrams/Planck_CMB.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="figure">
<div id="modern-universe-non-linear-function-figure" class="figure-frame">
<div style="fontsize:120px;vertical-align:middle">
<img src="../slides/diagrams/earth_PNG37.png" width="20%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(=f\Bigg(\)</span><img src="../slides/diagrams/Planck_CMB.png"  width="50%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">\(\Bigg)\)</span>
</div>
</div>
</div>
</section>
<section id="low-rank-gaussian-processes" class="slide level2">
<h2>Low Rank Gaussian Processes</h2>
</section>
<section id="approximations" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-1.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="approximations-1" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-2.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="approximations-2" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-3.png" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="approximations-3" class="slide level2">
<h2>Approximations</h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/sparse-gps-4.png" width="45%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="caption" style="">
Figure: Image credit: Kai Arulkumaran
</div>
</section>
<section id="low-rank-motivation" class="slide level2">
<h2>Low Rank Motivation</h2>
<ul>
<li><p>Inference in a GP has the following demands:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData^3)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData^2)\)</span></td>
</tr>
</tbody>
</table></li>
<li><p>Inference in a low rank GP has the following demands:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Complexity:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData\numInducing^2)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: right;">Storage:</td>
<td style="text-align: left;"><span class="math inline">\(\bigO(\numData\numInducing)\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\numInducing\)</span> is a user chosen parameter.</p></li>
</ul>
<p><small><span class="citation" data-cites="Snelson:pseudo05">Snelson and Ghahramani (n.d.)</span>,<span class="citation" data-cites="Quinonero:unifying05">Quiñonero Candela and Rasmussen (2005)</span>,<span class="citation" data-cites="Lawrence:larger07">Lawrence (n.d.)</span>,<span class="citation" data-cites="Titsias:variational09">Titsias (n.d.)</span>,<span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span></small></p>
</section>
<section id="variational-compression" class="slide level2">
<h2>Variational Compression</h2>
<ul>
<li>Inducing variables are a compression of the real observations.</li>
<li>They are like pseudo-data. They can be in space of <span class="math inline">\(\mappingFunctionVector\)</span> or a space that is related through a linear operator <span class="citation" data-cites="Alvarez:efficient10">(Álvarez et al., 2010)</span> — e.g. a gradient or convolution.</li>
</ul>
</section>
<section id="variational-compression-ii" class="slide level2">
<h2>Variational Compression II</h2>
<ul>
<li>Introduce <em>inducing</em> variables.</li>
<li>Compress information into the inducing variables and avoid the need to store all the data.</li>
<li>Allow for scaling e.g. stochastic variational <span class="citation" data-cites="Hensman:bigdata13">Hensman et al. (n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14">Gal et al. (n.d.)</span>,<span class="citation" data-cites="Dai:gpu14">Dai et al. (2014)</span>, <span class="citation" data-cites="Seeger:auto17">Seeger et al. (2017)</span></li>
</ul>
<!--include{_gp/includes/larger-factorize.md}-->
</section>
<section id="nonparametric-gaussian-processes" class="slide level2">
<h2>Nonparametric Gaussian Processes</h2>
<ul>
<li><p>We’ve seen how we go from parametric to non-parametric.</p></li>
<li><p>The limit implies infinite dimensional <span class="math inline">\(\mappingVector\)</span>.</p></li>
<li><p>Gaussian processes are generally non-parametric: combine data with covariance function to get model.</p></li>
<li><p>This representation <em>cannot</em> be summarized by a parameter vector of a fixed size.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck" class="slide level2">
<h2>The Parametric Bottleneck</h2>
<ul>
<li><p>Parametric models have a representation that does not respond to increasing training set size.</p></li>
<li><p>Bayesian posterior distributions over parameters contain the information about the training data.</p>
<ul>
<li><p>Use Bayes’ rule from training data, <span class="math inline">\(p\left(\mappingVector|\dataVector, \inputMatrix\right)\)</span>,</p></li>
<li><p>Make predictions on test data <span class="math display">\[p\left(\dataScalar_*|\inputMatrix_*, \dataVector, \inputMatrix\right) = \int
        p\left(\dataScalar_*|\mappingVector,\inputMatrix_*\right)p\left(\mappingVector|\dataVector,
          \inputMatrix)\text{d}\mappingVector\right).\]</span></p></li>
</ul></li>
<li><p><span class="math inline">\(\mappingVector\)</span> becomes a bottleneck for information about the training set to pass to the test set.</p></li>
<li><p>Solution: increase <span class="math inline">\(\numBasisFunc\)</span> so that the bottleneck is so large that it no longer presents a problem.</p></li>
<li><p>How big is big enough for <span class="math inline">\(\numBasisFunc\)</span>? Non-parametrics says <span class="math inline">\(\numBasisFunc \rightarrow \infty\)</span>.</p></li>
</ul>
</section>
<section id="the-parametric-bottleneck-1" class="slide level2">
<h2>The Parametric Bottleneck</h2>
<ul>
<li>Now no longer possible to manipulate the model through the standard parametric form.</li>
</ul>
<div class="fragment">
<ul>
<li>However, it <em>is</em> possible to express <em>parametric</em> as GPs: <span class="math display">\[\kernelScalar\left(\inputVector_i,\inputVector_j\right)=\basisFunction_:\left(\inputVector_i\right)^\top\basisFunction_:\left(\inputVector_j\right).\]</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>These are known as degenerate covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Their rank is at most <span class="math inline">\(\numBasisFunc\)</span>, non-parametric models have full rank covariance matrices.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Most well known is the “linear kernel”, <span class="math inline">\(\kernelScalar(\inputVector_i, \inputVector_j) = \inputVector_i^\top\inputVector_j\)</span>.</li>
</ul>
</div>
</section>
<section id="making-predictions" class="slide level2">
<h2>Making Predictions</h2>
<ul>
<li>For non-parametrics prediction at new points <span class="math inline">\(\mappingFunctionVector_*\)</span> is made by conditioning on <span class="math inline">\(\mappingFunctionVector\)</span> in the joint distribution.</li>
</ul>
<div class="fragment">
<ul>
<li>In GPs this involves combining the training data with the covariance function and the mean function.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Parametric is a special case when conditional prediction can be summarized in a <em>fixed</em> number of parameters.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Complexity of parametric model remains fixed regardless of the size of our training data set.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>For a non-parametric model the required number of parameters grows with the size of the training data.</li>
</ul>
</div>
</section>
<section id="augment-variable-space" class="slide level2">
<h2>Augment Variable Space</h2>
<ul>
<li>Augment variable space with inducing observations, <span class="math inline">\(\inducingVector\)</span> <span class="math display">\[
\begin{bmatrix}
\mappingFunctionVector\\
\inducingVector
\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\kernelMatrix}
\]</span> with <span class="math display">\[
\kernelMatrix =
\begin{bmatrix}
\Kff &amp; \Kfu \\
\Kuf &amp; \Kuu
\end{bmatrix}
\]</span></li>
</ul>
</section>
<section id="joint-density" class="slide level2">
<h2>Joint Density</h2>
<p><span class="math display">\[
p(\mappingFunctionVector, \inducingVector) = p(\mappingFunctionVector| \inducingVector) p(\inducingVector)
\]</span> to augment our model <span class="math display">\[
\dataScalar(\inputVector) = \mappingFunction(\inputVector) + \noiseScalar,
\]</span> giving <span class="math display">\[
p(\dataVector) = \int p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector) \text{d}\mappingFunctionVector,
\]</span> where for the independent case we have <span class="math inline">\(p(\dataVector | \mappingFunctionVector) = \prod_{i=1}^\numData p(\dataScalar_i|\mappingFunction_i)\)</span>.</p>
</section>
<section id="auxilliary-variables" class="slide level2">
<h2>Auxilliary Variables</h2>
<p><span class="math display">\[
p(\dataVector) = \int p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector)  p(\inducingVector)  \text{d}\inducingVector \text{d}\mappingFunctionVector.
\]</span> Integrating over <span class="math inline">\(\mappingFunctionVector\)</span> <span class="math display">\[
p(\dataVector) = \int p(\dataVector|\inducingVector)   p(\inducingVector)  \text{d}\inducingVector.
\]</span></p>
</section>
<section id="parametric-comparison" class="slide level2">
<h2>Parametric Comparison</h2>
<p><span class="math display">\[
\dataScalar(\inputVector) = \weightVector^\top\basisVector(\inputVector) + \noiseScalar
\]</span></p>
<p><span class="math display">\[
p(\dataVector) = \int p(\dataVector|\weightVector) p(\weightVector) \text{d} \weightVector
\]</span></p>
<p><span class="math display">\[
p(\dataVector^*|\dataVector) = \int p(\dataVector^*|\weightVector) p(\weightVector|\dataVector) \text{d} \weightVector
\]</span></p>
</section>
<section id="new-form" class="slide level2">
<h2>New Form</h2>
<p><span class="math display">\[
p(\dataVector^*|\dataVector) = \int p(\dataVector^*|\inducingVector) p(\inducingVector|\dataVector) \text{d} \inducingVector
\]</span></p>
<ul>
<li><p>but <span class="math inline">\(\inducingVector\)</span> is not a <em>parameter</em></p></li>
<li><p>Unfortunately computing <span class="math inline">\(p(\dataVector|\inducingVector)\)</span> is intractable</p></li>
</ul>
</section>
<section id="variational-bound-on-pdatavector-inducingvector" class="slide level2">
<h2>Variational Bound on <span class="math inline">\(p(\dataVector |\inducingVector)\)</span></h2>
<p><span class="math display">\[
\begin{aligned}
    \log p(\dataVector|\inducingVector) &amp; = \log \int p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector) \text{d}\mappingFunctionVector\\ &amp; = \int q(\mappingFunctionVector) \log \frac{p(\dataVector|\mappingFunctionVector) p(\mappingFunctionVector|\inducingVector)}{q(\mappingFunctionVector)}\text{d}\mappingFunctionVector + \KL{q(\mappingFunctionVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)}.
\end{aligned}
\]</span></p>
</section>
<section id="choose-form-for-qcdot" class="slide level2">
<h2>Choose form for <span class="math inline">\(q(\cdot)\)</span></h2>
<ul>
<li>Set <span class="math inline">\(q(\mappingFunctionVector)=p(\mappingFunctionVector|\inducingVector)\)</span>, <span class="math display">\[
\log p(\dataVector|\inducingVector) \geq \int p(\mappingFunctionVector|\inducingVector) \log p(\dataVector|\mappingFunctionVector)\text{d}\mappingFunctionVector.
\]</span> <span class="math display">\[
p(\dataVector|\inducingVector) \geq \exp \int p(\mappingFunctionVector|\inducingVector) \log p(\dataVector|\mappingFunctionVector)\text{d}\mappingFunctionVector.
\]</span> <span style="text-align:right"><span class="citation" data-cites="Titsias:variational09">(Titsias, n.d.)</span></span></li>
</ul>
</section>
<section id="optimal-compression-in-inducing-variables" class="slide level2">
<h2>Optimal Compression in Inducing Variables</h2>
<ul>
<li><p>Maximizing lower bound minimizes the KL divergence (information gain): <span class="math display">\[
\KL{p(\mappingFunctionVector|\inducingVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)} = \int p(\mappingFunctionVector|\inducingVector) \log \frac{p(\mappingFunctionVector|\inducingVector)}{p(\mappingFunctionVector|\dataVector, \inducingVector)}\text{d}\inducingVector
\]</span></p></li>
<li><p>This is minimized when the information stored about <span class="math inline">\(\dataVector\)</span> is stored already in <span class="math inline">\(\inducingVector\)</span>.</p></li>
<li><p>The bound seeks an <em>optimal compression</em> from the <em>information gain</em> perspective.</p></li>
<li><p>If <span class="math inline">\(\inducingVector = \mappingFunctionVector\)</span> bound is exact (<span class="math inline">\(\mappingFunctionVector\)</span> <span class="math inline">\(d\)</span>-separates <span class="math inline">\(\dataVector\)</span> from <span class="math inline">\(\inducingVector\)</span>).</p></li>
</ul>
</section>
<section id="choice-of-inducing-variables" class="slide level2">
<h2>Choice of Inducing Variables</h2>
<ul>
<li>Free to choose whatever heuristics for the inducing variables.</li>
<li>Can quantify which heuristics perform better through checking lower bound.</li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
\mappingFunctionVector\\
\inducingVector
\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\kernelMatrix}
\]</span> with <span class="math display">\[
\kernelMatrix =
\begin{bmatrix}
\Kff &amp; \Kfu \\
\Kuf &amp; \Kuu
\end{bmatrix}
\]</span></p>
</section>
<section id="variational-compression-1" class="slide level2">
<h2>Variational Compression</h2>
<ul>
<li>Inducing variables are a compression of the real observations.</li>
<li>They are like pseudo-data. They can be in space of <span class="math inline">\(\mappingFunctionVector\)</span> or a space that is related through a linear operator <span class="citation" data-cites="Alvarez:efficient10">(Álvarez et al., 2010)</span> — e.g. a gradient or convolution.</li>
</ul>
</section>
<section id="variational-compression-ii-1" class="slide level2">
<h2>Variational Compression II</h2>
<ul>
<li>Resulting algorithms reduce computational complexity.</li>
<li>Also allow deployment of more standard scaling techniques.</li>
<li>E.g. Stochastic variational inference <span class="citation" data-cites="Hoffman:stochastic12">Hoffman et al. (2012)</span></li>
<li>Allow for scaling e.g. stochastic variational <span class="citation" data-cites="Hensman:bigdata13">Hensman et al. (n.d.)</span> or parallelization <span class="citation" data-cites="Gal:distributed14 Dai:gpu14 Seeger:auto17">(Dai et al., 2014; Gal et al., n.d.; Seeger et al., 2017)</span></li>
</ul>
<!--include{_gp/includes/larger-graph-intro.md}-->
</section>
<section id="full-gaussian-process-fit" class="slide level2">
<h2>Full Gaussian Process Fit</h2>
<div class="figure">
<div id="sparse-demo-full-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="inducing-variable-fit" class="slide level2">
<h2>Inducing Variable Fit</h2>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-unlearned-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-unlearned-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="inducing-variable-param-optimize" class="slide level2">
<h2>Inducing Variable Param Optimize</h2>
<div class="figure">
<div id="sparse-demo-constrained-inducing-6-learned-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-constrained-inducing-6-learned-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="inducing-variable-full-optimize" class="slide level2">
<h2>Inducing Variable Full Optimize</h2>
<div class="figure">
<div id="sparse-demo-unconstrained-inducing-6-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-unconstrained-inducing-6-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="eight-optimized-inducing-variables" class="slide level2">
<h2>Eight Optimized Inducing Variables</h2>
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg" width style=" ">
</object>
</section>
<section id="full-gaussian-process-fit-1" class="slide level2">
<h2>Full Gaussian Process Fit</h2>
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width style=" ">
</object>
<div class="figure">
<div id="sparse-demo-sparse-inducing-8-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-sparse-inducing-8-gp.svg" width="80%" style=" ">
</object>
<object class="svgplot " data="../slides/diagrams/gp/sparse-demo-full-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="modern-review" class="slide level2">
<h2>Modern Review</h2>
<ul>
<li><p><em>A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</em> <span class="citation" data-cites="Thang:unifying17">Bui et al. (2017)</span></p></li>
<li><p><em>Deep Gaussian Processes and Variational Propagation of Uncertainty</em> <span class="citation" data-cites="Damianou:thesis2015">Damianou (2015)</span></p></li>
</ul>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/mackay-baby.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="structure-of-priors" class="slide level2">
<h2>Structure of Priors</h2>
<p>MacKay: NeurIPS Tutorial 1997 “Have we thrown out the baby with the bathwater?” <span class="citation" data-cites="MacKay:gpintroduction98">(Published as MacKay, n.d.)</span></p>
</section>
<section id="deep-neural-network" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/deep-nn1.svg" width="50%" style=" ">
</object>
</section>
<section id="deep-neural-network-1" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/deep-nn2.svg" width="50%" style=" ">
</object>
</section>
<section id="mathematically" class="slide level2">
<h2>Mathematically</h2>
<p><span class="math display">\[
\begin{align}
    \hiddenVector_{1} &amp;= \basisFunction\left(\mappingMatrix_1 \inputVector\right)\\
    \hiddenVector_{2} &amp;=  \basisFunction\left(\mappingMatrix_2\hiddenVector_{1}\right)\\
    \hiddenVector_{3} &amp;= \basisFunction\left(\mappingMatrix_3 \hiddenVector_{2}\right)\\
    \dataVector &amp;= \mappingVector_4 ^\top\hiddenVector_{3}
\end{align}
\]</span></p>
</section>
<section id="overfitting" class="slide level2">
<h2>Overfitting</h2>
<ul>
<li><p>Potential problem: if number of nodes in two adjacent layers is big, corresponding <span class="math inline">\(\mappingMatrix\)</span> is also very big and there is the potential to overfit.</p></li>
<li><p>Proposed solution: “dropout”.</p></li>
<li><p>Alternative solution: parameterize <span class="math inline">\(\mappingMatrix\)</span> with its SVD. <span class="math display">\[
\mappingMatrix = \eigenvectorMatrix\eigenvalueMatrix\eigenvectwoMatrix^\top
\]</span> or <span class="math display">\[
\mappingMatrix = \eigenvectorMatrix\eigenvectwoMatrix^\top
\]</span> where if <span class="math inline">\(\mappingMatrix \in \Re^{k_1\times k_2}\)</span> then <span class="math inline">\(\eigenvectorMatrix\in \Re^{k_1\times q}\)</span> and <span class="math inline">\(\eigenvectwoMatrix \in \Re^{k_2\times q}\)</span>, i.e. we have a low rank matrix factorization for the weights.</p></li>
</ul>
</section>
<section id="low-rank-approximation" class="slide level2">
<h2>Low Rank Approximation</h2>
<div class="figure">
<div id="low-rank-mapping-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/wisuvt.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="bottleneck-layers-in-deep-neural-networks" class="slide level2">
<h2>Bottleneck Layers in Deep Neural Networks</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/deep-nn-bottleneck1.svg" width="60%" style=" ">
</object>
</section>
<section id="deep-neural-network-2" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/deep-nn-bottleneck2.svg" width="60%" style=" ">
</object>
</section>
<section id="mathematically-1" class="slide level2">
<h2>Mathematically</h2>
<p>The network can now be written mathematically as <span class="math display">\[
\begin{align}
  \latentVector_{1} &amp;= \eigenvectwoMatrix^\top_1 \inputVector\\
  \hiddenVector_{1} &amp;= \basisFunction\left(\eigenvectorMatrix_1 \latentVector_{1}\right)\\
  \latentVector_{2} &amp;= \eigenvectwoMatrix^\top_2 \hiddenVector_{1}\\
  \hiddenVector_{2} &amp;= \basisFunction\left(\eigenvectorMatrix_2 \latentVector_{2}\right)\\
  \latentVector_{3} &amp;= \eigenvectwoMatrix^\top_3 \hiddenVector_{2}\\
  \hiddenVector_{3} &amp;= \basisFunction\left(\eigenvectorMatrix_3 \latentVector_{3}\right)\\
  \dataVector &amp;= \mappingVector_4^\top\hiddenVector_{3}.
\end{align}
\]</span></p>
</section>
<section id="a-cascade-of-neural-networks" class="slide level2">
<h2>A Cascade of Neural Networks</h2>
<p><span class="math display">\[
\begin{align}
  \latentVector_{1} &amp;= \eigenvectwoMatrix^\top_1 \inputVector\\
  \latentVector_{2} &amp;= \eigenvectwoMatrix^\top_2 \basisFunction\left(\eigenvectorMatrix_1 \latentVector_{1}\right)\\
  \latentVector_{3} &amp;= \eigenvectwoMatrix^\top_3 \basisFunction\left(\eigenvectorMatrix_2 \latentVector_{2}\right)\\
  \dataVector &amp;= \mappingVector_4 ^\top \latentVector_{3}
\end{align}
\]</span></p>
</section>
<section id="cascade-of-gaussian-processes" class="slide level2">
<h2>Cascade of Gaussian Processes</h2>
<ul>
<li><p>Replace each neural network with a Gaussian process <span class="math display">\[
\begin{align}
\latentVector_{1} &amp;= \mappingFunctionVector_1\left(\inputVector\right)\\
\latentVector_{2} &amp;= \mappingFunctionVector_2\left(\latentVector_{1}\right)\\
\latentVector_{3} &amp;= \mappingFunctionVector_3\left(\latentVector_{2}\right)\\
\dataVector &amp;= \mappingFunctionVector_4\left(\latentVector_{3}\right)
\end{align}
\]</span></p></li>
<li><p>Equivalent to prior over parameters, take width of each layer to infinity.</p></li>
</ul>
<!-- SECTION Deep Learning -->
</section>
<section id="deep-learning" class="slide level2">
<h2>Deep Learning</h2>
<!-- No slide titles in this context -->
</section>
<section id="deepface" class="slide level2">
<h2>DeepFace</h2>
<p><span class="fragment fade-in"><small>Outline of the DeepFace architecture. A front-end of a single convolution-pooling-convolution filtering on the rectified input, followed by three locally-connected layers and two fully-connected layers. Color illustrates feature maps produced at each layer. The net includes more than 120 million parameters, where more than 95% come from the local and fully connected.</small></span></p>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<p><span style="text-align:right"><small>Source: DeepFace <span class="citation" data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small></span></p>
</section>
<section id="deep-learning-as-pinball" class="slide level2">
<h2>Deep Learning as Pinball</h2>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/pinball001.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/pinball002.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="mathematically-2" class="slide level2">
<h2>Mathematically</h2>
<ul>
<li><p>Composite <em>multivariate</em> function</p>
<p><span class="math display">\[
\mathbf{g}(\inputVector)=\mappingFunctionVector_5(\mappingFunctionVector_4(\mappingFunctionVector_3(\mappingFunctionVector_2(\mappingFunctionVector_1(\inputVector))))).
\]</span></p></li>
</ul>
</section>
<section id="equivalent-to-markov-chain" class="slide level2">
<h2>Equivalent to Markov Chain</h2>
<ul>
<li>Composite <em>multivariate</em> function <span class="math display">\[
p(\dataVector|\inputVector)= p(\dataVector|\mappingFunctionVector_5)p(\mappingFunctionVector_5|\mappingFunctionVector_4)p(\mappingFunctionVector_4|\mappingFunctionVector_3)p(\mappingFunctionVector_3|\mappingFunctionVector_2)p(\mappingFunctionVector_2|\mappingFunctionVector_1)p(\mappingFunctionVector_1|\inputVector)
\]</span></li>
</ul>
<div class="figure">
<div id="deep-markov-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/deep-markov.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/deep-markov-vertical.svg" width="7%" style=" ">
</object>
</div>
</div>
</section>
<section id="why-deep" class="slide level2">
<h2>Why Deep?</h2>
<ul>
<li><p>Gaussian processes give priors over functions.</p></li>
<li><p>Elegant properties:</p>
<ul>
<li>e.g. <em>Derivatives</em> of process are also Gaussian distributed (if they exist).</li>
</ul></li>
<li><p>For particular covariance functions they are ‘universal approximators’, i.e. all functions can have support under the prior.</p></li>
<li><p>Gaussian derivatives might ring alarm bells.</p></li>
<li><p>E.g. a priori they don’t believe in function ‘jumps’.</p></li>
</ul>
</section>
<section id="stochastic-process-composition" class="slide level2">
<h2>Stochastic Process Composition</h2>
<ul>
<li><p>From a process perspective: <em>process composition</em>.</p></li>
<li><p>A (new?) way of constructing more complex <em>processes</em> based on simpler components.</p></li>
</ul>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<object class="svgplot " data="../slides/diagrams/deepgp/deep-markov-vertical.svg" width style=" ">
</object>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-side-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/deep-markov-vertical-side.svg" width="15%" style=" ">
</object>
</div>
</div>
</section>
<section id="difficulty-for-probabilistic-approaches" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot 80%" data="../slides/diagrams/dimred/nonlinear-mapping-3d-plot.svg" width="center" style=" ">
</object>
</div>
</div>
</section>
<section id="difficulty-for-probabilistic-approaches-1" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot 80%" data="../slides/diagrams/dimred/nonlinear-mapping-2d-plot.svg" width="center" style=" ">
</object>
</div>
</div>
</section>
<section id="difficulty-for-probabilistic-approaches-2" class="slide level2">
<h2>Difficulty for Probabilistic Approaches</h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/dimred/gaussian-through-nonlinear.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="standard-variational-approach-fails" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Standard variational bound has the form: <span class="math display">\[
\likelihoodBound = \expDist{\log p(\dataVector|\latentMatrix)}{q(\latentMatrix)} + \KL{q(\latentMatrix)}{p(\latentMatrix)}
\]</span></li>
</ul>
</section>
<section id="standard-variational-approach-fails-1" class="slide level2">
<h2>Standard Variational Approach Fails</h2>
<ul>
<li>Requires expectation of <span class="math inline">\(\log p(\dataVector|\latentMatrix)\)</span> under <span class="math inline">\(q(\latentMatrix)\)</span>. <span class="math display">\[
\begin{align}
\log p(\dataVector|\latentMatrix) = &amp; -\frac{1}{2}\dataVector^\top\left(\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2\eye\right)^{-1}\dataVector \\ &amp; -\frac{1}{2}\log \det{\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}+\dataStd^2 \eye} -\frac{\numData}{2}\log 2\pi
\end{align}
\]</span> <span class="math inline">\(\kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector}\)</span> is dependent on <span class="math inline">\(\latentMatrix\)</span> and it appears in the inverse.</li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Consider collapsed variational bound, <span class="fragment" data-fragment-index="1"><small><span class="math display">\[
  p(\dataVector)\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expSamp{\mappingFunctionVector}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
\]</span></small></span> <span class="fragment" data-fragment-index="2"><small><span class="math display">\[
  p(\dataVector|\latentMatrix )\geq \prod_{i=1}^\numData c_i \int \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}p(\inducingVector) \text{d}\inducingVector
\]</span></small></span> <span class="fragment" data-fragment-index="3"><small><span class="math display">\[
    \int p(\dataVector|\latentMatrix)p(\latentMatrix) \text{d}\latentMatrix \geq \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye} p(\latentMatrix)\text{d}\latentMatrix p(\inducingVector) \text{d}\inducingVector
\]</span></small></span></li>
</ul>
</section>
<section id="variational-bayesian-gp-lvm-1" class="slide level2">
<h2>Variational Bayesian GP-LVM</h2>
<ul>
<li>Apply variational lower bound to the inner integral. <small><span class="math display">\[
  \begin{align}
  \int \prod_{i=1}^\numData c_i \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye} p(\latentMatrix)\text{d}\latentMatrix \geq &amp; \expDist{\sum_{i=1}^\numData\log  c_i}{q(\latentMatrix)}\\ &amp; +\expDist{\log\gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector, \latentMatrix)}}{\dataStd^2\eye}}{q(\latentMatrix)}\\&amp; + \KL{q(\latentMatrix)}{p(\latentMatrix)}    
  \end{align}
\]</span></small></li>
<li>Which is analytically tractable for Gaussian <span class="math inline">\(q(\latentMatrix)\)</span> and some covariance functions.</li>
</ul>
</section>
<section id="required-expectations" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>Need expectations under <span class="math inline">\(q(\latentMatrix)\)</span> of: <small><span class="math display">\[
\log c_i = \frac{1}{2\dataStd^2} \left[\kernelScalar_{i, i} - \kernelVector_{i, \inducingVector}^\top \kernelMatrix_{\inducingVector, \inducingVector}^{-1} \kernelVector_{i, \inducingVector}\right]
\]</span></small> and <small><span class="math display">\[
\log \gaussianDist{\dataVector}{\expDist{\mappingFunctionVector}{p(\mappingFunctionVector|\inducingVector,\dataMatrix)}}{\dataStd^2\eye} = -\frac{1}{2}\log 2\pi\dataStd^2 - \frac{1}{2\dataStd^2}\left(\dataScalar_i - \kernelMatrix_{\mappingFunctionVector, \inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\inducingVector\right)^2
\]</span></small></li>
</ul>
</section>
<section id="required-expectations-1" class="slide level2">
<h2>Required Expectations</h2>
<ul>
<li>This requires the expectations <span class="math display">\[
\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}}{q(\latentMatrix)}
\]</span> and <span class="math display">\[
\expDist{\kernelMatrix_{\mappingFunctionVector,\inducingVector}\kernelMatrix_{\inducingVector,\inducingVector}^{-1}\kernelMatrix_{\inducingVector,\mappingFunctionVector}}{q(\latentMatrix)}
\]</span> which can be computed analytically for some covariance functions <span class="citation" data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or through sampling <span class="citation" data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015; Salimbeni and Deisenroth, 2017)</span>.</li>
</ul>
</section>
<section id="see-also" class="slide level2">
<h2>See also …</h2>
<ul>
<li>MAP approach <span class="citation" data-cites="Lawrence:hgplvm07">(Lawrence and Moore, 2007)</span>.</li>
<li>Hamiltonian Monte Carlo approach <span class="citation" data-cites="Havasi:deepgp18">(Havasi et al., 2018)</span>.</li>
<li>Expectation Propagation approach <span class="citation" data-cites="Bui:deep16">(Bui et al., 2016)</span>.</li>
</ul>
</section>
<section id="neural-networks" class="slide level2">
<h2>Neural Networks</h2>
<div class="figure">
<div id="neural-network-uncertainty-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepgp/neural-network-uncertainty.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<p><span style="text-align:right"><span class="citation" data-cites="Izmailov:subspace19">Izmailov et al. (2019)</span></span></p>
</section>
<section id="deep-gaussian-processes-1" class="slide level2">
<h2>Deep Gaussian Processes</h2>
<ul>
<li>Deep architectures allow abstraction of features <span class="citation" data-cites="Bengio:deep09 Hinton:fast06 Salakhutdinov:quantitative08">(Bengio, 2009; Hinton and Osindero, 2006; Salakhutdinov and Murray, n.d.)</span></li>
<li>We use variational approach to stack GP models.</li>
</ul>
</section>
<section id="stacked-pca" class="slide level2">
<h2>Stacked PCA</h2>
<script>
showDivs(0, 'stack-pca-sample');
</script>
<p><small></small> <input id="range-stack-pca-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-pca-sample')" oninput="setDivs('stack-pca-sample')"> <button onclick="plusDivs(-1, 'stack-pca-sample')">❮</button> <button onclick="plusDivs(1, 'stack-pca-sample')">❯</button></p>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-pca-sample-0.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-pca-sample-1.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-pca-sample-2.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-pca-sample-3.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-pca-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="stacked-gp" class="slide level2">
<h2>Stacked GP</h2>
<script>
showDivs(0, 'stack-gp-sample');
</script>
<p><small></small> <input id="range-stack-gp-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-gp-sample')" oninput="setDivs('stack-gp-sample')"> <button onclick="plusDivs(-1, 'stack-gp-sample')">❮</button> <button onclick="plusDivs(1, 'stack-gp-sample')">❯</button></p>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-gp-sample-0.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-gp-sample-1.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-gp-sample-2.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-gp-sample-3.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/stack-gp-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="analysis-of-deep-gps" class="slide level2">
<h2>Analysis of Deep GPs</h2>
<ul>
<li><p><em>Avoiding pathologies in very deep networks</em> <span class="citation" data-cites="Duvenaud:pathologies14">Duvenaud et al. (2014)</span> show that the derivative distribution of the process becomes more <em>heavy tailed</em> as number of layers increase.</p></li>
<li><p><em>How Deep Are Deep Gaussian Processes?</em> <span class="citation" data-cites="Dunlop:deep2017">Dunlop et al. (n.d.)</span> perform a theoretical analysis possible through conditional Gaussian Markov property.</p></li>
</ul>
</section>
<section id="stacked-gps-video-by-david-duvenaud" class="slide level2">
<h2>Stacked GPs (video by David Duvenaud)</h2>
<div class="figure">
<div id="visualization-deep-gp-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/XhIvygQYFFQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" width style=" ">
</object>
</div>
</div>
</section>
<section id="alan-turing" class="slide level2">
<h2>Alan Turing</h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
</section>
<section id="probability-winning-olympics" class="slide level2">
<h2>Probability Winning Olympics?</h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
</section>
<section id="olympic-marathon-data-gp" class="slide level2">
<h2>Olympic Marathon Data GP</h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-marathon-gp.svg" width style=" ">
</object>
</div>
</div>
</section>
<section id="deep-gp-fit" class="slide level2">
<h2>Deep GP Fit</h2>
<ul>
<li><p>Can a Deep Gaussian process help?</p></li>
<li><p>Deep GP is one GP feeding into another.</p></li>
</ul>
</section>
<section id="olympic-marathon-data-deep-gp" class="slide level2">
<h2>Olympic Marathon Data Deep GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/olympic-marathon-deep-gp.svg" width="100%" style=" ">
</object>
</div>
</div>
</section>
<section id="olympic-marathon-data-deep-gp-1" class="slide level2">
<h2>Olympic Marathon Data Deep GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
</section>
<section id="olympic-marathon-data-latent-1" class="slide level2">
<h2>Olympic Marathon Data Latent 1</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-0.svg" width style=" ">
</object>
</div>
</div>
</section>
<section id="olympic-marathon-data-latent-2" class="slide level2">
<h2>Olympic Marathon Data Latent 2</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-layer-1.svg" width style=" ">
</object>
</div>
</div>
</section>
<section id="olympic-marathon-pinball-plot" class="slide level2">
<h2>Olympic Marathon Pinball Plot</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/olympic-marathon-deep-gp-pinball.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="della-gatta-gene-data" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level2">
<h2>Della Gatta Gene Data</h2>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="gene-expression-example" class="slide level2">
<h2>Gene Expression Example</h2>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
</section>
<section id="tp53-gene-data-gp" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="tp53-gene-data-gp-1" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="tp53-gene-data-gp-2" class="slide level2">
<h2>TP53 Gene Data GP</h2>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="multiple-optima" class="slide level2">
<h2>Multiple Optima</h2>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
</div>
<!--
## Multiple Optima  {}



<object class="svgplot " data="../slides/diagrams/gp/multiple-optima001.svg" width="" style=" "></object>-->
</section>
<section id="della-gatta-gene-data-deep-gp" class="slide level2">
<h2>Della Gatta Gene Data Deep GP</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="della-gatta-gene-data-deep-gp-1" class="slide level2">
<h2>Della Gatta Gene Data Deep GP</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
</section>
<section id="della-gatta-gene-data-latent-1" class="slide level2">
<h2>Della Gatta Gene Data Latent 1</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-layer-0.svg" width="50%" style=" ">
</object>
</div>
</div>
</section>
<section id="della-gatta-gene-data-latent-2" class="slide level2">
<h2>Della Gatta Gene Data Latent 2</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-layer-1.svg" width="50%" style=" ">
</object>
</div>
</div>
</section>
<section id="tp53-gene-pinball-plot" class="slide level2">
<h2>TP53 Gene Pinball Plot</h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/della-gatta-gene-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="step-function-data" class="slide level2">
<h2>Step Function Data</h2>
<div class="figure">
<div id="step-function-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/step-function.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="step-function-data-gp" class="slide level2">
<h2>Step Function Data GP</h2>
<div class="figure">
<div id="step-function-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/step-function-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="step-function-data-deep-gp" class="slide level2">
<h2>Step Function Data Deep GP</h2>
<div class="figure">
<div id="step-function-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/step-function-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="step-function-data-deep-gp-1" class="slide level2">
<h2>Step Function Data Deep GP</h2>
<div class="figure">
<div id="step-function-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/step-function-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="step-function-data-latent-1" class="slide level2">
<h2>Step Function Data Latent 1</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/step-function-deep-gp-layer-0.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-2" class="slide level2">
<h2>Step Function Data Latent 2</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/step-function-deep-gp-layer-1.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-3" class="slide level2">
<h2>Step Function Data Latent 3</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/step-function-deep-gp-layer-2.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-4" class="slide level2">
<h2>Step Function Data Latent 4</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/step-function-deep-gp-layer-3.svg" width style=" ">
</object>
</section>
<section id="step-function-pinball-plot" class="slide level2">
<h2>Step Function Pinball Plot</h2>
<div class="figure">
<div id="step-function-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/step-function-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="motorcycle-helmet-data" class="slide level2">
<h2>Motorcycle Helmet Data</h2>
<div class="figure">
<div id="motorcycle-helment-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/motorcycle-helmet.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="motorcycle-helmet-data-gp" class="slide level2">
<h2>Motorcycle Helmet Data GP</h2>
<div class="figure">
<div id="motorcycle-helmet-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/motorcycle-helmet-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="motorcycle-helmet-data-deep-gp" class="slide level2">
<h2>Motorcycle Helmet Data Deep GP</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="motorcycle-helmet-data-deep-gp-1" class="slide level2">
<h2>Motorcycle Helmet Data Deep GP</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="motorcycle-helmet-data-latent-1" class="slide level2">
<h2>Motorcycle Helmet Data Latent 1</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="motorcycle-helmet-data-latent-2" class="slide level2">
<h2>Motorcycle Helmet Data Latent 2</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="motorcycle-helmet-pinball-plot" class="slide level2">
<h2>Motorcycle Helmet Pinball Plot</h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/motorcycle-helmet-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="robot-wireless-ground-truth" class="slide level2">
<h2>Robot Wireless Ground Truth</h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="robot-wifi-data" class="slide level2">
<h2>Robot WiFi Data</h2>
<div class="figure">
<div id="robot-wireless-data-dim-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/robot-wireless-dim-1.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="robot-wifi-data-gp" class="slide level2">
<h2>Robot WiFi Data GP</h2>
<div class="figure">
<div id="robot-wireless-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/robot-wireless-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="robot-wifi-data-deep-gp" class="slide level2">
<h2>Robot WiFi Data Deep GP</h2>
<div class="figure">
<div id="robot-wireless-deep-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/robot-wireless-deep-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="robot-wifi-data-deep-gp-1" class="slide level2">
<h2>Robot WiFi Data Deep GP</h2>
<div class="figure">
<div id="robot-wireless-deep-gp-samples-dim-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/robot-wireless-deep-gp-samples-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="robot-wifi-data-latent-space" class="slide level2">
<h2>Robot WiFi Data Latent Space</h2>
<object class="svgplot " data="../slides/diagrams/deepgp/robot-wireless-ground-truth.svg" width style=" ">
</object>
<p>}</p>
<div class="figure">
<div id="robot-wireless-latent-space-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deepgp/robot-wireless-latent-space.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="high-five-motion-capture-data" class="slide level2">
<h2>‘High Five’ Motion Capture Data</h2>
<ul>
<li>‘High five’ data.</li>
<li>Model learns structure between two interacting subjects.</li>
</ul>
</section>
<section id="shared-lvm" class="slide level2">
<h2>Shared LVM</h2>
<div class="figure">
<div id="shared-latent-variable-model-graph-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/shared.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-gp-high-five-figure" class="figure-frame">
<p><img class="" src="../slides/diagrams/deep-gp-high-five2.png" width="80%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
</div>
</div>
</section>
<section id="subsample-of-the-mnist-data" class="slide level2">
<h2>Subsample of the MNIST Data</h2>
</section>
<section id="fitting-a-deep-gp-to-a-the-mnist-digits-subsample" class="slide level2">
<h2>Fitting a Deep GP to a the MNIST Digits Subsample</h2>
<p><small><span style="text-align:right">Thanks to: Zhenwen Dai and Neil D. Lawrence</span></small></p>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-latent-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/mnist-digits-subsample-latent.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/mnist-digits-subsample-hidden-1-0.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/mnist-digits-subsample-hidden-2-0.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/mnist-digits-subsample-hidden-3-0.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/mnist-digits-subsample-hidden-4-0.svg" width="60%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<div class="figure">
<div id="digit-samples-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/digit-samples-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="deep-health" class="slide level2">
<h2>Deep Health</h2>
<div class="figure">
<div id="deep-health-model-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/deep-health.svg" width="70%" style=" ">
</object>
</div>
</div>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Alvarez:efficient10">
<p>Álvarez, M.A., Luengo, D., Titsias, M.K., Lawrence, N.D., 2010. Efficient multioutput Gaussian processes through variational inducing kernels, in:. pp. 25–32.</p>
</div>
<div id="ref-Bengio:deep09">
<p>Bengio, Y., 2009. Learning Deep Architectures for AI. Found. Trends Mach. Learn. 2, 1–127. <a href="https://doi.org/10.1561/2200000006">https://doi.org/10.1561/2200000006</a></p>
</div>
<div id="ref-Thang:unifying17">
<p>Bui, T.D., Yan, J., Turner, R.E., 2017. A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation. Journal of Machine Learning Research 18, 1–72.</p>
</div>
<div id="ref-Bui:deep16">
<p>Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., Turner, R., 2016. Deep Gaussian processes for regression using approximate expectation propagation, in: Balcan, M.F., Weinberger, K.Q. (Eds.), Proceedings of the 33rd International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, New York, New York, USA, pp. 1472–1481.</p>
</div>
<div id="ref-Dai:gpu14">
<p>Dai, Z., Damianou, A., Hensman, J., Lawrence, N.D., 2014. Gaussian process models with parallelization and GPU acceleration.</p>
</div>
<div id="ref-Damianou:thesis2015">
<p>Damianou, A., 2015. Deep Gaussian processes and variational propagation of uncertainty (PhD thesis). University of Sheffield.</p>
</div>
<div id="ref-Damianou:variational15">
<p>Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference for latent variables and uncertain inputs in Gaussian processes. Journal of Machine Learning Research 17.</p>
</div>
<div id="ref-DellaGatta:direct08">
<p>Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D., Missero, C., Bernardo, D. di, 2008. Direct targets of the trp63 transcription factor revealed by a combination of gene expression profiling and reverse engineering. Genome Research 18, 939–948. <a href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a></p>
</div>
<div id="ref-Dunlop:deep2017">
<p>Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. How deep are deep Gaussian processes? Journal of Machine Learning Research 19, 1–46.</p>
</div>
<div id="ref-Duvenaud:pathologies14">
<p>Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding pathologies in very deep networks, in:.</p>
</div>
<div id="ref-Gal:distributed14">
<p>Gal, Y., Wilk, M. van der, Rasmussen, C.E., n.d. Distributed variational inference in sparse Gaussian process regression and latent variable models, in:.</p>
</div>
<div id="ref-Havasi:deepgp18">
<p>Havasi, M., Hernández-Lobato, J.M., Murillo-Fuentes, J.J., 2018. Inference in deep Gaussian processes using stochastic gradient Hamiltonian Monte Carlo, in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 31. Curran Associates, Inc., pp. 7506–7516.</p>
</div>
<div id="ref-Hensman:bigdata13">
<p>Hensman, J., Fusi, N., Lawrence, N.D., n.d. Gaussian processes for big data, in:.</p>
</div>
<div id="ref-Hinton:fast06">
<p>Hinton, G.E., Osindero, S., 2006. A fast learning algorithm for deep belief nets. Neural Computation 18, 2006.</p>
</div>
<div id="ref-Hoffman:stochastic12">
<p>Hoffman, M., Blei, D.M., Wang, C., Paisley, J., 2012. Stochastic variational inference, arXiv preprint arXiv:1206.7051.</p>
</div>
<div id="ref-Izmailov:subspace19">
<p>Izmailov, P., Maddox, W.J., Kirichenko, P., Garipov, T., Vetrov, D.P., Wilson, A.G., 2019. Subspace inference for bayesian deep learning. CoRR abs/1907.07504.</p>
</div>
<div id="ref-Kalaitzis:simple11">
<p>Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking differentially expressed gene expression time courses through Gaussian process regression. BMC Bioinformatics 12. <a href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a></p>
</div>
<div id="ref-Lawrence:larger07">
<p>Lawrence, N.D., n.d. Learning for larger datasets with the Gaussian process latent variable model, in:. pp. 243–250.</p>
</div>
<div id="ref-Lawrence:hgplvm07">
<p>Lawrence, N.D., Moore, A.J., 2007. Hierarchical Gaussian process latent variable models, in:. pp. 481–488.</p>
</div>
<div id="ref-MacKay:gpintroduction98">
<p>MacKay, D.J.C., n.d. Introduction to Gaussian processes, in:. pp. 133–166.</p>
</div>
<div id="ref-Quinonero:unifying05">
<p>Quiñonero Candela, J., Rasmussen, C.E., 2005. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research 6, 1939–1959.</p>
</div>
<div id="ref-Salakhutdinov:quantitative08">
<p>Salakhutdinov, R., Murray, I., n.d. On the quantitative analysis of deep belief networks, in:. pp. 872–879.</p>
</div>
<div id="ref-Salimbeni:doubly2017">
<p>Salimbeni, H., Deisenroth, M., 2017. Doubly stochastic variational inference for deep Gaussian processes, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural Information Processing Systems 30. Curran Associates, Inc., pp. 4591–4602.</p>
</div>
<div id="ref-Seeger:auto17">
<p>Seeger, M.W., Hetzel, A., Dai, Z., Lawrence, N.D., 2017. Auto-differentiating linear algebra. CoRR abs/1710.08717.</p>
</div>
<div id="ref-Snelson:pseudo05">
<p>Snelson, E., Ghahramani, Z., n.d. Sparse Gaussian processes using pseudo-inputs, in:.</p>
</div>
<div id="ref-Taigman:deepface14">
<p>Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. DeepFace: Closing the gap to human-level performance in face verification, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. <a href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a></p>
</div>
<div id="ref-Titsias:variational09">
<p>Titsias, M.K., n.d. Variational learning of inducing variables in sparse Gaussian processes, in:. pp. 567–574.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
