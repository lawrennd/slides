<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2019-11-11">
  <title>What is Artificial Intelligence?</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="figure-animate.js"></script>
</head>
<body>
\[\newcommand{\tk}[1]{}
%\newcommand{\tk}[1]{\textbf{TK}: #1}
\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">What is Artificial Intelligence?</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2019-11-11</time></p>
  <p class="venue" style="text-align:center">Council on Ethics, The Norwegian Government Pension Fund</p>
</section>

<section class="slide level3">

<!-- Front matter -->
<!-- Front matter -->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!--Back matter-->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!-- SECTION Introduction -->
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction (\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span class="math inline">\(\errorFunction (\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</section>
<section id="machine-learning" class="slide level3">
<h3>Machine Learning</h3>
<ul>
<li>Driver of two different domains:
<ol type="1">
<li><em>Data Science</em>: arises from the fact that we now capture data by happenstance.</li>
<li><em>Artificial Intelligence</em>: emulation of human behaviour.</li>
</ol></li>
<li>Connection: Internet of Things</li>
</ul>
</section>
<section id="machine-learning-1" class="slide level3">
<h3>Machine Learning</h3>
<ul>
<li>Driver of two different domains:
<ol type="1">
<li><em>Data Science</em>: arises from the fact that we now capture data by happenstance.</li>
<li><em>Artificial Intelligence</em>: emulation of human behaviour.</li>
</ol></li>
<li>Connection: Internet of <del>Things</del></li>
</ul>
</section>
<section id="machine-learning-2" class="slide level3">
<h3>Machine Learning</h3>
<ul>
<li>Driver of two different domains:
<ol type="1">
<li><em>Data Science</em>: arises from the fact that we now capture data by happenstance.</li>
<li><em>Artificial Intelligence</em>: emulation of human behaviour.</li>
</ol></li>
<li>Connection: Internet of People</li>
</ul>
</section>
<section id="section" class="slide level3">
<h3></h3>
<center>
Convention for the Protection of <em>Individuals</em> with regard to Automatic Processing of <em>Personal Data</em> (1981/1/28)
</center>
</section>
<section id="what-does-machine-learning-do" class="slide level3">
<h3>What does Machine Learning do?</h3>
<ul>
<li>ML Automates through Data
<ul>
<li><em>Strongly</em> related to statistics.</li>
<li>Field underpins revolution in <em>data science</em> and <em>AI</em></li>
</ul></li>
<li>With AI:
<ul>
<li><em>logic</em>, <em>robotics</em>, <em>computer vision</em>, <em>speech</em></li>
</ul></li>
<li>With Data Science:
<ul>
<li><em>databases</em>, <em>data mining</em>, <em>statistics</em>, <em>visualization</em></li>
</ul></li>
</ul>
</section>
<section id="what-does-machine-learning-do-1" class="slide level3">
<h3>What does Machine Learning do?</h3>
<ul>
<li>Automation scales by codifying processes and automating them.</li>
<li>Need:
<ul>
<li>Interconnected components</li>
<li>Compatible components</li>
</ul></li>
<li>Early examples:
<ul>
<li>cf Colt 45, Ford Model T</li>
</ul></li>
</ul>
</section>
<section id="codify-through-mathematical-functions" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression</li>
</ul>
<p><span class="math display">\[ \text{odds} = \frac{p(\text{bought})}{p(\text{not bought})} \]</span></p>
<p><span class="math display">\[ \log \text{odds}  = \beta_0 + \beta_1 \text{age} + \beta_2 \text{latitude}.\]</span></p>
</section>
<section id="codify-through-mathematical-functions-1" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression</li>
</ul>
<p><span class="math display">\[ p(\text{bought}) =  \sigmoid{\beta_0 + \beta_1 \text{age} + \beta_2 \text{latitude}}.\]</span></p>
</section>
<section id="codify-through-mathematical-functions-2" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression</li>
</ul>
<p><span class="math display">\[ p(\text{bought}) =  \sigmoid{\boldsymbol{\beta}^\top \inputVector}.\]</span></p>
</section>
<section id="codify-through-mathematical-functions-3" class="slide level3">
<h3>Codify Through Mathematical Functions</h3>
<ul>
<li>How does machine learning work?</li>
<li>Jumper (jersey/sweater) purchase with logistic regression</li>
</ul>
<p><span class="math display">\[ \dataScalar =  \mappingFunction\left(\inputVector, \boldsymbol{\beta}\right).\]</span></p>
<div class="fragment">
<p>We call <span class="math inline">\(\mappingFunction(\cdot)\)</span> the <em>prediction function</em>.</p>
</div>
</section>
<section id="fit-to-data" class="slide level3">
<h3>Fit to Data</h3>
<ul>
<li>Use an objective function</li>
</ul>
<p><span class="math display">\[\errorFunction(\boldsymbol{\beta}, \dataMatrix, \inputMatrix)\]</span></p>
<div class="fragment">
<ul>
<li>E.g. least squares <span class="math display">\[\errorFunction(\boldsymbol{\beta}, \dataMatrix, \inputMatrix) = \sum_{i=1}^\numData \left(\dataScalar_i - \mappingFunction(\inputVector_i, \boldsymbol{\beta})\right)^2.\]</span></li>
</ul>
</div>
</section>
<section id="two-components" class="slide level3">
<h3>Two Components</h3>
<ul>
<li>Prediction function, <span class="math inline">\(\mappingFunction(\cdot)\)</span></li>
<li>Objective function, <span class="math inline">\(\errorFunction(\cdot)\)</span></li>
</ul>
<!-- SECTION Deep Learning -->
</section>
<section id="deep-learning" class="slide level3">
<h3>Deep Learning</h3>
<ul>
<li><p>These are interpretable models: vital for disease modeling etc.</p></li>
<li><p>Modern machine learning methods are less interpretable</p></li>
<li><p>Example: face recognition</p></li>
</ul>
<!-- No slide titles in this context -->
</section>
<section id="deepface" class="slide level3">
<h3>DeepFace</h3>
<p><span class="fragment fade-in"><small>Outline of the DeepFace architecture. A front-end of a single convolution-pooling-convolution filtering on the rectified input, followed by three locally-connected layers and two fully-connected layers. Color illustrates feature maps produced at each layer. The net includes more than 120 million parameters, where more than 95% come from the local and fully connected.</small></span></p>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<p><span style="text-align:right"><small>Source: DeepFace <span class="citation" data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small></span></p>
</section>
<section id="deep-learning-as-pinball" class="slide level3">
<h3>Deep Learning as Pinball</h3>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/pinball001.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/pinball002.svg" width="80%" style=" ">
</object>
</div>
</div>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<table>
<tr>
<td>
</td>
<td align="center">
<div class="fragment" data-fragment-index="6" style="">
<object class="svgplot " data="../slides/diagrams/computer.svg" width="100%" style=" ">
</object>
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="4" style="">
<object class="svgplot " data="../slides/diagrams/human.svg" width="100%" style=" ">
</object>
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="1" style="">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Jean-Dominique_Bauby.jpg" width="150%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</td>
</tr>
<tr>
<td>
<div class="fragment" data-fragment-index="2" style="">
bits/min
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="6" style="">
billions
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="5" style="">
2000
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="3" style="">
6
</div>
</td>
</tr>
<tr>
<td>
<div class="fragment" data-fragment-index="7" style="">
billion<br>calculations/s
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="9" style="">
~100
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="8" style="">
a billion
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="8" style="">
a billion
</div>
</td>
</tr>
<tr>
<td>
<div class="fragment" data-fragment-index="10" style="">
embodiment
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="11" style="">
20 minutes
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="12" style="">
5 billion years
</div>
</td>
<td align="center">
<div class="fragment" data-fragment-index="12" style="">
15 trillion years
</div>
</td>
</tr>
</table>
<!--include{_data-science/includes/evolved-relationship-society.md}-->
<!--include{_ai/includes/embodiment-and-narrative.md}-->
</section>
<section id="turing-ai-fellowship" class="slide level3">
<h3>Turing AI Fellowship</h3>
<ul>
<li>Work in this space now funded by a <a href="https://www.turing.ac.uk/news/welcoming-world-class-turing-ai-fellows-institute">Senior AI Fellowship at the Turing Institute</a>.</li>
</ul>
</section>
<section id="project-description" class="slide level3">
<h3>Project Description</h3>
<blockquote>
<p>It used to be true that computers only did what we programmed them to do, but today AI systems are learning from our data. This introduces new problems in how these systems respond to their environment.</p>
<p>We need to better monitor how data is influencing decision making and take corrective action as required.</p>
</blockquote>
</section>
<section id="aim" class="slide level3">
<h3>Aim</h3>
<ul>
<li>Scale safe and reliable AI solutions.</li>
<li>Move from Auto ML to Auto AI</li>
<li>Bayesian Optimisation to Bayesian System Optimisation</li>
</ul>
</section>
<section id="motivating-example" class="slide level3">
<h3>Motivating Example</h3>
</section>
<section id="safeboda" class="slide level3">
<h3>SafeBoda</h3>
<div class="figure">
<div id="safe-boda-system-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ai/safe-boda.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="safeboda-1" class="slide level3">
<h3>SafeBoda</h3>
<blockquote>
<p>With road accidents set to match HIV/AIDS as the highest cause of death in low/middle income countries by 2030, SafeBoda’s aim is to modernise informal transportation and ensure safe access to mobility.</p>
</blockquote>
<!-- SECTION Data Trusts -->
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<div class="figure">
<div id="data-trusts-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/data-science/data-trusts.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<div class="figure">
<div id="hall-presenti-report-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/data-science/data-trusts-review.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="motivation" class="slide level3">
<h3>Motivation</h3>
<ol type="1">
<li>Indsidious decision-making that has downstream instrumental effects we don’t control.</li>
<li>A power-asymmetry between data-controllers and data-subjects</li>
<li>A loss of personhood in the re-representation of ourselves in the digital world.</li>
<li>The GDPR’s endeavour to curb contractual freedom cannot by itself reverse the power-asymmetry between data-controllers and data-subjects.</li>
</ol>
</section>
<section id="analogy" class="slide level3">
<h3>Analogy</h3>
<ul>
<li>Digital Democracy vs Digital Oligarchy <span class="citation" data-cites="Lawrence:digitaloligarchy15">Lawrence (2015a)</span> or Information Feudalism <span class="citation" data-cites="Lawrence:informationbarons15">Lawrence (2015b)</span></li>
<li>Data <em>subjects</em>, data <em>controllers</em> and data <em>processors</em>.</li>
</ul>
</section>
<section id="legal-mechanism-of-trusts" class="slide level3">
<h3>Legal Mechanism of Trusts</h3>
<ul>
<li>Fiduciary responsibility of Trustees.
<ul>
<li>Burden of proof in negligence is reversed.</li>
</ul></li>
<li>Trustees are data <em>controllers</em></li>
<li>Beneficiaries are data <em>subjects</em></li>
<li>Power of data accumulation wielded on the beneficiaries behalf</li>
<li>See <span class="citation" data-cites="Edwards:privacy04">Edwards (2004)</span>, <span class="citation" data-cites="Delacroix:trusts19">Delacroix and Lawrence (2019)</span> and <span class="citation" data-cites="Lawrence:trusts16">Lawrence (2016)</span></li>
</ul>
</section>
<section id="five-ai-myths" class="slide level3">
<h3>Five AI Myths</h3>
<ol type="1">
<li>AI will be the first wave of automation that adapts to us.</li>
<li>Hearsay data has significant value.</li>
<li>The big tech companies have the landscape all ‘sewn up’</li>
<li>‘data scientists’ will come and solve all problems.</li>
<li>The normal rules of business don’t apply to AI.</li>
</ol>
</section>
<section id="mythbusting" class="slide level3">
<h3>Mythbusting</h3>
<ul>
<li>Area of good data:
<ul>
<li>Finance</li>
</ul></li>
</ul>
</section>
<section id="criteria-for-success" class="slide level3">
<h3>Criteria for Success</h3>
<ul>
<li>Executive sponsorship (Office of CEO).</li>
<li>Technical Expertise (Open minded expert).</li>
<li>Financial buy in (CFO).</li>
<li>Assimilated knownledge (CTO).</li>
</ul>
</section>
<section id="normal-organisational-rules-apply" class="slide level3">
<h3>Normal Organisational Rules Apply</h3>
<ul>
<li>AI is not magical pixie dust</li>
<li>Standard organisational instincts apply</li>
<li>Disruption requires <em>agile</em> thinking.
<ul>
<li>Don’t be the Grand Old Duke of York</li>
<li>Be Special Forces</li>
</ul></li>
</ul>
</section>
<section id="five-ai-myths-1" class="slide level3">
<h3>Five AI Myths</h3>
<ol type="1">
<li>AI will be the first wave of automation that adapts to us.</li>
<li>There is value locked into data which is thoughtlessly collected.</li>
<li>The big tech companies have the landscape all ‘sewn up’</li>
<li>There is a new emergent class of ‘data scientist’ that will solve all problems.</li>
<li>The normal rules of business don’t apply to AI.</li>
</ol>
</section>
<section id="thanks" class="slide level3">
<h3>Thanks!</h3>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li><p>Guardian article on <a href="https://www.theguardian.com/media-network/https://www.theguardian.com/media-network/2016/jun/03/data-trusts-privacy-fears-feudalism-democracy">Data Trusts</a></p></li>
<li><p>Guardian article on <a href="https://www.theguardian.com/media-network/https://www.theguardian.com/media-network/2015/mar/05/digital-oligarchy-algorithms-personal-data">Digital Oligarchies</a></p></li>
<li><p>Guardian article on <a href="https://www.theguardian.com/media-network/https://www.theguardian.com/media-network/2015/nov/16/information-barons-threaten-autonomy-privacy-online">Information Feudalism</a></p></li>
<li><p>Blog post on <a href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What is Machine Learning?</a></p></li>
<li><p>Blog post on <a href="http://inverseprobability.com/2017/01/12/data-readiness-levels">Data Readiness Levels</a></p></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Delacroix:trusts19">
<p>Delacroix, S., Lawrence, N.D., 2019. Bottom-up data trusts: Disturbing the “one size fits all” approach to data governance. International Data Privacy Law. <a href="https://doi.org/10.1093/idpl/ipz014" class="uri">https://doi.org/10.1093/idpl/ipz014</a></p>
</div>
<div id="ref-Edwards:privacy04">
<p>Edwards, L., 2004. The problem with privacy. International Review of Law Computers &amp; Technology 18, 263–294.</p>
</div>
<div id="ref-Lawrence:trusts16">
<p>Lawrence, N.D., 2016. Data trusts could allay our privacy fears.</p>
</div>
<div id="ref-Lawrence:digitaloligarchy15">
<p>Lawrence, N.D., 2015a. Beware the rise of the digital oligarchy.</p>
</div>
<div id="ref-Lawrence:informationbarons15">
<p>Lawrence, N.D., 2015b. The information barons threaten our autonomy and our privacy.</p>
</div>
<div id="ref-Taigman:deepface14">
<p>Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014. DeepFace: Closing the gap to human-level performance in face verification, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. <a href="https://doi.org/10.1109/CVPR.2014.220" class="uri">https://doi.org/10.1109/CVPR.2014.220</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/math/math.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
