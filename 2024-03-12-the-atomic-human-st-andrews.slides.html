<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2024-03-12">
  <title>The Atomic Human</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="https://inverseprobability.com/assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">The Atomic Human</h1>
  <p class="subtitle" style="text-align:center">Understanding Ourselves
in the Age of AI</p>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil
D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2024-03-12</time></p>
  <p class="venue" style="text-align:center">St Andrews’ Distinguished
Lecture Series</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-atomic-human-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/the-atomic-human.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a href="https://www.amazon.co.uk/dp/B0CGZHBSLL">The Atomic Human</a>
<span class="citation" data-cites="Lawrence-atomic24">(Lawrence,
2024)</span> due for release in June 2024.
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="figure">
<div id="ford-model-t-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/1925_Ford_Model_T_touring.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A 1925 Ford Model T built at Henry Ford’s Highland Park Plant in
Dearborn, Michigan. This example now resides in Australia, owned by the
founder of FordModelT.net. From <a
href="https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg"
class="uri">https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg</a>
</aside>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Embodiment factors are the ratio between our ability to compute and our
ability to communicate. Relative to the machine we are also locked in.
In the table we represent embodiment as the length of time it would take
to communicate one second’s worth of computation. For computers it is a
matter of minutes, but for a human, it is a matter of thousands of
millions of years. See also “Living Together: Mind and Machine
Intelligence” <span class="citation"
data-cites="Lawrence:embodiment17">Lawrence (2017)</span>
</aside>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="figure">
<div id="chicago-cuneiform-stone-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//cuneiform/chicago-cuneiform-stone.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Chicago Stone, side 2, recording sale of a number of fields, probably
from Isin, Early Dynastic Period, c. 2600 BC, black basalt
</aside>
<!-- Faster horse -->
<!-- Embodiment Factors -->
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="figure">
<div id="claude-shannon-figure" class="figure-frame">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ClaudeShannon_MFO3807.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Claude Shannon</em>
</center>
</div>
</div>
<aside class="notes">
Claude Shannon (1916-2001)
</aside>
<aside class="notes">
Just as we measure temperature. Just as Celsius gave us the scale for
temperature, so entropy measures ignorance. Formalised most famously by
Claude Shannon
</aside>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Embodiment factors are the ratio between our ability to compute and our
ability to communicate. Relative to the machine we are also locked in.
In the table we represent embodiment as the length of time it would take
to communicate one second’s worth of computation. For computers it is a
matter of minutes, but for a human, it is a matter of thousands of
millions of years.
</aside>
<!-- Information Triangle -->
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information001.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information002.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information003.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The trinity of human, data, and computer, and highlights the modern
phenomenon. The communication channel between computer and data now has
an extremely high bandwidth. The channel between human and computer and
the channel between data and human is narrow. New direction of
information flow, information is reaching us mediated by the computer.
The focus on classical statistics reflected the importance of the direct
communication between human and data. The modern challenges of data
science emerge when that relationship is being mediated by the machine.
</aside>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<ul>
<li>This phenomenon has already revolutionised biology.
<ul>
<li>Large scale data acquisition and distribution.</li>
<li>Transcriptomics, genomics, epigenomics, ‘rich phenomics’.</li>
</ul></li>
<li>Great <em>promise</em> for personalized health.</li>
</ul>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<ul>
<li>Automated decision making within the computer based only on the
data.</li>
<li>Subjective biases need to be better understood.</li>
<li>Particularly important where treatments are being prescribed.
<ul>
<li>Interventions could be far more subtle.</li>
</ul></li>
</ul>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<ul>
<li>Shift in dynamic:
<ul>
<li>from direct human-data to indirect human-computer-data</li>
<li>modern data analysis is mediated by the machine</li>
</ul></li>
<li>This change of dynamics gives us the modern and emerging domain of
data science</li>
</ul>
<!-- AI Fallacy -->
<!-- SECTION The Great AI Fallacy -->
</section>
<section id="the-great-ai-fallacy" class="slide level2">
<h2>The Great AI Fallacy</h2>
<div class="figure">
<div id="jeeves-springtime-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jeeves_in_the_Springtime_01.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
We seem to have fallen for a perspective on AI that suggests it will
adapt to our schedule, rather in the manner of a 1930s manservant.
</aside>
<aside class="notes">
Since the machine learning field was rebranded as AI, done public
understanding, but also more interconnection with other fields.
Struggled to find a consistent definition for AI. But the definitions I
think public uses have one thing in common. The idea that AI will be the
first generation of automation to adapt to us.
</aside>
<!-- Mathematical Statistics -->
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>??</p>
</blockquote>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and ‘big data’</p>
<p>Neil Lawrence 1972-?</p>
</blockquote>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="figure">
<div id="portrait-of-karl-pearson-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Portrait_of_Karl_Pearson.jpg" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Karl Pearson (1857-1936), one of the founders of Mathematical
Statistics.
</aside>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="figure">
<div id="question-mark-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Question_mark.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
‘Founded’ by ? (?-?)
</aside>
<!-- Conversation -->
</section>
<section id="section-18" class="slide level2">
<h2></h2>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation000.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation001.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation002.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation003.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation004.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation005.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation006.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation007.svg" width="70%" style=" ">
</object>
</div>
<!-- Fritz Heider -->
</section>
<section id="section-27" class="slide level2">
<h2></h2>
<div class="figure">
<div id="heider-simmel-shapes-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/8FIEZXMUM2I?start=7" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Fritz Heider and Marianne Simmel’s video of shapes from <span
class="citation" data-cites="Heider-experimental44">Heider and Simmel
(1944)</span>.
</aside>
</section>
<section id="section-28" class="slide level2">
<h2></h2>
<div class="figure">
<div id="classic-baby-shoes-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Classic_baby_shoes.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
For sale: baby shoes, never worn
</center>
</div>
</div>
<aside class="notes">
Consider the six-word novel, apocryphally credited to Ernest Hemingway,
“For sale: baby shoes, never worn”. To understand what that means to a
human, you need a great deal of additional context. Context that is not
directly accessible to a machine that has not got both the evolved and
contextual understanding of our own condition to realize both the
implication of the advert and what that implication means emotionally to
the previous owner.
</aside>
<aside class="notes">
Ernest Hemingway’s apocryphal six-word novel would mean nothing to a
computer.
</aside>
<!-- Conversation LLM -->
</section>
<section id="section-29" class="slide level2">
<h2></h2>
</section>
<section id="section-30" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation000.svg" width="80%" style=" ">
</object>
</section>
<section id="section-31" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation001.svg" width="80%" style=" ">
</object>
</section>
<section id="section-32" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation002.svg" width="80%" style=" ">
</object>
</section>
<section id="section-33" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation003.svg" width="80%" style=" ">
</object>
</section>
<section id="section-34" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation004.svg" width="80%" style=" ">
</object>
</section>
<section id="section-35" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation005.svg" width="80%" style=" ">
</object>
</section>
<section id="section-36" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation006.svg" width="80%" style=" ">
</object>
</section>
<section id="section-37" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation007.svg" width="80%" style=" ">
</object>
<aside class="notes">
This can be disturbing to humans because we are used to a low bandwidth
communication rate.
</aside>
</section>
<section id="section-38" class="slide level2">
<h2></h2>
<div class="figure">
<div id="anne-probability-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-probability-conversation.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.
</aside>
</section>
<section id="section-39" class="slide level2">
<h2></h2>
<ul>
<li><p>There is a lot of evidence that probabilities aren’t
interpretable.</p></li>
<li><p>See e.g. <span class="citation"
data-cites="Thompson-juries89">Thompson (1989)</span></p></li>
</ul>
</section>
<section id="section-40" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-computers-interacting-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-computers-interacting.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Humans and computers interacting should be a major focus of our research
and engineering efforts.
</aside>
</section>
<section id="section-41" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-culture-interacting-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-culture-interacting.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Humans use culture, facts and ‘artefacts’ to communicate.
</aside>
</section>
<section id="section-42" class="slide level2">
<h2></h2>
<div class="figure">
<div id="david-andrew-marr-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/9388XmWIHXg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Professor Sir David Spiegelhalter on Andrew Marr on 10th May 2020
speaking about some of the challengers around data, data presentation,
and decision making in a pandemic. David mentions number theatre at 9
minutes 10 seconds.
</aside>
<!--includebbcvideo{p08csg28}-->
</section>
<section id="section-43" class="slide level2">
<h2></h2>
<div class="figure">
<div id="data-theatre-000-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//business/data-theatre000.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The phenomenon of number theatre or <em>data theatre</em> was described
by David Spiegelhalter and is nicely summarized by Martin Robbins in
this sub-stack article <a
href="https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards"
class="uri">https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards</a>.
</aside>
</section>
<section id="section-44" class="slide level2">
<h2></h2>
<div class="figure">
<div id="data-theatre-001-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//business/data-theatre001.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The phenomenon of number theatre or <em>data theatre</em> was described
by David Spiegelhalter and is nicely summarized by Martin Robbins in
this sub-stack article <a
href="https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards"
class="uri">https://martinrobbins.substack.com/p/data-theatre-why-the-digital-dashboards</a>.
</aside>
<!--






## {}






<div class="figure">
<div class="figure-frame" id="monkey-business-figure">
<iframe width="600" height="450" src="https://www.youtube.com/embed/vJG698U2Mvo?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
</div>
<aside class="notes">Daniel Simon's famous illusion "monkey business". Focus on the movement of the ball distracts the viewer from seeing other aspects of the image.</aside>












-->
<!--include{_data-science/includes/data-selection-attention-bias.md}-->
</section>
<section id="section-45" class="slide level2">
<h2></h2>
<div class="figure">
<div id="anne-llm-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-llm-conversation.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.
</aside>
</section>
<section id="section-46" class="slide level2">
<h2></h2>
<ul>
<li><p>LLMs are already being used for robot planning <span
class="citation" data-cites="Huang-inner22">Huang et al.
(2023)</span></p></li>
<li><p>Ambiguities are reduced when the machine has had large scale
access to human cultural understanding.</p></li>
</ul>
</section>
<section id="section-47" class="slide level2">
<h2></h2>
<div class="figure">
<div id="ai-for-data-analytics-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/0sJjdxn5kcI?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
The Inner Monologue paper suggests using LLMs for robotic planning <span
class="citation" data-cites="Huang-inner22">(Huang et al., 2023)</span>.
</aside>
</section>
<section id="section-48" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-moniac-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Bill Phillips and his MONIAC (completed in 1949). The machine is an
analogue computer designed to simulate the workings of the UK economy.
</aside>
</section>
<section id="section-49" class="slide level2">
<h2></h2>
<div class="figure">
<div id="donald-maccrimmon-mackay-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//people/DonaldMacKay1952.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Donald M. MacKay (1922-1987), a physicist who was an early member of the
cybernetics community and member of the Ratio Club.
</aside>
</section>
<section id="section-50" class="slide level2">
<h2></h2>
<div class="figure">
<div id="low-angle-fire-control-team-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/low-angle-fire-control-team.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The fire control computer set at the centre of a system of observation
and tracking <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
</aside>
</section>
<section id="section-51" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-measurement-of-inclination-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-measurement-of-inclination.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Measuring inclination between two ships <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
Sophisticated fire control computers allowed the ship to continue to
fire while under maneuvers.
</aside>
</section>
<section id="section-52" class="slide level2">
<h2></h2>
<div class="figure">
<div id="typical-modern-fire-control-table-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/typical-modern-fire-control-table.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A second world war gun computer’s control table <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
</aside>
</section>
<section id="section-53" class="slide level2">
<h2></h2>
<div class="figure">
<div id="us-navy-training-film-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/gwf5mAlI7Ug?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
U.S. Navy training film MN-6783a. Basic Mechanisms of Fire Control
Computers. Mechanical Computer Instructional Film 27794 (1953) for the
Mk 1A Fire Control Computer.
</aside>
</section>
<section id="section-54" class="slide level2">
<h2></h2>
<div class="figure">
<div id="behind-the-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/behind-the-eye.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="https://www.amazon.co.uk/Behind-Eye-Gifford-Lectures-MACKAY/dp/0631173323">Behind
the Eye</a> <span class="citation" data-cites="Mackay-behind91">(MacKay,
1991)</span> summarises MacKay’s Gifford Lectures, where MacKay uses the
operation of the eye as a window on the operation of the brain.
</aside>
</section>
<section id="section-55" class="slide level2">
<h2></h2>
<blockquote>
<p>Later in the 1940’s, when I was doing my Ph.D. work, there was much
talk of the brain as a computer and of the early digital computers that
were just making the headlines as “electronic brains.” As an analogue
computer man I felt strongly convinced that the brain, whatever it was,
was not a digital computer. I didn’t think it was an analogue computer
either in the conventional sense.</p>
</blockquote>
</section>
<section id="section-56" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-analogue-machine-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/human-analogue-machine.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The human analogue machine creates a feature space which is analagous to
that we use to reason, one way of doing this is to have a machine
attempt to compress all human generated text in an auto-regressive
manner.
</aside>
</section>
<section id="section-57" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-analogue-machine-2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor-ham.svg" width="40%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<ul>
<li><p>A human-analogue machine is a machine that has created a feature
space that is analagous to the “feature space” our brain uses to
reason.</p></li>
<li><p>The latest generation of LLMs are exhibiting this charateristic,
giving them ability to converse.</p></li>
</ul>
</section>
<section id="section-58" class="slide level2">
<h2></h2>
<ul>
<li>Perils of this include <em>counterfeit people</em>.</li>
<li>Daniel Dennett has described the challenges these bring in <a
href="https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/">an
article in The Atlantic</a>.</li>
</ul>
</section>
<section id="section-59" class="slide level2">
<h2></h2>
<ul>
<li><p>But if correctly done, the machine can be appropriately
“psychologically represented”</p></li>
<li><p>This might allow us to deal with the challenge of
<em>intellectual debt</em> where we create machines we cannot
explain.</p></li>
</ul>
</section>
<section id="section-60" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-4-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information004.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The trinity of human, data, and computer, and highlights the modern
phenomenon. The communication channel between computer and data now has
an extremely high bandwidth. The channel between human and computer and
the channel between data and human is narrow. New direction of
information flow, information is reaching us mediated by the computer.
The focus on classical statistics reflected the importance of the direct
communication between human and data. The modern challenges of data
science emerge when that relationship is being mediated by the machine.
</aside>
</section>
<section id="section-61" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-ham-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information-ham.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The HAM now sits between us and the traditional digital computer.
</aside>
</section>
<section id="section-62" class="slide level2">
<h2></h2>
<blockquote>
<p>One thing is I can live with is doubt, and uncertainty and not
knowing. I think it’s much more interesting to live with not knowing
than to have an answer that might be wrong.</p>
<p>Richard P. Feynmann in the <em>The Pleasure of Finding Things
Out</em> 1981.</p>
</blockquote>
</section>
<section id="section-63" class="slide level2">
<h2></h2>
</section>
<section id="section-64" class="slide level2">
<h2></h2>
<div class="figure">
<div id="hydrodynamica-danielis-bernoulli-figure" class="figure-frame">
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=3yRVAAAAcAAJ&amp;pg=PP7"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/3yRVAAAAcAAJ-PP7.png" /></a>
</div>
</div>
</div>
<aside class="notes">
Daniel Bernoulli’s <em>Hydrodynamica</em> published in 1738. It was one
of the first works to use the idea of conservation of energy. It used
Newton’s laws to predict the behaviour of gases.
</aside>
</section>
<section id="section-65" class="slide level2">
<h2></h2>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=3yRVAAAAcAAJ&amp;pg=PA200"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/3yRVAAAAcAAJ-PA200.png" /></a>
</div>
</div>
</div>
<aside class="notes">
Daniel Bernoulli’s chapter on the kinetic theory of gases, for a review
on the context of this chapter see <span class="citation"
data-cites="Mikhailov:hydrodynamica05">Mikhailov (n.d.)</span>. For 1738
this is extraordinary thinking. The notion of kinetic theory of gases
wouldn’t become fully accepted in Physics until 1908 when a model of
Einstein’s was verified by Jean Baptiste Perrin.
</aside>
</section>
<section id="section-66" class="slide level2">
<h2></h2>
<div class="figure">
<div id="entropy-billiards-js-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="multiball-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left ">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="multiball-entropy">
</output>
</div>
<div id="multiball-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="multiball-newball" style="text-align:right">
New Ball
</button>
<button id="multiball-pause" style="text-align:right">
Pause
</button>
<button id="multiball-skip" style="text-align:right">
Skip 1000s
</button>
<button id="multiball-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/ballworld.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/multiball.js"></script>
</div>
</div>
<aside class="notes">
Bernoulli’s simple kinetic models of gases assume that the molecules of
air operate like billiard balls.
</aside>
</section>
<section id="section-67" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gaussian-histogram-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ml/gaussian-histogram.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
James Clerk Maxwell 1831-1879 Derived distribution of velocities of
particles in an ideal gas (elastic fluid).
</aside>
</section>
<section id="section-68" class="slide level2">
<h2></h2>
<div class="figure">
<div id="maxwell-boltzmann-gibbs-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906) Josiah
Willard Gibbs (1839-1903)
</aside>
</section>
<section id="section-69" class="slide level2">
<h2></h2>
<div class="figure">
<div id="boltzmann-warmetheorie-figure" class="figure-frame">
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=Vuk5AQAAMAAJ&amp;pg=PA373"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/Vuk5AQAAMAAJ-PA373.png" /></a>
</div>
</div>
</div>
<aside class="notes">
Boltzmann’s paper <span class="citation"
data-cites="Boltzmann-warmetheorie77">Boltzmann (n.d.)</span> which
introduced the relationship between entropy and probability. A
translation with notes is available in <span class="citation"
data-cites="Kim-translation15">Sharp and Matschinsky (2015)</span>.
</aside>
</section>
<section id="section-70" class="slide level2">
<h2></h2>
<div class="figure">
<div id="eddington-book-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/arthur-stanley-eddington.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_7.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Eddington’s book on the Nature of the Physical World <span
class="citation" data-cites="Eddington:nature29">(Eddington,
1929)</span>
</aside>
</section>
<section id="section-71" class="slide level2">
<h2></h2>
<div class="figure">
<div id="physical-world-chandra-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/ChandraNobel.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Chandrasekhar (1910-1995) derived the limit at which a star collapses in
on itself. Eddington’s confidence in the 2nd law may have been what
drove him to dismiss Chandrasekhar’s ideas, humiliating a young
scientist who would later receive a Nobel prize for the work.
</aside>
</section>
<section id="section-72" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deepest-humiliation-eddington-cropped-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Eddington makes his feelings about the primacy of the second law clear.
This primacy is perhaps because the second law can be demonstrated
mathematically, building on the work of Maxwell, Gibbs and Boltzmann.
<span class="citation" data-cites="Eddington:nature29">Eddington
(1929)</span>
</aside>
</section>
<section id="section-73" class="slide level2">
<h2></h2>
<div class="figure">
<div id="albert-einstein-photo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Albert Einstein’s 1905 paper on Brownian motion introduced stochastic
differential equations which can be used to model the ‘curve of a simple
molecule of air’.
</aside>
</section>
<section id="section-74" class="slide level2">
<h2></h2>
<div class="figure">
<div id="russell-wiener-russell-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Betrand Russell</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="85%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Albert Einstein</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Norbert Wiener</em>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Bertrand Russell (1872-1970), Albert Einstein (1879-1955), Norbert
Wiener, (1894-1964)
</aside>
<aside class="notes">
Wiener came to Cambridge in 1913. Russell showed him Einstein’s 1905
paper on Brownian motion (<span class="citation"
data-cites="Einstein-brownian05">Einstein (1905)</span>)
</aside>
</section>
<section id="section-75" class="slide level2">
<h2></h2>
<div class="figure">
<div id="brownian-motion-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Brownian motion of a large particle in a group of smaller particles. The
movement is known as a <em>Wiener process</em> after Norbert Wiener.
</aside>
</section>
<section id="section-76" class="slide level2">
<h2></h2>
<div class="figure">
<div id="norbert-wiener-yellow-peril-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/wiener-yellow-peril.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Norbert Wiener (1894 - 1964). Founder of cybernetics and the information
era. He used Gibbs’s ideas to develop a “theory of ignorance” that he
deployed in early communication. On the right is Wiener’s wartime report
that used stochastic processes in forecasting with applications in radar
control (image from <span class="citation"
data-cites="Coales-yellow14">Coales and Kane (2014)</span>).
</aside>
<!-- Lecture 2 -->
</section>
<section id="section-77" class="slide level2">
<h2></h2>
<div class="figure">
<div id="kappen-ball-figure" class="figure-frame">
<div>
<div style="width:900px;text-align:center;display:inline">
<span style="float:left;">Score:
<output id="kappenball-score">
</output>
</span> <span style="float:right;">Energy:
<output id="kappenball-energy">
</output>
</span>
<div style="clear: both;">

</div>
</div>
<canvas id="kappenball-canvas" width="900" height="500" style="border:1px solid black;display:inline;text-align:center ">
</canvas>
<div>
<input type="range" min="0" max="100" value="0" class="slider" id="kappenball-stochasticity" style="width:900px;"/>
</div>
<div>
<button id="kappenball-newball" style="text-align:right">
New Ball
</button>
<button id="kappenball-pause" style="text-align:right">
Pause
</button>
</div>
<output id="kappenball-count">
</output>
<script src="https://inverseprobability.com/talks/scripts//ballworld/kappenball.js"></script>
</div>
</div>
</div>
<aside class="notes">
Kappen Ball
</aside>
</section>
<section id="section-78" class="slide level2">
<h2></h2>
</section>
<section id="section-79" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-rules-loneliness-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>loneliness</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
‘Death’ through loneliness in Conway’s game of life. If a cell is
surrounded by less than three cells, it ‘dies’ through loneliness.
</aside>
</section>
<section id="section-80" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>overcrowding</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
‘Death’ through overpopulation in Conway’s game of life. If a cell is
surrounded by more than three cells, it ‘dies’ through loneliness.
</aside>
</section>
<section id="section-81" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>birth</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Birth in Conway’s life. Any position surrounded by precisely three live
cells will give birth to a new cell at the next turn.
</aside>
</section>
<section id="section-82" class="slide level2">
<h2></h2>
<div class="figure">
<div id="glider-loafer-conway-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Glider (1969)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Glider.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
<em>Left</em> A Glider pattern discovered 1969 by Richard K. Guy.
<em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020). The glider is an oscillator that moves diagonally after
creation. From the simple rules of Life it’s not obvious that such an
object does exist, until you do the necessary computation.
</aside>
</section>
<section id="section-83" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gosper-glider-gun-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Gosperglidergun.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Gosper glider gun is a configuration that creates gliders. A new
glider is released after every 30 turns.
</aside>
</section>
<section id="section-84" class="slide level2">
<h2></h2>
</section>
<section id="section-85" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-loafer-spaceship-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Loafer (2013)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Loafer.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
<em>Left</em> A Loafer pattern discovered by Josh Ball in 2013.
<em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020).
</aside>
</section>
<section id="section-86" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-in-life-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-in-life.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Game of Life running in Life. The video is drawing out recursively
showing pixels that are being formed by filling cells with moving
spaceships. Each individual pixel in this game of life is made up of
<span class="math inline">\(2048 \times 2048\)</span> pixels called an
<a href="https://www.conwaylife.com/wiki/OTCA_metapixel">OTCA
metapixel</a>.
</aside>
</section>
<section id="section-87" class="slide level2">
<h2></h2>
<div class="figure">
<div id="intro-to-life-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/Kk2MH9O4pXY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
An introduction to the Game of Life by Alan Zucconi.
</aside>
</section>
<section id="section-88" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions
defined by our covarariance
</aside>
</section>
<section id="section-89" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in
total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-90" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in
Bayesian inference we discard all samples that are distant from the
data.
</aside>
</section>
<section id="section-91" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the
collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-92" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed
analytically using linear algebra.
</aside>
</section>
<section id="section-93" class="slide level2">
<h2></h2>
<p>Time scales, how when you expand or contract time signal becomes
noise and noise becomes signal illustrate with Dirac delta and and
stochastic processes in Fourier space, ito calculus. Latent force
models.</p>
<p>Practical examples of what happens understochasticity:</p>
<ol start="0" type="1">
<li><p>Derive U = W + TS?? Go from microscopic to macroscopic.</p></li>
<li><p>Kappenball — world in between where interesting things
happen,</p></li>
<li><p>Queue efficiency (M/M/1 1/(1-))</p></li>
<li><p>Input to the system being in the form of bias and variance (or
perhaps Brownian motion, wiener process)</p></li>
</ol>
<p>(Latent force models being driven by this???? Latent force as high
frequency information processing? Environment as slow?</p>
<!-- lecture 3 -->
<p>Connect supply chain as a “challenge” tot he abstraction of
Schroedinger’s bridge. Link to Optimal Transport (matching without the
“physics”). Maxwell’s demon.</p>
<p>Control ability paper with Mauricio and Simo??)</p>
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level2">
<h2>What is Machine Learning?</h2>
</section>
<section id="section-94" class="slide level2">
<h2></h2>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively
acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience
(other data! transfer learning etc), or beliefs about the regularities
of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a
categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="section-95" class="slide level2">
<h2></h2>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span
class="math inline">\(f(\cdot)\)</span> includes our beliefs about the
regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span
class="math inline">\(E(\cdot)\)</span> defines the cost of
misprediction.</li>
</ul>
</section>
<section id="section-96" class="slide level2">
<h2></h2>
<ul>
<li>Machine learning is a mainstay because of importance of
prediction.</li>
</ul>
</section>
<section id="section-97" class="slide level2">
<h2></h2>
<ul>
<li>Uncertainty in prediction arises from:</li>
<li>scarcity of training data and</li>
<li>mismatch between the set of prediction functions we choose and all
possible prediction functions.</li>
<li>Also uncertainties in objective, leave those for another day.</li>
</ul>
</section>
<section id="section-98" class="slide level2">
<h2></h2>
<ul>
<li>adaptive non-linear function models inspired by simple neuron models
<span class="citation" data-cites="McCulloch-neuron43">(McCulloch and
Pitts, 1943)</span></li>
<li>have become popular because of their ability to model data.</li>
<li>can be composed to form highly complex functions</li>
<li>start by focussing on one hidden layer</li>
</ul>
</section>
<section id="section-99" class="slide level2">
<h2></h2>
<p><span class="math display">\[
f(\mathbf{ x}) = \left.\mathbf{ w}^{(2)}\right.^\top \boldsymbol{
\phi}(\mathbf{W}_{1}, \mathbf{ x})
\]</span></p>
<p><span class="math inline">\(f(\cdot)\)</span> is a scalar function
with vector inputs,</p>
<p><span class="math inline">\(\boldsymbol{ \phi}(\cdot)\)</span> is a
vector function with vector inputs.</p>
<ul>
<li><p>dimensionality of the vector function is known as the number of
hidden units, or the number of neurons.</p></li>
<li><p>elements of <span class="math inline">\(\boldsymbol{
\phi}(\cdot)\)</span> are the <em>activation</em> function of the neural
network</p></li>
<li><p>elements of <span class="math inline">\(\mathbf{W}_{1}\)</span>
are the parameters of the activation functions.</p></li>
</ul>
</section>
<section id="section-100" class="slide level2">
<h2></h2>
<ul>
<li><p>In statistics activation functions are known as <em>basis
functions</em>.</p></li>
<li><p>would think of this as a <em>linear model</em>: not linear
predictions, linear in the parameters</p></li>
<li><p><span class="math inline">\(\mathbf{ w}_{1}\)</span> are
<em>static</em> parameters.</p></li>
</ul>
</section>
<section id="section-101" class="slide level2">
<h2></h2>
<ul>
<li>In machine learning we optimize <span
class="math inline">\(\mathbf{W}_{1}\)</span> as well as <span
class="math inline">\(\mathbf{W}_{2}\)</span> (which would normally be
denoted in statistics by <span
class="math inline">\(\boldsymbol{\beta}\)</span>).</li>
</ul>
</section>
<section id="section-102" class="slide level2">
<h2></h2>
<ul>
<li><p>Revisit that decision: follow the path of <span class="citation"
data-cites="Neal:bayesian94">Neal (1994)</span> and <span
class="citation" data-cites="MacKay:bayesian92">MacKay
(1992)</span>.</p></li>
<li><p>Consider the probabilistic approach.</p></li>
</ul>
</section>
<section id="section-103" class="slide level2">
<h2></h2>
<ul>
<li>Probabilistically we want, <span class="math display">\[
p(y_*|\mathbf{ y}, \mathbf{X}, \mathbf{ x}_*),
\]</span> <span class="math inline">\(y_*\)</span> is a test output
<span class="math inline">\(\mathbf{ x}_*\)</span> is a test input <span
class="math inline">\(\mathbf{X}\)</span> is a training input matrix
<span class="math inline">\(\mathbf{ y}\)</span> is training
outputs</li>
</ul>
</section>
<section id="section-104" class="slide level2">
<h2></h2>
<p><span class="math display">\[
p(y_*|\mathbf{ y}, \mathbf{X}, \mathbf{ x}_*) = \int p(y_*|\mathbf{
x}_*, \mathbf{W}) p(\mathbf{W}| \mathbf{ y}, \mathbf{X}) \text{d}
\mathbf{W}
\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\mathbf{W}\)</span> contains <span
class="math inline">\(\mathbf{W}_1\)</span> and <span
class="math inline">\(\mathbf{W}_2\)</span></p>
<p><span class="math inline">\(p(\mathbf{W}| \mathbf{ y},
\mathbf{X})\)</span> is posterior density</p>
</div>
</section>
<section id="section-105" class="slide level2">
<h2></h2>
<p><span class="math inline">\(p(y|\mathbf{ x}, \mathbf{W})\)</span> is
the <em>likelihood</em> of data point</p>
<div class="fragment">
<p>Normally assume independence: <span class="math display">\[
p(\mathbf{ y}|\mathbf{X}, \mathbf{W}) = \prod_{i=1}^np(y_i|\mathbf{
x}_i, \mathbf{W}),\]</span></p>
</div>
</section>
<section id="section-106" class="slide level2">
<h2></h2>
<p><span class="math display">\[
p(y_i | f(\mathbf{ x}_i)) = \frac{1}{\sqrt{2\pi \sigma^2}}
\exp\left(-\frac{\left(y_i - f(\mathbf{
x}_i)\right)^2}{2\sigma^2}\right)
\]</span></p>
</section>
<section id="section-107" class="slide level2">
<h2></h2>
<ul>
<li><p>Can also consider priors over latents <span
class="math display">\[
p(\mathbf{ y}_*|\mathbf{ y}) = \int p(\mathbf{ y}_*|\mathbf{X}_*,
\mathbf{W}) p(\mathbf{W}| \mathbf{ y}, \mathbf{X}) p(\mathbf{X})
p(\mathbf{X}_*) \text{d} \mathbf{W}\text{d}
\mathbf{X}\text{d}\mathbf{X}_*
\]</span></p></li>
<li><p>This gives <em>unsupervised learning</em>.</p></li>
</ul>
</section>
<section id="section-108" class="slide level2">
<h2></h2>
<ul>
<li><p>Data: <span class="math inline">\(\mathbf{ y}\)</span></p></li>
<li><p>Model: <span class="math inline">\(p(\mathbf{ y}, \mathbf{
y}^*)\)</span></p></li>
<li><p>Prediction: <span class="math inline">\(p(\mathbf{ y}^*| \mathbf{
y})\)</span></p></li>
</ul>
</section>
<section id="section-109" class="slide level2">
<h2></h2>
<ul>
<li>Represent joint distribution through <em>conditional
dependencies</em>.</li>
<li>E.g. Markov chain</li>
</ul>
<p><span class="math display">\[p(\mathbf{ y}) = p(y_n| y_{n-1})
p(y_{n-1}|y_{n-2}) \dots p(y_{2} | y_{1})\]</span></p>
<div class="figure">
<div id="markov-chain-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ml/markov.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Markov chain is a simple form of probabilistic graphical model
providing a particular decomposition of the joint density.
</aside>
</section>
<section id="section-110" class="slide level2">
<h2></h2>
<p>Predict Perioperative Risk of Clostridium Difficile Infection
Following Colon Surgery <span class="citation"
data-cites="Steele:predictive12">(Steele et al., 2012)</span></p>
<div class="figure">
<div id="c-difficile-bayes-net-diagnosis-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//bayes-net-diagnosis.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A probabilistic directed graph used to predict the perioperative risk of
<em>C Difficile</em> infection following colon surgery. When these
models have good predictive performance they are often difficult to
interpret. This may be due to the limited representation capability of
the conditional densities in the model.
</aside>
</section>
<section id="section-111" class="slide level2">
<h2></h2>
<ul>
<li><p>Easy to write in probabilities</p></li>
<li><p>But underlying this is a wealth of computational
challenges.</p></li>
<li><p>High dimensional integrals typically require
approximation.</p></li>
</ul>
</section>
<section id="section-112" class="slide level2">
<h2></h2>
<ul>
<li><p>In statistics, focussed more on <em>linear</em> model implied by
<span class="math display">\[
f(\mathbf{ x}) = \left.\mathbf{ w}^{(2)}\right.^\top \boldsymbol{
\phi}(\mathbf{W}_1, \mathbf{ x})
\]</span></p></li>
<li><p>Hold <span class="math inline">\(\mathbf{W}_1\)</span> fixed for
given analysis.</p></li>
<li><p>Gaussian prior for <span
class="math inline">\(\mathbf{W}\)</span>, <span class="math display">\[
\mathbf{ w}^{(2)} \sim \mathcal{N}\left(\mathbf{0},\mathbf{C}\right).
\]</span> <span class="math display">\[
y_i = f(\mathbf{ x}_i) + \epsilon_i,
\]</span> where <span class="math display">\[
\epsilon_i \sim \mathcal{N}\left(0,\sigma^2\right)
\]</span></p></li>
</ul>
</section>
<section id="section-113" class="slide level2">
<h2></h2>
<ul>
<li>Normally integrals are complex but for this Gaussian linear case
they are trivial.</li>
</ul>
</section>
<section id="section-114" class="slide level2">
<h2></h2>
</section>
<section id="section-115" class="slide level2">
<h2></h2>
<div class="fragment">
<ol type="1">
<li>Sum of Gaussian variables is also Gaussian.</li>
</ol>
<p><span class="math display">\[y_i \sim
\mathcal{N}\left(\mu_i,\sigma_i^2\right)\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\sum_{i=1}^{n} y_i \sim
\mathcal{N}\left(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2\right)\]</span></p>
</div>
</section>
<section id="section-116" class="slide level2">
<h2></h2>
<ol start="2" type="1">
<li>Scaling a Gaussian leads to a Gaussian.</li>
</ol>
<div class="fragment">
<p><span class="math display">\[y\sim
\mathcal{N}\left(\mu,\sigma^2\right)\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[wy\sim \mathcal{N}\left(w\mu,w^2
\sigma^2\right)\]</span></p>
</div>
</section>
<section id="section-117" class="slide level2">
<h2></h2>
<div style="text-align:left">
If
</div>
<p><span class="math display">\[\mathbf{ x}\sim
\mathcal{N}\left(\boldsymbol{ \mu},\mathbf{C}\right)\]</span></p>
<div class="fragment">
<div style="text-align:left">
And
</div>
<p><span class="math display">\[\mathbf{ y}= \mathbf{W}\mathbf{
x}\]</span></p>
</div>
<div class="fragment">
<div style="text-align:left">
Then
</div>
<p><span class="math display">\[\mathbf{ y}\sim
\mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top\right)\]</span></p>
</div>
</section>
<section id="section-118" class="slide level2">
<h2></h2>
<ol type="1">
<li>linear Gaussian models are easier to deal with</li>
<li>Even the parameters <em>within</em> the process can be handled, by
considering a particular limit.</li>
</ol>
</section>
<section id="section-119" class="slide level2">
<h2></h2>
<ul>
<li><p>If <span class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span></p></li>
<li><p>Assume <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{
\mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim
\mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span></p></li>
<li><p>Then <span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{
\Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic PCA <span
class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop,
1999)</span>.</p></li>
</ul>
</section>
<section id="section-120" class="slide level2">
<h2></h2>
<ul>
<li>Set each activation function computed at each data point to be</li>
</ul>
<p><span class="math display">\[
\phi_{i,j} = \phi(\mathbf{ w}^{(1)}_{j}, \mathbf{ x}_{i})
\]</span> Define <em>design matrix</em> <span class="math display">\[
\boldsymbol{ \Phi}=
\begin{bmatrix}
\phi_{1, 1} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, h} \\
\phi_{1, 2} &amp; \phi_{1, 2} &amp; \dots &amp; \phi_{1, n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\phi_{n, 1} &amp; \phi_{n, 2} &amp; \dots &amp; \phi_{n, h}
\end{bmatrix}.
\]</span></p>
</section>
<section id="section-121" class="slide level2">
<h2></h2>
<p><span class="math display">\[y\left(\mathbf{ x}\right) = \boldsymbol{
\phi}\left(\mathbf{ x}\right)^\top \mathbf{ w}+ \epsilon\]</span></p>
<div class="fragment">
<p><span class="math display">\[\mathbf{ y}= \boldsymbol{ \Phi}\mathbf{
w}+ \boldsymbol{ \epsilon}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\boldsymbol{ \epsilon}\sim
\mathcal{N}\left(\mathbf{0},\sigma^2\mathbf{I}\right)\]</span></p>
</div>
</section>
<section id="section-122" class="slide level2">
<h2></h2>
<ul>
<li><p>If <span class="math display">\[
\mathbf{ y}= \mathbf{W}\mathbf{ x}+ \boldsymbol{ \epsilon},
\]</span></p></li>
<li><p>Assume <span class="math display">\[
\begin{align}
\mathbf{ x}&amp; \sim \mathcal{N}\left(\boldsymbol{
\mu},\mathbf{C}\right)\\
\boldsymbol{ \epsilon}&amp; \sim
\mathcal{N}\left(\mathbf{0},\boldsymbol{ \Sigma}\right)
\end{align}
\]</span></p></li>
<li><p>Then <span class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{W}\boldsymbol{
\mu},\mathbf{W}\mathbf{C}\mathbf{W}^\top + \boldsymbol{ \Sigma}\right).
\]</span> If <span class="math inline">\(\boldsymbol{
\Sigma}=\sigma^2\mathbf{I}\)</span>, this is Probabilistic PCA <span
class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop,
1999)</span>.</p></li>
</ul>
</section>
<section id="section-123" class="slide level2">
<h2></h2>
<ul>
<li>Define <span class="math display">\[
\mathbf{ w}\sim \mathcal{N}\left(\mathbf{0},\alpha\mathbf{I}\right),
\]</span></li>
<li>Rules of multivariate Gaussians to see that, <span
class="math display">\[
\mathbf{ y}\sim \mathcal{N}\left(\mathbf{0},\alpha \boldsymbol{
\Phi}\boldsymbol{ \Phi}^\top + \sigma^2 \mathbf{I}\right).
\]</span></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top + \sigma^2
\mathbf{I}.
\]</span></p>
</section>
<section id="section-124" class="slide level2">
<h2></h2>
<ul>
<li>Elements are a function <span class="math inline">\(k_{i,j} =
k\left(\mathbf{ x}_i, \mathbf{ x}_j\right)\)</span></li>
</ul>
<p><span class="math display">\[
\mathbf{K}= \alpha \boldsymbol{ \Phi}\boldsymbol{ \Phi}^\top + \sigma^2
\mathbf{I}.
\]</span></p>
</section>
<section id="section-125" class="slide level2">
<h2></h2>
<p><span class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)
\]</span></p>
<ul>
<li>formed by inner products of the rows of the <em>design
matrix</em>.</li>
</ul>
</section>
<section id="section-126" class="slide level2">
<h2></h2>
<ul>
<li><p>Instead of making assumptions about our density over each data
point, <span class="math inline">\(y_i\)</span> as i.i.d.</p></li>
<li><p>make a joint Gaussian assumption over our data.</p></li>
<li><p>covariance matrix is now a function of both the parameters of the
activation function, <span class="math inline">\(\mathbf{W}_1\)</span>,
and the input variables, <span
class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li><p>Arises from integrating out <span class="math inline">\(\mathbf{
w}^{(2)}\)</span>.</p></li>
</ul>
</section>
<section id="section-127" class="slide level2">
<h2></h2>
<ul>
<li>Can be very complex, such as deep kernels, <span class="citation"
data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or could even put a
convolutional neural network inside.</li>
<li>Viewing a neural network in this way is also what allows us to
beform sensible <em>batch</em> normalizations <span class="citation"
data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</li>
</ul>
</section>
<section id="section-128" class="slide level2">
<h2></h2>
<ul>
<li>This process is <em>degenerate</em>.</li>
<li>Covariance function is of rank at most <span
class="math inline">\(h\)</span>.</li>
<li>As <span class="math inline">\(n\rightarrow \infty\)</span>,
covariance matrix is not full rank.</li>
<li>Leading to <span class="math inline">\(\det{\mathbf{K}} =
0\)</span></li>
</ul>
</section>
<section id="section-129" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Radford Neal
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/radford-neal.jpg" clip-path="url(#clip0)"/>
</svg>
</div>
<ul>
<li>In ML Radford Neal <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span> asked “what would
happen if you took <span class="math inline">\(h\rightarrow
\infty\)</span>?”</li>
</ul>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Page 37 of <a
href="http://www.cs.toronto.edu/~radford/ftp/thesis.pdf">Radford Neal’s
1994 thesis</a>
</aside>
</section>
<section id="section-130" class="slide level2">
<h2></h2>
<ul>
<li>Instead of <span class="math display">\[
\begin{align*}
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) &amp; = \alpha \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_i\right)^\top \boldsymbol{
\phi}\left(\mathbf{W}_1, \mathbf{ x}_j\right)\\
&amp; = \alpha \sum_k \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{
x}_i\right) \phi\left(\mathbf{ w}^{(1)}_k, \mathbf{ x}_j\right)
\end{align*}
\]</span></li>
<li>Sample infinitely many from a prior density, <span
class="math inline">\(p(\mathbf{ w}^{(1)})\)</span>, <span
class="math display">\[
k_f\left(\mathbf{ x}_i, \mathbf{ x}_j\right) = \alpha \int
\phi\left(\mathbf{ w}^{(1)}, \mathbf{ x}_i\right) \phi\left(\mathbf{
w}^{(1)}, \mathbf{ x}_j\right) p(\mathbf{ w}^{(1)}) \text{d}\mathbf{
w}^{(1)}
\]</span></li>
<li>Also applies for non-Gaussian <span class="math inline">\(p(\mathbf{
w}^{(1)})\)</span> because of the <em>central limit theorem</em>.</li>
</ul>
</section>
<section id="section-131" class="slide level2">
<h2></h2>
<ul>
<li><p>If <span class="math display">\[
\begin{align*}
\mathbf{ w}^{(1)} &amp; \sim p(\cdot)\\ \phi_i &amp; =
\phi\left(\mathbf{ w}^{(1)}, \mathbf{ x}_i\right),
\end{align*}
\]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a
Gaussian process.</p></li>
</ul>
</section>
<section id="section-132" class="slide level2">
<h2></h2>
<ul>
<li><p>Chapter 2 of Neal’s thesis <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>Rest of Neal’s thesis. <span class="citation"
data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>David MacKay’s PhD thesis <span class="citation"
data-cites="MacKay:bayesian92">(MacKay, 1992)</span></p></li>
</ul>
<!-- ### Two Dimensional Gaussian Distribution -->
<!-- include{_ml/includes/two-d-gaussian.md} -->
</section>
<section id="section-133" class="slide level2">
<h2></h2>
</section>
<section id="section-134" class="slide level2">
<h2></h2>
<p><strong>Multi-variate Gaussians</strong></p>
<ul>
<li>We will consider a Gaussian with a particular structure of
covariance matrix.</li>
<li>Generate a single sample from this 25 dimensional Gaussian density,
<span class="math display">\[
\mathbf{ f}=\left[f_{1},f_{2}\dots f_{25}\right].
\]</span></li>
<li>We will plot these points against their index.</li>
</ul>
</section>
<section id="section-135" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'two_point_sample');
</script>
<p><small></small>
<input id="range-two_point_sample" type="range" min="0" max="8" value="0" onchange="setDivs('two_point_sample')" oninput="setDivs('two_point_sample')">
<button onclick="plusDivs(-1, 'two_point_sample')">❮</button>
<button onclick="plusDivs(1, 'two_point_sample')">❯</button></p>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample000.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample001.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample002.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample003.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample004.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample005.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample006.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample007.svg" width style=" ">
</object>
</div>
<div class="two_point_sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample008.svg" width style=" ">
</object>
</div>
</section>
<section id="section-136" class="slide level2">
<h2></h2>
<script>
showDivs(9, 'two-point-sample');
</script>
<p><small></small>
<input id="range-two-point-sample" type="range" min="9" max="12" value="9" onchange="setDivs('two-point-sample')" oninput="setDivs('two-point-sample')">
<button onclick="plusDivs(-1, 'two-point-sample')">❮</button>
<button onclick="plusDivs(1, 'two-point-sample')">❯</button></p>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample000.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample001.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample002.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample003.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample004.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample005.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample006.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample007.svg" width="80%" style=" ">
</object>
</div>
<div class="two-point-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample008.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="section-137" class="slide level2">
<h2></h2>
</section>
<section id="section-138" class="slide level2">
<h2></h2>
<script>
showDivs(9, 'two_point_sample2');
</script>
<p><small></small>
<input id="range-two_point_sample2" type="range" min="9" max="12" value="9" onchange="setDivs('two_point_sample2')" oninput="setDivs('two_point_sample2')">
<button onclick="plusDivs(-1, 'two_point_sample2')">❮</button>
<button onclick="plusDivs(1, 'two_point_sample2')">❯</button></p>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample009.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample010.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample011.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample012.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="section-139" class="slide level2">
<h2></h2>
<div class="figure">
<div id="uluru-as-probability-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/799px-Uluru_Panorama.jpg" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Uluru, the sacred rock in Australia. If we think of it as a probability
density, viewing it from this side gives us one <em>marginal</em> from
the density. Figuratively speaking, slicing through the rock would give
a conditional density.
</aside>
</section>
<section id="section-140" class="slide level2">
<h2></h2>
<ul>
<li>Prediction of <span class="math inline">\(f_2\)</span> from <span
class="math inline">\(f_1\)</span> requires <em>conditional
density</em>.</li>
<li>Conditional density is <em>also</em> Gaussian. <span
class="math display">\[
p(f_2|f_1) = \mathcal{N}\left(f_2|\frac{k_{1, 2}}{k_{1, 1}}f_1, k_{2, 2}
- \frac{k_{1,2}^2}{k_{1,1}}\right)
\]</span> where covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} k_{1, 1} &amp; k_{1, 2}\\ k_{2, 1} &amp;
k_{2, 2}.\end{bmatrix}
\]</span></li>
</ul>
</section>
<section id="section-141" class="slide level2">
<h2></h2>
</section>
<section id="section-142" class="slide level2">
<h2></h2>
<script>
showDivs(13, 'two_point_sample3');
</script>
<p><small></small>
<input id="range-two_point_sample3" type="range" min="13" max="17" value="13" onchange="setDivs('two_point_sample3')" oninput="setDivs('two_point_sample3')">
<button onclick="plusDivs(-1, 'two_point_sample3')">❮</button>
<button onclick="plusDivs(1, 'two_point_sample3')">❯</button></p>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample013.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample014.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample015.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample016.svg" width="80%" style=" ">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/two_point_sample017.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="section-143" class="slide level2">
<h2></h2>
<ul>
<li>The single contour of the Gaussian density represents the
<font color="yellow">joint distribution, <span
class="math inline">\(p(f_1, f_8)\)</span></font></li>
</ul>
<div class="fragment">
<ul>
<li>We observe a value for <font color="magenta"><span
class="math inline">\(f_1=-?\)</span></font></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Conditional density: <font color="cyan"><span
class="math inline">\(p(f_8|f_1=?)\)</span></font>.</li>
</ul>
</div>
</section>
<section id="section-144" class="slide level2">
<h2></h2>
<ul>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{ f},\mathbf{
f}}^{-1}\mathbf{ f},\mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{
f},*}\right)}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="section-145" class="slide level2">
<h2></h2>
<ul>
<li><p>Prediction of <span class="math inline">\(\mathbf{ f}_*\)</span>
from <span class="math inline">\(\mathbf{ f}\)</span> requires
multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian.
<large> <span class="math display">\[
p(\mathbf{ f}_*|\mathbf{ f}) = {\mathcal{N}\left(\mathbf{
f}_*|\boldsymbol{ \mu},\boldsymbol{ \Sigma}\right)}
\]</span> <span class="math display">\[
\boldsymbol{ \mu}= \mathbf{K}_{*,\mathbf{ f}}\mathbf{K}_{\mathbf{
f},\mathbf{ f}}^{-1}\mathbf{ f}
\]</span> <span class="math display">\[
\boldsymbol{ \Sigma}= \mathbf{K}_{*,*}-\mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}_{\mathbf{ f},\mathbf{ f}}^{-1}\mathbf{K}_{\mathbf{ f},*}
\]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span
class="math display">\[
\mathbf{K}= \begin{bmatrix} \mathbf{K}_{\mathbf{ f}, \mathbf{ f}} &amp;
\mathbf{K}_{*, \mathbf{ f}}\\ \mathbf{K}_{\mathbf{ f}, *} &amp;
\mathbf{K}_{*, *}\end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="section-146" class="slide level2">
<h2></h2>
<ul>
<li>Covariance function, <span
class="math inline">\(\mathbf{K}\)</span></li>
<li>Determines properties of samples.</li>
<li>Function of <span class="math inline">\(\mathbf{X}\)</span>, <span
class="math display">\[k_{i,j} = k(\mathbf{ x}_i, \mathbf{
x}_j)\]</span></li>
</ul>
</section>
<section id="section-147" class="slide level2">
<h2></h2>
<ul>
<li><p>Posterior mean <span class="math display">\[f_D(\mathbf{ x}_*) =
\mathbf{ k}(\mathbf{ x}_*, \mathbf{X}) \mathbf{K}^{-1}
\mathbf{ y}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* =
\mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{ f}, *}\]</span></p></li>
</ul>
</section>
<section id="section-148" class="slide level2">
<h2></h2>
<ul>
<li><p>Posterior mean</p>
<p><span class="math display">\[f_D(\mathbf{ x}_*) = \mathbf{
k}(\mathbf{ x}_*, \mathbf{X}) \boldsymbol{\alpha}\]</span></p></li>
<li><p>Posterior covariance <span class="math display">\[\mathbf{C}_* =
\mathbf{K}_{*,*} - \mathbf{K}_{*,\mathbf{ f}}
\mathbf{K}^{-1} \mathbf{K}_{\mathbf{ f}, *}\]</span></p></li>
</ul>
</section>
<section id="section-149" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\exp\left(-\frac{\left\Vert \mathbf{ x}-\mathbf{ x}^\prime
\right\Vert_2^2}{2\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/eq_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The exponentiated quadratic covariance function.
</aside>
</section>
<section id="section-150" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="section-151" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="section-152" class="slide level2">
<h2></h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Alan Turing, in 1946 he was only 11 minutes slower than the winner of
the 1948 games. Would he have won a hypothetical games held in 1946?
Source:
<a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank">Alan
Turing Internet Scrapbook</a>.
</aside>
</section>
<section id="section-153" class="slide level2">
<h2></h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had
been held in 1946?</li>
</ul>
</section>
<section id="section-154" class="slide level2">
<h2></h2>
</section>
<section id="section-155" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/olympic-marathon-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Marathon data. The error bars are
too large, perhaps due to the outlier from 1904.
</aside>
</section>
<section id="section-156" class="slide level2">
<h2></h2>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-157" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}{\det{\mathbf{K}}^{\frac{1}{2}}}}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\]</span></p>
</section>
<section id="section-158" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=\frac{1}{(2\pi)^\frac{n}{2}\color{yellow}{\det{\mathbf{K}}^{\frac{1}{2}}}}\color{cyan}{\exp\left(-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-159" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{aligned}
    \log \mathcal{N}\left(\mathbf{
y}|\mathbf{0},\mathbf{K}\right)=&amp;\color{yellow}{-\frac{1}{2}\log\det{\mathbf{K}}}\color{cyan}{-\frac{\mathbf{
y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}} \\ &amp;-\frac{n}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
E(\boldsymbol{ \theta}) =
\color{yellow}{\frac{1}{2}\log\det{\mathbf{K}}} +
\color{cyan}{\frac{\mathbf{ y}^{\top}\mathbf{K}^{-1}\mathbf{ y}}{2}}
\]</span></p>
</section>
<section id="section-160" class="slide level2">
<h2></h2>
<p>The parameters are <em>inside</em> the covariance function (matrix).
<span class="math display">\[k_{i, j} = k(\mathbf{ x}_i, \mathbf{ x}_j;
\boldsymbol{ \theta})\]</span></p>
</section>
<section id="section-161" class="slide level2">
<h2></h2>
<p><span> <span class="math display">\[\mathbf{K}=
\mathbf{R}\boldsymbol{ \Lambda}^2 \mathbf{R}^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\boldsymbol{ \Lambda}\)</span> represents
distance on axes. <span class="math inline">\(\mathbf{R}\)</span> gives
rotation.
</td>
</tr>
</table>
</section>
<section id="section-162" class="slide level2">
<h2></h2>
<ul>
<li><span class="math inline">\(\boldsymbol{ \Lambda}\)</span> is
<em>diagonal</em>, <span
class="math inline">\(\mathbf{R}^\top\mathbf{R}=
\mathbf{I}\)</span>.</li>
<li>Useful representation since <span
class="math inline">\(\det{\mathbf{K}} = \det{\boldsymbol{ \Lambda}^2} =
\det{\boldsymbol{ \Lambda}}^2\)</span>.</li>
</ul>
</section>
<section id="section-163" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'gp-optimise-determinant');
</script>
<p><small></small>
<input id="range-gp-optimise-determinant" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise-determinant')" oninput="setDivs('gp-optimise-determinant')">
<button onclick="plusDivs(-1, 'gp-optimise-determinant')">❮</button>
<button onclick="plusDivs(1, 'gp-optimise-determinant')">❯</button></p>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant002.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant003.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant004.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant005.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant006.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant007.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant008.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-determinant" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-determinant009.svg" width="80%" style=" ">
</object>
</div>
</section>
<section id="section-164" class="slide level2">
<h2></h2>
</section>
<section id="section-165" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'gp-optimise-quadratic');
</script>
<p><small></small>
<input id="range-gp-optimise-quadratic" type="range" min="0" max="2" value="0" onchange="setDivs('gp-optimise-quadratic')" oninput="setDivs('gp-optimise-quadratic')">
<button onclick="plusDivs(-1, 'gp-optimise-quadratic')">❮</button>
<button onclick="plusDivs(1, 'gp-optimise-quadratic')">❯</button></p>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-quadratic000.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-quadratic001.svg" width="80%" style=" ">
</object>
</div>
<div class="gp-optimise-quadratic" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
<div class="figure">
<div id="gp-optimise-quadratic-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise-quadratic002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The data fit term of the Gaussian process is a quadratic loss centered
around zero. This has eliptical contours, the principal axes of which
are given by the covariance matrix.
</aside>
</section>
<section id="section-166" class="slide level2">
<h2></h2>
</section>
<section id="section-167" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'gp-optimise');
</script>
<p><small></small>
<input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')">
<button onclick="plusDivs(-1, 'gp-optimise')">❮</button>
<button onclick="plusDivs(1, 'gp-optimise')">❯</button></p>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise000.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise001.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise002.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise003.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise004.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise005.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise006.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise007.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise008.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise009.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise010.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise011.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise012.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise013.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise014.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise015.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise016.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise017.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise018.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise019.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise020.svg" width style=" ">
</object>
</td>
<td width="50%">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/gp-optimise021.svg" width style=" ">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="section-168" class="slide level2">
<h2></h2>
<ul>
<li>Given given expression levels in the form of a time series from
<span class="citation" data-cites="DellaGatta:direct08">Della Gatta et
al. (2008)</span>.</li>
</ul>
</section>
<section id="section-169" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gene expression levels over time for a gene from data provided by <span
class="citation" data-cites="DellaGatta:direct08">Della Gatta et al.
(2008)</span>. We would like to understand whether there is signal in
the data, or we are only observing noise.
</aside>
</section>
<section id="section-170" class="slide level2">
<h2></h2>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene
<span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and
Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-171" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Freddie Kalaitzis
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/freddie-kalaitzis.jpg" clip-path="url(#clip1)"/>
</svg>
</div>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The example is taken from the paper “A Simple Approach to Ranking
Differentially Expressed Gene Expression Time Courses through Gaussian
Process Regression.” <span class="citation"
data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.
</aside>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180"
class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-172" class="slide level2">
<h2></h2>
</section>
<section id="section-173" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale
parameter initialized to 50 minutes.
</aside>
</section>
<section id="section-174" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale
parameter initialized to 2000 minutes.
</aside>
</section>
<section id="section-175" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the noise
initialized low (standard deviation 0.1) and the time scale parameter
initialized to 20 minutes.
</aside>
</section>
<section id="section-176" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<!--
## {}



<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima001.svg" width="" style=" "></object>-->
</section>
<section id="section-177" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip2">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Martin Mubangizi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/martin-mubangizi.png" clip-path="url(#clip2)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip3">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Ricardo Andrade Pacecho
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/ricardo-andrade-pacheco.png" clip-path="url(#clip3)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
John Quinn
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/john-quinn.jpg" clip-path="url(#clip4)"/>
</svg>
</div>
<ul>
<li>Work with Ricardo Andrade Pacheco, John Quinn and Martin Mubangizi
(Makerere University, Uganda)</li>
<li>See <a href="http://air.ug/research.html">AI-DEV Group</a>.</li>
<li>See <a href="https://diseaseoutbreaks.unglobalpulse.net/uganda/">UN
Global Pulse Disease Outbreaks Site</a></li>
</ul>
</section>
<section id="section-178" class="slide level2">
<h2></h2>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Ugandan districts. Data SRTM/NASA from <a
href="https://dds.cr.usgs.gov/srtm/version2_1"
class="uri">https://dds.cr.usgs.gov/srtm/version2_1</a>.
</aside>
<div style="text-align:right">
<span class="citation"
data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco
et al., 2014; Mubangizi et al., 2014)</span>
</div>
</section>
<section id="section-179" class="slide level2">
<h2></h2>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Kapchorwa_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kapchorwa District, home district of Stephen Kiprotich.
</aside>
</section>
<section id="section-180" class="slide level2">
<h2></h2>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Tororo_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Tororo district, where the sentinel site, Nagongera, is located.
</aside>
</section>
<section id="section-181" class="slide level2">
<h2></h2>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Sentinel and HMIS data along with rainfall and temperature for the
Nagongera sentinel station in the Tororo district.
</aside>
</section>
<section id="section-182" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Mubende_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Mubende District.
</aside>
</section>
<section id="section-183" class="slide level2">
<h2></h2>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Prediction of malaria incidence in Mubende.
</aside>
</section>
<section id="section-184" class="slide level2">
<h2></h2>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The project arose out of the Gaussian process summer school held at
Makerere in Kampala in 2013. The school led, in turn, to the Data
Science Africa initiative.
</aside>
</section>
<section id="section-185" class="slide level2">
<h2></h2>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//health/Kabarole_District_in_Uganda.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The Kabarole district in Uganda.
</aside>
</section>
<section id="section-186" class="slide level2">
<h2></h2>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Estimate of the current disease situation in the Kabarole district over
time. Estimate is constructed with a Gaussian process with an additive
covariance funciton.
</aside>
</section>
<section id="section-187" class="slide level2">
<h2></h2>
<div class="figure">
<div id="early-warning-system-map-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The map of Ugandan districts with an overview of the Malaria situation
in each district.
</aside>
</section>
<section id="section-188" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k_f(\mathbf{ x}, \mathbf{ x}^\prime) =
k_g(\mathbf{ x}, \mathbf{ x}^\prime) + k_h(\mathbf{ x}, \mathbf{
x}^\prime)\]</span>
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/add_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
An additive covariance function formed by combining a linear and an
exponentiated quadratic covariance functions.
</aside>
</section>
<section id="section-189" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Aki Vehtari
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/aki-vehtari.jpg" clip-path="url(#clip5)"/>
</svg>
</div>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
This is a retrospective analysis of US births by Aki Vehtari. The
challenges of forecasting. Even with seasonal and weekly effects removed
there are significant effects on holidays, weekends, etc.
</aside>
</section>
<section id="section-190" class="slide level2">
<h2></h2>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Two different editions of Bayesian Data Analysis <span class="citation"
data-cites="Gelman:bayesian13">(Gelman et al., 2013)</span>.
</aside>
<div style="text-align:right">
<span class="citation" data-cites="Gelman:bayesian13">Gelman et al.
(2013)</span>
</div>
</section>
<section id="section-191" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\boldsymbol{ \phi}(\mathbf{ x})^\top \boldsymbol{ \phi}(\mathbf{
x}^\prime)\]</span>
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/basis_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
A covariance function based on a non-linear basis given by <span
class="math inline">\(\boldsymbol{ \phi}(\mathbf{ x})\)</span>.
</aside>
</section>
<section id="section-192" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(t, t^\prime)=\alpha \min(t,
t^\prime)\]</span>
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/brownian_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Brownian motion covariance function.
</aside>
</section>
<section id="section-193" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\arcsin\left(\frac{w \mathbf{ x}^\top \mathbf{ x}^\prime +
b}{\sqrt{\left(w \mathbf{ x}^\top \mathbf{ x}+ b + 1\right)\left(w
\left.\mathbf{ x}^\prime\right.^\top \mathbf{ x}^\prime + b +
1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/mlp_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The multi-layer perceptron covariance function. This is derived by
considering the infinite limit of a neural network with probit
activation functions.
</aside>
</section>
<section id="section-194" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha \arcsin\left(\frac{w \mathbf{ x}^\top \mathbf{ x}^\prime + b}
{\sqrt{\left(w \mathbf{ x}^\top \mathbf{ x}+ b + 1\right)
\left(w \left.\mathbf{ x}^\prime\right.^\top \mathbf{ x}^\prime + b +
1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="relu-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/relu_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/relu_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Rectified linear unit covariance function.
</aside>
</section>
<section id="section-195" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) = \alpha
\text{sinc}\left(\pi w r\right)\]</span>
</center>
<div class="figure">
<div id="sinc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/sinc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/sinc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Sinc covariance function.
</aside>
</section>
<section id="section-196" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha(w \mathbf{ x}^\top\mathbf{ x}^\prime + b)^d\]</span>
</center>
<div class="figure">
<div id="polynomial-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/polynomial_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Polynomial covariance function.
</aside>
</section>
<section id="section-197" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(\mathbf{ x}, \mathbf{ x}^\prime) =
\alpha\exp\left(\frac{-2\sin(\pi rw)^2}{\ell^2}\right)\]</span>
</center>
<div class="figure">
<div id="periodic-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/periodic_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/periodic_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Periodic covariance function.
</aside>
</section>
<section id="section-198" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(i, j, \mathbf{ x}, \mathbf{ x}^\prime) =
b_{i,j} k(\mathbf{ x}, \mathbf{ x}^\prime)\]</span>
</center>
<div class="figure">
<div id="lmc-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/lmc_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/lmc_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Linear model of coregionalization covariance function.
</aside>
</section>
<section id="section-199" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[k(i, j, \mathbf{ x}, \mathbf{ x}^\prime) =
b_{i,j} k(\mathbf{ x}, \mathbf{ x}^\prime)\]</span>
</center>
<div class="figure">
<div id="icm-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class data="../slides/diagrams/kern/icm_covariance.svg" width="100%" style=" ">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/icm_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Intrinsic coregionalization model covariance function.
</aside>
</section>
<section id="section-200" class="slide level2">
<h2></h2>
<ul>
<li><em>Deep Gaussian Processes and Variational Propagation of
Uncertainty</em> <span class="citation"
data-cites="Damianou:thesis2015">Damianou (2015)</span></li>
</ul>
</section>
<section id="section-201" class="slide level2">
<h2></h2>
</section>
<section id="section-202" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/mackay-baby.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<p>MacKay: NeurIPS Tutorial 1997 “Have we thrown out the baby with the
bathwater?” <span class="citation"
data-cites="MacKay:gpintroduction98">(Published as MacKay,
n.d.)</span></p>
</section>
<section id="section-203" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn1.svg" width="50%" style=" ">
</object>
</section>
<section id="section-204" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn2.svg" width="50%" style=" ">
</object>
</section>
<section id="section-205" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{
h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{
h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
</section>
<section id="section-206" class="slide level2">
<h2></h2>
<ul>
<li><p>Potential problem: if number of nodes in two adjacent layers is
big, corresponding <span class="math inline">\(\mathbf{W}\)</span> is
also very big and there is the potential to overfit.</p></li>
<li><p>Proposed solution: “dropout”.</p></li>
<li><p>Alternative solution: parameterize <span
class="math inline">\(\mathbf{W}\)</span> with its SVD. <span
class="math display">\[
\mathbf{W}= \mathbf{U}\boldsymbol{ \Lambda}\mathbf{V}^\top
\]</span> or <span class="math display">\[
\mathbf{W}= \mathbf{U}\mathbf{V}^\top
\]</span> where if <span class="math inline">\(\mathbf{W}\in
\Re^{k_1\times k_2}\)</span> then <span
class="math inline">\(\mathbf{U}\in \Re^{k_1\times q}\)</span> and <span
class="math inline">\(\mathbf{V}\in \Re^{k_2\times q}\)</span>, i.e. we
have a low rank matrix factorization for the weights.</p></li>
</ul>
</section>
<section id="section-207" class="slide level2">
<h2></h2>
<div class="figure">
<div id="low-rank-mapping-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//wisuvt.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pictorial representation of the low rank form of the matrix <span
class="math inline">\(\mathbf{W}\)</span>.
</aside>
</section>
<section id="section-208" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck1.svg" width="60%" style=" ">
</object>
</section>
<section id="section-209" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck2.svg" width="60%" style=" ">
</object>
</section>
<section id="section-210" class="slide level2">
<h2></h2>
<p>The network can now be written mathematically as <span
class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{U}_1 \mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \mathbf{ h}_{1}\\
  \mathbf{ h}_{2} &amp;= \phi\left(\mathbf{U}_2 \mathbf{ z}_{2}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \mathbf{ h}_{2}\\
  \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{U}_3 \mathbf{ z}_{3}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4^\top\mathbf{ h}_{3}.
\end{align}
\]</span></p>
</section>
<section id="section-211" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \phi\left(\mathbf{U}_1
\mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \phi\left(\mathbf{U}_2
\mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4 ^\top \mathbf{ z}_{3}
\end{align}
\]</span></p>
</section>
<section id="section-212" class="slide level2">
<h2></h2>
<ul>
<li><p>Replace each neural network with a Gaussian process <span
class="math display">\[
\begin{align}
\mathbf{ z}_{1} &amp;= \mathbf{ f}_1\left(\mathbf{ x}\right)\\
\mathbf{ z}_{2} &amp;= \mathbf{ f}_2\left(\mathbf{ z}_{1}\right)\\
\mathbf{ z}_{3} &amp;= \mathbf{ f}_3\left(\mathbf{ z}_{2}\right)\\
\mathbf{ y}&amp;= \mathbf{ f}_4\left(\mathbf{ z}_{3}\right)
\end{align}
\]</span></p></li>
<li><p>Equivalent to prior over parameters, take width of each layer to
infinity.</p></li>
</ul>
<!-- SECTION Deep Learning -->
</section>
<section id="deep-learning" class="slide level2">
<h2>Deep Learning</h2>
<!-- No slide titles in this context -->
</section>
<section id="section-213" class="slide level2">
<h2></h2>
<p><span class="fragment fade-in"><small>Outline of the DeepFace
architecture. A front-end of a single convolution-pooling-convolution
filtering on the rectified input, followed by three locally-connected
layers and two fully-connected layers. Color illustrates feature maps
produced at each layer. The net includes more than 120 million
parameters, where more than 95% come from the local and fully
connected.</small></span></p>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.
</aside>
<div style="text-align:right">
<small>Source: DeepFace <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small>
</div>
</section>
<section id="section-214" class="slide level2">
<h2></h2>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Deep learning models are composition of simple functions. We can think
of a pinball machine as an analogy. Each layer of pins corresponds to
one of the layers of functions in the model. Input data is represented
by the location of the ball from left to right when it is dropped in
from the top. Output class comes from the position of the ball as it
leaves the pins at the bottom.
</aside>
</section>
<section id="section-215" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
At initialization, the pins, which represent the parameters of the
function, aren’t in the right place to bring the balls to the correct
decisions.
</aside>
</section>
<section id="section-216" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
After learning the pins are now in the right place to bring the balls to
the correct decisions.
</aside>
</section>
<section id="section-217" class="slide level2">
<h2></h2>
<ul>
<li><p>Composite <em>multivariate</em> function</p>
<p><span class="math display">\[
\mathbf{g}(\mathbf{ x})=\mathbf{ f}_5(\mathbf{ f}_4(\mathbf{
f}_3(\mathbf{ f}_2(\mathbf{ f}_1(\mathbf{ x}))))).
\]</span></p></li>
</ul>
</section>
<section id="section-218" class="slide level2">
<h2></h2>
<ul>
<li>Composite <em>multivariate</em> function <span
class="math display">\[
p(\mathbf{ y}|\mathbf{ x})= p(\mathbf{ y}|\mathbf{ f}_5)p(\mathbf{
f}_5|\mathbf{ f}_4)p(\mathbf{ f}_4|\mathbf{ f}_3)p(\mathbf{
f}_3|\mathbf{ f}_2)p(\mathbf{ f}_2|\mathbf{ f}_1)p(\mathbf{
f}_1|\mathbf{ x})
\]</span></li>
</ul>
<div class="figure">
<div id="deep-markov-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Probabilistically the deep Gaussian process can be represented as a
Markov chain. Indeed they can even be analyzed in this way <span
class="citation" data-cites="Dunlop:deep2017">(Dunlop et al.,
n.d.)</span>.
</aside>
</section>
<section id="section-219" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical.svg" width="7%" style=" ">
</object>
</div>
</div>
<aside class="notes">
More usually deep probabilistic models are written vertically rather
than horizontally as in the Markov chain.
</aside>
</section>
<section id="section-220" class="slide level2">
<h2></h2>
<ul>
<li><p>Gaussian processes give priors over functions.</p></li>
<li><p>Elegant properties:</p>
<ul>
<li>e.g. <em>Derivatives</em> of process are also Gaussian distributed
(if they exist).</li>
</ul></li>
<li><p>For particular covariance functions they are ‘universal
approximators’, i.e. all functions can have support under the
prior.</p></li>
<li><p>Gaussian derivatives might ring alarm bells.</p></li>
<li><p>E.g. a priori they don’t believe in function ‘jumps’.</p></li>
</ul>
</section>
<section id="section-221" class="slide level2">
<h2></h2>
<ul>
<li><p>From a process perspective: <em>process
composition</em>.</p></li>
<li><p>A (new?) way of constructing more complex <em>processes</em>
based on simpler components.</p></li>
</ul>
</section>
<section id="section-222" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical.svg" width style=" ">
</object>
</section>
<section id="section-223" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-side-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical-side.svg" width="15%" style=" ">
</object>
</div>
</div>
<aside class="notes">
More generally we aren’t constrained by the Markov chain. We can design
structures that respect our belief about the underlying conditional
dependencies. Here we are adding a side note from the chain.
</aside>
</section>
<section id="section-224" class="slide level2">
<h2></h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear
mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-3d-plot.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A two dimensional grid mapped into three dimensions to form a two
dimensional manifold.
</aside>
</section>
<section id="section-225" class="slide level2">
<h2></h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear
mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-2d-plot.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A one dimensional line mapped into two dimensions by two separate
independent functions. Each point can be mapped exactly through the
mappings.
</aside>
</section>
<section id="section-226" class="slide level2">
<h2></h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear
mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Gaussian density over the input of a non linear function leads to a
very non Gaussian output. Here the output is multimodal.
</aside>
</section>
<section id="section-227" class="slide level2">
<h2></h2>
<ul>
<li>Standard variational bound has the form: <span
class="math display">\[
\mathcal{L}= \left\langle\log p(\mathbf{
y}|\mathbf{Z})\right\rangle_{q(\mathbf{Z})} + \text{KL}\left(
q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
</section>
<section id="section-228" class="slide level2">
<h2></h2>
<ul>
<li>Requires expectation of <span class="math inline">\(\log p(\mathbf{
y}|\mathbf{Z})\)</span> under <span
class="math inline">\(q(\mathbf{Z})\)</span>. <span
class="math display">\[
\begin{align}
\log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{
y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{
f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log
\det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}}
-\frac{n}{2}\log 2\pi
\end{align}
\]</span> <span class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{
f}}\)</span> is dependent on <span
class="math inline">\(\mathbf{Z}\)</span> and it appears in the
inverse.</li>
</ul>
</section>
<section id="section-229" class="slide level2">
<h2></h2>
<ul>
<li>Consider collapsed variational bound, <span class="fragment"
data-fragment-index="1"><small><span class="math display">\[
  p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{
f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{
u}
\]</span></small></span> <span class="fragment"
data-fragment-index="2"><small><span class="math display">\[
  p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int
\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment"
data-fragment-index="3"><small><span class="math display">\[
    \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq
\int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span></li>
</ul>
</section>
<section id="section-230" class="slide level2">
<h2></h2>
<ul>
<li>Apply variational lower bound to the inner integral. <small><span
class="math display">\[
  \begin{align}
  \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp;
\left\langle\sum_{i=1}^n\log  c_i\right\rangle_{q(\mathbf{Z})}\\ &amp;
+\left\langle\log\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)\right\rangle_{q(\mathbf{Z})}\\&amp;
+ \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
  \end{align}
\]</span></small></li>
<li>Which is analytically tractable for Gaussian <span
class="math inline">\(q(\mathbf{Z})\)</span> and some covariance
functions.</li>
</ul>
</section>
<section id="section-231" class="slide level2">
<h2></h2>
<ul>
<li>Need expectations under <span
class="math inline">\(q(\mathbf{Z})\)</span> of: <small><span
class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{
u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i,
\mathbf{ u}}\right]
\]</span></small> and <small><span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{
u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log
2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f},
\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{
u}\right)^2
\]</span></small></li>
</ul>
</section>
<section id="section-232" class="slide level2">
<h2></h2>
<ul>
<li>This requires the expectations <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{
u}}\right\rangle_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{
u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{
f}}\right\rangle_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance
functions <span class="citation"
data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or
through sampling <span class="citation"
data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015;
Salimbeni and Deisenroth, 2017)</span>.</li>
</ul>
</section>
<section id="section-233" class="slide level2">
<h2></h2>
<ul>
<li>MAP approach <span class="citation"
data-cites="Lawrence:hgplvm07">(Lawrence and Moore, 2007)</span>.</li>
<li>Hamiltonian Monte Carlo approach <span class="citation"
data-cites="Havasi:deepgp18">(Havasi et al., 2018)</span>.</li>
<li>Expectation Propagation approach <span class="citation"
data-cites="Bui:deep16">(Bui et al., 2016)</span>.</li>
</ul>
</section>
<section id="section-234" class="slide level2">
<h2></h2>
<div class="figure">
<div id="neural-network-uncertainty-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepgp/neural-network-uncertainty.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Even the latest work on Bayesian neural networks has severe problems
handling uncertainty. In this example, <span class="citation"
data-cites="Izmailov:subspace19">(Izmailov et al., 2019)</span>, methods
even fail to interpolate through the data correctly or provide well
calibrated error bars in regions where data is observed.
</aside>
<div style="text-align:right">
<span class="citation" data-cites="Izmailov:subspace19">Izmailov et al.
(2019)</span>
</div>
</section>
<section id="section-235" class="slide level2">
<h2></h2>
<ul>
<li>Deep architectures allow abstraction of features <span
class="citation"
data-cites="Bengio:deep09 Hinton:fast06 Salakhutdinov:quantitative08">(Bengio,
2009; Hinton and Osindero, 2006; Salakhutdinov and Murray,
n.d.)</span></li>
<li>We use variational approach to stack GP models.</li>
</ul>
</section>
<section id="section-236" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'stack-pca-sample');
</script>
<p><small></small>
<input id="range-stack-pca-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-pca-sample')" oninput="setDivs('stack-pca-sample')">
<button onclick="plusDivs(-1, 'stack-pca-sample')">❮</button>
<button onclick="plusDivs(1, 'stack-pca-sample')">❯</button></p>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-0.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-1.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-2.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-3.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-237" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'stack-gp-sample');
</script>
<p><small></small>
<input id="range-stack-gp-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-gp-sample')" oninput="setDivs('stack-gp-sample')">
<button onclick="plusDivs(-1, 'stack-gp-sample')">❮</button>
<button onclick="plusDivs(1, 'stack-gp-sample')">❯</button></p>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-0.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-1.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-2.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-3.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-238" class="slide level2">
<h2></h2>
<ul>
<li><p><em>Avoiding pathologies in very deep networks</em> <span
class="citation" data-cites="Duvenaud:pathologies14">Duvenaud et al.
(2014)</span> show that the derivative distribution of the process
becomes more <em>heavy tailed</em> as number of layers
increase.</p></li>
<li><p><em>How Deep Are Deep Gaussian Processes?</em> <span
class="citation" data-cites="Dunlop:deep2017">Dunlop et al.
(n.d.)</span> perform a theoretical analysis possible through
conditional Gaussian Markov property.</p></li>
</ul>
</section>
<section id="section-239" class="slide level2">
<h2></h2>
<div class="figure">
<div id="visualization-deep-gp-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/XhIvygQYFFQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Visualization of mapping of a two dimensional space through a deep
Gaussian process.
</aside>
</section>
<section id="section-240" class="slide level2">
<h2></h2>
<ul>
<li><p>Can a Deep Gaussian process help?</p></li>
<li><p>Deep GP is one GP feeding into another.</p></li>
</ul>
</section>
<section id="section-241" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep GP fit to the Olympic marathon data. Error bars now change as the
prediction evolves.
</aside>
</section>
<section id="section-242" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Point samples run through the deep Gaussian process show the
distribution of output locations.
</aside>
</section>
<section id="section-243" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-0.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from input to the latent layer is broadly, with some
flattening as time goes on. Variance is high across the input range.
</aside>
</section>
<section id="section-244" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-1.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from the latent layer to the output layer.
</aside>
</section>
<section id="section-245" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-pinball.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A pinball plot shows the movement of the ‘ball’ as it passes through
each layer of the Gaussian processes. Mean directions of movement are
shown by lines. Shading gives one standard deviation of movement
position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. There is
some grouping of later points towards the right in the first layer,
which also injects a large amount of uncertainty. Due to flattening of
the curve in the second layer towards the right the uncertainty is
reduced in final output.
</aside>
</section>
<section id="section-246" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the Della Gatta gene expression data.
</aside>
</section>
<section id="section-247" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process samples fitted to the Della Gatta gene expression
data.
</aside>
</section>
<section id="section-248" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-0.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process mapping from input to latent layer for the della Gatta
gene expression data.
</aside>
</section>
<section id="section-249" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-1.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process mapping from latent to output layer for the della Gatta
gene expression data.
</aside>
</section>
<section id="section-250" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A pinball plot shows the movement of the ‘ball’ as it passes through
each layer of the Gaussian processes. Mean directions of movement are
shown by lines. Shading gives one standard deviation of movement
position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. Pinball
plot of the della Gatta gene expression data.
</aside>
</section>
<section id="section-251" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/step-function.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Simulation study of step function data artificially generated. Here
there is a small overlap between the two lines.
</aside>
</section>
<section id="section-252" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian
process models in Python. It is designed for teaching and modelling. We
welcome contributions which can be made through the GitHub repository <a
href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="section-253" class="slide level2">
<h2></h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use
GPs.</li>
<li>Available through GitHub <a
href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="section-254" class="slide level2">
<h2></h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the
algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="section-255" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/step-function-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the step function data. Note the large error
bars and the over-smoothing of the discontinuity. Error bars are shown
at two standard deviations.
</aside>
</section>
<section id="section-256" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the step function data.
</aside>
</section>
<section id="section-257" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process model for the step function fit.
</aside>
</section>
<section id="section-258" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-0.svg" width style=" ">
</object>
</section>
<section id="section-259" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-1.svg" width style=" ">
</object>
</section>
<section id="section-260" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-2.svg" width style=" ">
</object>
</section>
<section id="section-261" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-3.svg" width style=" ">
</object>
</section>
<section id="section-262" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pinball plot of the deep GP fitted to the step function data. Each layer
of the model pushes the ‘ball’ towards the left or right, saturating at
1 and 0. This causes the final density to be be peaked at 0 and 1.
Transitions occur driven by the uncertainty of the mapping in each
layer.
</aside>
</section>
<section id="section-263" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helment-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/motorcycle-helmet.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Motorcycle helmet data. The data consists of acceleration readings on a
motorcycle helmet undergoing a collision. The data exhibits
heteroschedastic (time varying) noise levles and non-stationarity.
</aside>
</section>
<section id="section-264" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/motorcycle-helmet-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="section-265" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="section-266" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process as fitted to the motorcycle
helmet accelerometer data.
</aside>
</section>
<section id="section-267" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Mappings from the input to the latent layer for the motorcycle helmet
accelerometer data.
</aside>
</section>
<section id="section-268" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Mappings from the latent layer to the output layer for the motorcycle
helmet accelerometer data.
</aside>
</section>
<section id="section-269" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pinball plot for the mapping from input to output layer for the
motorcycle helmet accelerometer data.
</aside>
</section>
<section id="section-270" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Ground truth movement for the position taken while recording the
multivariate time-course of wireless access point signal strengths.
</aside>
</section>
<section id="section-271" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-data-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-dim-1.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Output dimension 1 from the robot wireless data. This plot shows signal
strength changing over time.
</aside>
</section>
<section id="section-272" class="slide level2">
<h2></h2>
</section>
<section id="section-273" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/robot-wireless-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Robot Wireless dimension 1.
</aside>
</section>
<section id="section-274" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-deep-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of the deep Gaussian process to dimension 1 of the robot wireless
data.
</aside>
</section>
<section id="section-275" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-deep-gp-samples-dim-1-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-samples-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process fit to dimension 1 of the robot
wireless data.
</aside>
</section>
<section id="section-276" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The ground truth movement of the WiFi reception apparatus in the Robot
WiFi experiment
</aside>
</section>
<section id="section-277" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-latent-space-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-latent-space.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Inferred two dimensional latent space from the model for the robot
wireless data.
</aside>
</section>
<section id="section-278" class="slide level2">
<h2></h2>
<ul>
<li>‘High five’ data.</li>
<li>From CMU Mocap Database <span class="citation"
data-cites="CMU-mocap03">(CMU Motion Capture Lab, 2003)</span>.</li>
<li>Model learns structure between two interacting subjects.</li>
</ul>
</section>
<section id="section-279" class="slide level2">
<h2></h2>
<div class="figure">
<div id="shared-latent-variable-model-graph-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//shared.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Shared latent variable model structure. Here two related data sets are
brought together with a set of latent variables that are partially
shared and partially specific to one of the data sets.
</aside>
</section>
<section id="section-280" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-gp-high-five-figure" class="figure-frame">
<p><img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deep-gp-high-five2.png" width="80%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
</div>
</div>
<aside class="notes">
Latent spaces of the ‘high five’ data. The structure of the model is
automatically learnt. One of the latent spaces is coordinating how the
two figures walk together, the other latent spaces contain latent
variables that are specific to each of the figures separately.
</aside>
</section>
<section id="section-281" class="slide level2">
<h2></h2>
</section>
<section id="section-282" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip6">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Zhenwen Dai
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/zhenwen-dai.jpg" clip-path="url(#clip6)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip7">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/andreas-damianou.png" clip-path="url(#clip7)"/>
</svg>
</div>
</section>
<section id="section-283" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-latent-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-latent.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Latent space for the deep Gaussian process learned through unsupervised
learning and fitted to a subset of the MNIST digits subsample.
</aside>
</section>
<section id="section-284" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-1-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-285" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-2-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-286" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-3-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-287" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-4-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-288" class="slide level2">
<h2></h2>
<div class="figure">
<div id="digit-samples-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/digit-samples-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
These digits are produced by taking a tour of the two dimensional latent
space (as described by a Gaussian process sample) and mapping the tour
into the data space. We visualize the mean of the mapping in the images.
</aside>
</section>
<section id="section-289" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-health-model-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deep-health.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The deep health model uses different layers of abstraction in the deep
Gaussian process to represent information about diagnostics and
treatment to model interelationships between a patients different data
modalities.
</aside>
</section>
<section id="section-290" class="slide level2">
<h2></h2>
<ul>
<li>HAMs change how we share ambiguous information.</li>
<li>We need to think about how that effects our sharing of
proabilities.</li>
</ul>
</section>
<section id="section-291" class="slide level2 scrollable">
<h2 class="scrollable"></h2>
<ul>
<li><p>book: <a
href="https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248">The
Atomic Human</a></p></li>
<li><p>twitter: <a
href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></p></li>
<li><p>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></p></li>
<li><p>blog posts:</p>
<p><a
href="http://inverseprobability.com/2016/11/19/lies-damned-lies-big-data">Lies,
Damned Lies and Big Data</a></p>
<p><a
href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What
is Machine Learning?</a></p></li>
</ul>
</section>
<section id="section-292" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable"></h2>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Andrade:consistent14" class="csl-entry" role="listitem">
Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014.
Consistent mapping of government malaria records across a changing
territory delimitation. Malaria Journal 13. <a
href="https://doi.org/10.1186/1475-2875-13-S1-P5">https://doi.org/10.1186/1475-2875-13-S1-P5</a>
</div>
<div id="ref-Bengio:deep09" class="csl-entry" role="listitem">
Bengio, Y., 2009. <span class="nocase">Learning Deep Architectures for
AI</span>. Found. Trends Mach. Learn. 2, 1–127. <a
href="https://doi.org/10.1561/2200000006">https://doi.org/10.1561/2200000006</a>
</div>
<div id="ref-Boltzmann-warmetheorie77" class="csl-entry"
role="listitem">
Boltzmann, L., n.d. Über die <span>B</span>eziehung zwischen dem zweiten
<span>H</span>auptsatze der mechanischen <span>W</span>armetheorie und
der <span>W</span>ahrscheinlichkeitsrechnung, respective den
<span>S</span>ätzen über das wärmegleichgewicht. Sitzungberichte der
Kaiserlichen Akademie der Wissenschaften. Mathematisch-Naturwissen
Classe. Abt. II LXXVI, 373–435.
</div>
<div id="ref-Bui:deep16" class="csl-entry" role="listitem">
Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., Turner, R.,
2016. <a href="http://proceedings.mlr.press/v48/bui16.html">Deep
<span>G</span>aussian processes for regression using approximate
expectation propagation</a>, in: Balcan, M.F., Weinberger, K.Q. (Eds.),
Proceedings of the 33rd International Conference on Machine Learning,
Proceedings of Machine Learning Research. PMLR, New York, New York, USA,
pp. 1472–1481.
</div>
<div id="ref-Cho:deep09" class="csl-entry" role="listitem">
Cho, Y., Saul, L.K., 2009. <a
href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel
methods for deep learning</a>, in: Bengio, Y., Schuurmans, D., Lafferty,
J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural
Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.
</div>
<div id="ref-CMU-mocap03" class="csl-entry" role="listitem">
CMU Motion Capture Lab, 2003. The <span>CMU</span> mocap database.
</div>
<div id="ref-Coales-yellow14" class="csl-entry" role="listitem">
Coales, J.F., Kane, S.J., 2014. The <span>“yellow peril”</span> and
after. IEEE Control Systems Magazine 34, 65–69. <a
href="https://doi.org/10.1109/MCS.2013.2287387">https://doi.org/10.1109/MCS.2013.2287387</a>
</div>
<div id="ref-Damianou:thesis2015" class="csl-entry" role="listitem">
Damianou, A., 2015. Deep <span>G</span>aussian processes and variational
propagation of uncertainty (PhD thesis). University of Sheffield.
</div>
<div id="ref-Damianou:variational15" class="csl-entry" role="listitem">
Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference
for latent variables and uncertain inputs in <span>G</span>aussian
processes. Journal of Machine Learning Research 17.
</div>
<div id="ref-DellaGatta:direct08" class="csl-entry" role="listitem">
Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D.,
Missero, C., Bernardo, D. di, 2008. Direct targets of the TRP63
transcription factor revealed by a combination of gene expression
profiling and reverse engineering. Genome Research 18, 939–948. <a
href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a>
</div>
<div id="ref-Dunlop:deep2017" class="csl-entry" role="listitem">
Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. <a
href="http://jmlr.org/papers/v19/18-015.html">How deep are deep
<span>G</span>aussian processes?</a> Journal of Machine Learning
Research 19, 1–46.
</div>
<div id="ref-Duvenaud:pathologies14" class="csl-entry" role="listitem">
Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding
pathologies in very deep networks.
</div>
<div id="ref-Eddington:nature29" class="csl-entry" role="listitem">
Eddington, A.S., 1929. The nature of the physical world. Dent (London).
<a
href="https://doi.org/10.2307/2180099">https://doi.org/10.2307/2180099</a>
</div>
<div id="ref-Einstein-brownian05" class="csl-entry" role="listitem">
Einstein, A., 1905. Über die von der molekularkinetischen
<span>T</span>heorie der <span>W</span>ärme geforderte
<span>B</span>ewegung von in ruhenden <span>F</span>lüssigkeiten
suspendierten <span>T</span>eilchen. Annalen der Physik 322, 549–560. <a
href="https://doi.org/10.1002/andp.19053220806">https://doi.org/10.1002/andp.19053220806</a>
</div>
<div id="ref-Gelman:bayesian13" class="csl-entry" role="listitem">
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin,
D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.
</div>
<div id="ref-Havasi:deepgp18" class="csl-entry" role="listitem">
Havasi, M., Hernández-Lobato, J.M., Murillo-Fuentes, J.J., 2018. <a
href="http://papers.nips.cc/paper/7979-inference-in-deep-gaussian-processes-using-stochastic-gradient-hamiltonian-monte-carlo.pdf">Inference
in deep <span>G</span>aussian processes using stochastic gradient
<span>H</span>amiltonian <span>M</span>onte <span>C</span>arlo</a>, in:
Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,
Garnett, R. (Eds.), Advances in Neural Information Processing Systems
31. Curran Associates, Inc., pp. 7506–7516.
</div>
<div id="ref-Heider-experimental44" class="csl-entry" role="listitem">
Heider, F., Simmel, M., 1944. An experimental study of apparent
behavior. The American Journal of Psychology 57, 243–259. <a
href="https://doi.org/10.2307/1416950">https://doi.org/10.2307/1416950</a>
</div>
<div id="ref-Hinton:fast06" class="csl-entry" role="listitem">
Hinton, G.E., Osindero, S., 2006. A fast learning algorithm for deep
belief nets. Neural Computation 18, 2006.
</div>
<div id="ref-Huang-inner22" class="csl-entry" role="listitem">
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng,
A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T.,
Brown, N., Luu, L., Levine, S., Hausman, K., ichter, brian, 2023. <a
href="https://proceedings.mlr.press/v205/huang23c.html">Inner monologue:
Embodied reasoning through planning with language models</a>, in: Liu,
K., Kulic, D., Ichnowski, J. (Eds.), Proceedings of the 6th Conference
on Robot Learning, Proceedings of Machine Learning Research. PMLR, pp.
1769–1782.
</div>
<div id="ref-Ioffe:batch15" class="csl-entry" role="listitem">
Ioffe, S., Szegedy, C., 2015. <a
href="http://proceedings.mlr.press/v37/ioffe15.html">Batch
normalization: Accelerating deep network training by reducing internal
covariate shift</a>, in: Bach, F., Blei, D. (Eds.), Proceedings of the
32nd International Conference on Machine Learning, Proceedings of
Machine Learning Research. PMLR, Lille, France, pp. 448–456.
</div>
<div id="ref-Izmailov:subspace19" class="csl-entry" role="listitem">
Izmailov, P., Maddox, W.J., Kirichenko, P., Garipov, T., Vetrov, D.P.,
Wilson, A.G., 2019. <a href="http://arxiv.org/abs/1907.07504">Subspace
inference for bayesian deep learning</a>. CoRR abs/1907.07504.
</div>
<div id="ref-Kalaitzis:simple11" class="csl-entry" role="listitem">
Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking
differentially expressed gene expression time courses through
<span>Gaussian</span> process regression. BMC Bioinformatics 12. <a
href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a>
</div>
<div id="ref-Lawrence-atomic24" class="csl-entry" role="listitem">
Lawrence, N.D., 2024. The atomic human: Understanding ourselves in the
age of AI. Allen Lane.
</div>
<div id="ref-Lawrence:embodiment17" class="csl-entry" role="listitem">
Lawrence, N.D., 2017. <a href="https://arxiv.org/abs/1705.07996">Living
together: Mind and machine intelligence</a>. arXiv.
</div>
<div id="ref-Lawrence:hgplvm07" class="csl-entry" role="listitem">
Lawrence, N.D., Moore, A.J., 2007. Hierarchical <span>G</span>aussian
process latent variable models. pp. 481–488.
</div>
<div id="ref-MacKay:gpintroduction98" class="csl-entry" role="listitem">
MacKay, D.J.C., n.d. Introduction to <span>G</span>aussian processes.
pp. 133–166.
</div>
<div id="ref-MacKay:bayesian92" class="csl-entry" role="listitem">
MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis).
California Institute of Technology.
</div>
<div id="ref-Mackay-behind91" class="csl-entry" role="listitem">
MacKay, D.M., 1991. Behind the eye. Basil Blackwell.
</div>
<div id="ref-McCulloch-neuron43" class="csl-entry" role="listitem">
McCulloch, W.S., Pitts, W., 1943. A logical calculus of the ideas
immanent in nervous activity. Bulletin of Mathematical Biophysics 5,
115–133. <a
href="https://doi.org/10.1007/BF02478259">https://doi.org/10.1007/BF02478259</a>
</div>
<div id="ref-Mikhailov:hydrodynamica05" class="csl-entry"
role="listitem">
Mikhailov, G.K., n.d. Daniel bernoulli, hydrodynamica (1738).
</div>
<div id="ref-Mubangizi:malaria14" class="csl-entry" role="listitem">
Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence,
N.D., 2014. Malaria surveillance with multiple data sources using
<span>Gaussian</span> process models, in: 1st International Conference
on the Use of Mobile <span>ICT</span> in Africa.
</div>
<div id="ref-Neal:bayesian94" class="csl-entry" role="listitem">
Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis).
Dept. of Computer Science, University of Toronto.
</div>
<div id="ref-Salakhutdinov:quantitative08" class="csl-entry"
role="listitem">
Salakhutdinov, R., Murray, I., n.d. On the quantitative analysis of deep
belief networks. pp. 872–879.
</div>
<div id="ref-Salimbeni:doubly2017" class="csl-entry" role="listitem">
Salimbeni, H., Deisenroth, M., 2017. <a
href="http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf">Doubly
stochastic variational inference for deep <span>G</span>aussian
processes</a>, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H.,
Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural
Information Processing Systems 30. Curran Associates, Inc., pp.
4591–4602.
</div>
<div id="ref-Kim-translation15" class="csl-entry" role="listitem">
Sharp, K., Matschinsky, F., 2015. Translation of <span>L</span>udwig
<span>B</span>oltzmann’s paper <span>“on the relationship between the
second fundamental theorem of the mechanical theory of heat and
probability calculations regarding the conditions for thermal
equilibrium.”</span> Entropy 17, 1971–2009. <a
href="https://doi.org/10.3390/e17041971">https://doi.org/10.3390/e17041971</a>
</div>
<div id="ref-Steele:predictive12" class="csl-entry" role="listitem">
Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson,
E., Avital, I., Stojadinovic, A., 2012. Using machine-learned
<span>B</span>ayesian belief networks to predict perioperative risk of
clostridium difficile infection following colon surgery. Interact J Med
Res 1, e6. <a
href="https://doi.org/10.2196/ijmr.2131">https://doi.org/10.2196/ijmr.2131</a>
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Admiralty-gunnery45" class="csl-entry" role="listitem">
The Admiralty, 1945. <a href="https://www.maritime.org/doc/br224/">The
gunnery pocket book, b.r. 224/45</a>.
</div>
<div id="ref-Thompson-juries89" class="csl-entry" role="listitem">
Thompson, W.C., 1989. <a href="http://www.jstor.org/stable/1191906">Are
juries competent to evaluate statistical evidence?</a> Law and
Contemporary Problems 52, 9–41.
</div>
<div id="ref-Tipping:probpca99" class="csl-entry" role="listitem">
Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component
analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a
href="https://doi.org/doi:10.1111/1467-9868.00196">https://doi.org/doi:10.1111/1467-9868.00196</a>
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
