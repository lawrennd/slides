<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2024-03-12">
  <title>The Atomic Human</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="https://inverseprobability.com/assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">The Atomic Human</h1>
  <p class="subtitle" style="text-align:center">Understanding Ourselves
in the Age of AI</p>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil
D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2024-03-12</time></p>
  <p class="venue" style="text-align:center">St Andrews’ Distinguished
Lecture Series</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="ford-model-t-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/1925_Ford_Model_T_touring.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A 1925 Ford Model T built at Henry Ford’s Highland Park Plant in
Dearborn, Michigan. This example now resides in Australia, owned by the
founder of FordModelT.net. From <a
href="https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg"
class="uri">https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg</a>
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="figure">
<div id="diving-bell-and-butterfly-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-diving-bell-and-the-butterfly.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Diving Bell and the Buttefly is the autobiography of Jean Dominique
Bauby.
</aside>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="figure">
<div id="diving-bell-letters-figure" class="figure-frame">
<div style="text-align:center;font-size:200%">
E S A R I N T U L <br> O M D P C F B V <br> H G J Q Z Y X K W
</div>
</div>
</div>
<aside class="notes">
The ordering of the letters that Bauby used for writing his
autobiography.
</aside>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="figure">
<div id="jean-dominique-bauby-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jean-Dominique_Bauby.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Jean Dominique Bauby was the Editor in Chief of the French Elle
Magazine, he suffered a stroke that destroyed his brainstem, leaving him
only capable of moving one eye. Jean Dominique became a victim of locked
in syndrome.
</aside>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="figure">
<div id="bauby-shannon-figure" class="figure-frame">
<table>
<tr>
<td width>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jean-Dominique_Bauby.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ClaudeShannon_MFO3807.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Claude Shannon developed information theory which allows us to quantify
how much Bauby can communicate. This allows us to compare how locked in
he is to us.
</aside>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-atomic-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/atomic-eye.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Atomic Eye, by slicing away aspects of the human that we used to
believe to be unique to us, but are now the preserve of the machine, we
learn something about what it means to be human.
</aside>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="figure">
<div id="colossus-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//computing/Colossus.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A Colossus Mark 2 codebreaking computer being operated by Dorothy Du
Boisson (left) and Elsie Booker (right). Colossus was designed by Tommy
Flowers, but programmed and operated by groups of Wrens based at
Bletchley Park.
</aside>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Embodiment factors are the ratio between our ability to compute and our
ability to communicate. Relative to the machine we are also locked in.
In the table we represent embodiment as the length of time it would take
to communicate one second’s worth of computation. For computers it is a
matter of minutes, but for a human, it is a matter of thousands of
millions of years. See also “Living Together: Mind and Machine
Intelligence” <span class="citation"
data-cites="Lawrence:embodiment17">Lawrence (2017)</span>
</aside>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information001.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information002.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information003.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The trinity of human, data, and computer, and highlights the modern
phenomenon. The communication channel between computer and data now has
an extremely high bandwidth. The channel between human and computer and
the channel between data and human is narrow. New direction of
information flow, information is reaching us mediated by the computer.
The focus on classical statistics reflected the importance of the direct
communication between human and data. The modern challenges of data
science emerge when that relationship is being mediated by the machine.
</aside>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation000.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation001.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation002.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation003.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation004.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation005.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation006.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation007.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="figure">
<div id="cappella-sistina-ceiling-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-sistine-chapel-ceiling.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The ceiling of the Sistine Chapel.
</aside>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-creation-of-man-michelangelo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-the-creation-of-man.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Photo of Detail of Creation of Man from the Sistine chapel ceiling.
</aside>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-creation-of-man-detail-god-michelangelo-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-the-creation-of-man-detail-god.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Photo detail of God.
</aside>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="figure">
<div id="classic-baby-shoes-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Classic_baby_shoes.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
For sale: baby shoes, never worn
</center>
</div>
</div>
<aside class="notes">
Consider the six-word novel, apocryphally credited to Ernest Hemingway,
“For sale: baby shoes, never worn”. To understand what that means to a
human, you need a great deal of additional context. Context that is not
directly accessible to a machine that has not got both the evolved and
contextual understanding of our own condition to realize both the
implication of the advert and what that implication means emotionally to
the previous owner.
</aside>
<aside class="notes">
Ernest Hemingway’s apocryphal six-word novel would mean nothing to a
computer.
</aside>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="figure">
<div id="chicago-cuneiform-stone-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//cuneiform/chicago-cuneiform-stone.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Chicago Stone, side 2, recording sale of a number of fields, probably
from Isin, Early Dynastic Period, c. 2600 BC, black basalt
</aside>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-future-of-professions-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/the-future-of-professions.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="https://www.amazon.co.uk/Future-Professions-Technology-Transform-Experts/dp/0198713398">The
Future of Professions</a> <span class="citation"
data-cites="Susskind-future15">(Susskind and Susskind, 2015)</span> is a
2015 book focussed on how the next wave of technology revolution is
going to effect the professions.
</aside>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<div class="figure">
<div id="coin-pusher-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//policy/Coin_pusher_2.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A coin pusher is a game where coins are dropped into th etop of the
machine, and they disrupt those on the existing steps. With any coin
drop, many coins move, but it is those on the edge, who are often only
indirectly effected, but also most traumatically effected by the change.
</aside>
<!-- AI Fallacy -->
<!-- SECTION The Great AI Fallacy -->
</section>
<section id="the-great-ai-fallacy" class="slide level2">
<h2>The Great AI Fallacy</h2>
<div class="figure">
<div id="jeeves-springtime-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jeeves_in_the_Springtime_01.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
We seem to have fallen for a perspective on AI that suggests it will
adapt to our schedule, rather in the manner of a 1930s manservant.
</aside>
<aside class="notes">
Since the machine learning field was rebranded as AI, done public
understanding, but also more interconnection with other fields.
Struggled to find a consistent definition for AI. But the definitions I
think public uses have one thing in common. The idea that AI will be the
first generation of automation to adapt to us.
</aside>
<!-- Mathematical Statistics -->
</section>
<section id="section-27" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>??</p>
</blockquote>
</section>
<section id="section-28" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
</section>
<section id="section-29" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
</section>
<section id="section-30" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and ‘big data’</p>
<p>Neil Lawrence 1972-?</p>
</blockquote>
</section>
<section id="section-31" class="slide level2">
<h2></h2>
<div class="figure">
<div id="portrait-of-karl-pearson-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Portrait_of_Karl_Pearson.jpg" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Karl Pearson (1857-1936), one of the founders of Mathematical
Statistics.
</aside>
</section>
<section id="section-32" class="slide level2">
<h2></h2>
<div class="figure">
<div id="question-mark-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Question_mark.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
‘Founded’ by ? (?-?)
</aside>
</section>
<section id="section-33" class="slide level2">
<h2></h2>
<div class="figure">
<div id="ml-report-cover-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/ml-report-cover-page.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Royal Society report on Machine Learning was released on 25th April
2017
</aside>
</section>
<section id="section-34" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-cover-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-report-mori-poll-cover.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Royal Society comissioned <a
href="https://royalsociety.org/-/media/policy/projects/machine-learning/publications/public-views-of-machine-learning-ipsos-mori.pdf">public
research from Mori</a> as part of the machine learning review.
</aside>
</section>
<section id="section-35" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-1-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-1.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
One of the questions focussed on machine learning applications.
</aside>
</section>
<section id="section-36" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-2-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-2.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The public were broadly supportive of a range of application areas.
</aside>
</section>
<section id="section-37" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-3-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-3.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
But they failed to see the point in AI’s that could produce poetry.
</aside>
<!-- Fritz Heider -->
</section>
<section id="section-38" class="slide level2">
<h2></h2>
<div class="figure">
<div id="heider-simmel-shapes-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/8FIEZXMUM2I?start=7" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Fritz Heider and Marianne Simmel’s video of shapes from <span
class="citation" data-cites="Heider-experimental44">Heider and Simmel
(1944)</span>.
</aside>
<!-- Conversation LLM -->
</section>
<section id="section-39" class="slide level2">
<h2></h2>
</section>
<section id="section-40" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation000.svg" width="80%" style=" ">
</object>
</section>
<section id="section-41" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation001.svg" width="80%" style=" ">
</object>
</section>
<section id="section-42" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation002.svg" width="80%" style=" ">
</object>
</section>
<section id="section-43" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation003.svg" width="80%" style=" ">
</object>
</section>
<section id="section-44" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation004.svg" width="80%" style=" ">
</object>
</section>
<section id="section-45" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation005.svg" width="80%" style=" ">
</object>
</section>
<section id="section-46" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation006.svg" width="80%" style=" ">
</object>
</section>
<section id="section-47" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation007.svg" width="80%" style=" ">
</object>
<aside class="notes">
This can be disturbing to humans because we are used to a low bandwidth
communication rate.
</aside>
</section>
<section id="section-48" class="slide level2">
<h2></h2>
<div class="figure">
<div id="anne-probability-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-probability-conversation.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.
</aside>
</section>
<section id="section-49" class="slide level2">
<h2></h2>
<ul>
<li><p>There is a lot of evidence that probabilities aren’t
interpretable.</p></li>
<li><p>See e.g. <span class="citation"
data-cites="Thompson-juries89">Thompson (1989)</span></p></li>
</ul>
</section>
<section id="section-50" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-computers-interacting-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-computers-interacting.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Humans and computers interacting should be a major focus of our research
and engineering efforts.
</aside>
</section>
<section id="section-51" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-culture-interacting-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-culture-interacting.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Humans use culture, facts and ‘artefacts’ to communicate.
</aside>
</section>
<section id="section-52" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-blue-marble-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/the-earth-seen-from-apollo-17.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Blue Marble, a photo of Earth taken from Apollo 17.
</aside>
<aside class="notes">
The blue marble image of the earth taken from Apollo 17 which became a
symbol of the environmental movement. December 1972??
</aside>
</section>
<section id="section-53" class="slide level2">
<h2></h2>
<div class="figure">
<div id="eagle-from-columbia-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/2131px-Earth,_Moon_and_Lunar_Module,_AS11-44-6643.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Eagle photographed from Columbia on its return from the Lunar surface.
</aside>
</section>
<section id="section-54" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-little-red-bus-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/the-little-red-bus.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Little Red Bus, Amelia Earhart’s plane in Derry after landing.
</aside>
</section>
<section id="section-55" class="slide level2">
<h2></h2>
<div class="figure">
<div id="naca-lmal-42612-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/NACA-LMAL-42612.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
1945 photo of the NACA test pilots, from left Mel Gough, Herb Hoover,
Jack Reeder, Stefan Cavallo and Bill Gray (photo NASA, NACA LMAL 42612)
</aside>
<aside class="notes">
Bob Gilruth worked on the flying qualities of aircraft.
</aside>
</section>
<section id="section-56" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-moniac-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Bill Phillips and his MONIAC (completed in 1949). The machine is an
analogue computer designed to simulate the workings of the UK economy.
</aside>
</section>
<section id="section-57" class="slide level2">
<h2></h2>
<div class="figure">
<div id="donald-maccrimmon-mackay-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//people/DonaldMacKay1952.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Donald M. MacKay (1922-1987), a physicist who was an early member of the
cybernetics community and member of the Ratio Club.
</aside>
</section>
<section id="section-58" class="slide level2">
<h2></h2>
<div class="figure">
<div id="low-angle-fire-control-team-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/low-angle-fire-control-team.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The fire control computer set at the centre of a system of observation
and tracking <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
</aside>
</section>
<section id="section-59" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-measurement-of-inclination-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-measurement-of-inclination.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Measuring inclination between two ships <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
Sophisticated fire control computers allowed the ship to continue to
fire while under maneuvers.
</aside>
</section>
<section id="section-60" class="slide level2">
<h2></h2>
<div class="figure">
<div id="typical-modern-fire-control-table-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/typical-modern-fire-control-table.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A second world war gun computer’s control table <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
</aside>
</section>
<section id="section-61" class="slide level2">
<h2></h2>
<div class="figure">
<div id="us-navy-training-film-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/gwf5mAlI7Ug?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
U.S. Navy training film MN-6783a. Basic Mechanisms of Fire Control
Computers. Mechanical Computer Instructional Film 27794 (1953) for the
Mk 1A Fire Control Computer.
</aside>
</section>
<section id="section-62" class="slide level2">
<h2></h2>
<div class="figure">
<div id="behind-the-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/behind-the-eye.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="https://www.amazon.co.uk/Behind-Eye-Gifford-Lectures-MACKAY/dp/0631173323">Behind
the Eye</a> <span class="citation" data-cites="Mackay-behind91">(MacKay,
1991)</span> summarises MacKay’s Gifford Lectures, where MacKay uses the
operation of the eye as a window on the operation of the brain.
</aside>
</section>
<section id="section-63" class="slide level2">
<h2></h2>
<blockquote>
<p>Later in the 1940’s, when I was doing my Ph.D. work, there was much
talk of the brain as a computer and of the early digital computers that
were just making the headlines as “electronic brains.” As an analogue
computer man I felt strongly convinced that the brain, whatever it was,
was not a digital computer. I didn’t think it was an analogue computer
either in the conventional sense.</p>
</blockquote>
</section>
<section id="section-64" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-analogue-machine-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/human-analogue-machine.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The human analogue machine creates a feature space which is analagous to
that we use to reason, one way of doing this is to have a machine
attempt to compress all human generated text in an auto-regressive
manner.
</aside>
</section>
<section id="section-65" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-analogue-machine-2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor-ham.svg" width="40%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<ul>
<li><p>A human-analogue machine is a machine that has created a feature
space that is analagous to the “feature space” our brain uses to
reason.</p></li>
<li><p>The latest generation of LLMs are exhibiting this charateristic,
giving them ability to converse.</p></li>
</ul>
</section>
<section id="section-66" class="slide level2">
<h2></h2>
<ul>
<li>Perils of this include <em>counterfeit people</em>.</li>
<li>Daniel Dennett has described the challenges these bring in <a
href="https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/">an
article in The Atlantic</a>.</li>
</ul>
</section>
<section id="section-67" class="slide level2">
<h2></h2>
<ul>
<li><p>But if correctly done, the machine can be appropriately
“psychologically represented”</p></li>
<li><p>This might allow us to deal with the challenge of
<em>intellectual debt</em> where we create machines we cannot
explain.</p></li>
</ul>
</section>
<section id="section-68" class="slide level2">
<h2></h2>
<div class="figure">
<div id="anne-llm-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-llm-conversation.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.
</aside>
</section>
<section id="section-69" class="slide level2">
<h2></h2>
<ul>
<li><p>LLMs are already being used for robot planning <span
class="citation" data-cites="Huang-inner22">Huang et al.
(2023)</span></p></li>
<li><p>Ambiguities are reduced when the machine has had large scale
access to human cultural understanding.</p></li>
</ul>
</section>
<section id="section-70" class="slide level2">
<h2></h2>
<div class="figure">
<div id="ai-for-data-analytics-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/0sJjdxn5kcI?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
The Inner Monologue paper suggests using LLMs for robotic planning <span
class="citation" data-cites="Huang-inner22">(Huang et al., 2023)</span>.
</aside>
</section>
<section id="section-71" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-4-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information004.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The trinity of human, data, and computer, and highlights the modern
phenomenon. The communication channel between computer and data now has
an extremely high bandwidth. The channel between human and computer and
the channel between data and human is narrow. New direction of
information flow, information is reaching us mediated by the computer.
The focus on classical statistics reflected the importance of the direct
communication between human and data. The modern challenges of data
science emerge when that relationship is being mediated by the machine.
</aside>
</section>
<section id="section-72" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-ham-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information-ham.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The HAM now sits between us and the traditional digital computer.
</aside>
<!--include{_ai/includes/p-n-fairness.md}-->
</section>
<section id="section-73" class="slide level2">
<h2></h2>
<div class="figure">
<div id="a-question-of-trust-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/a-question-of-trust.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a href="https://www.bbc.co.uk/programmes/p00gpzfq">A Question of Trust
by Onora O’Neil</a> which examines the nature of trust and its role in
society.
</aside>
</section>
<section id="section-74" class="slide level2">
<h2></h2>
<blockquote>
<p>Again Univesities are to treat each applicant fairly on the basis of
ability and promise, but they are supposed also to admit a socially more
representative intake.</p>
<p>There’s no guarantee that the process meets the target.</p>
<p>Onora O’Neill <em>A Question of Trust: Called to Account</em> Reith
Lectures 2002 <span class="citation" data-cites="ONeill-trust02">O’Neill
(2002)</span>]</p>
</blockquote>
</section>
<section id="section-75" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-0" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-0)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle">policy</text>
</svg>
</center>
</td>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-1" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-1)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle"><tspan x="100" y="90">data</tspan><tspan x="100" y="130">governance</tspan></text>
</svg>
</center>
</td>
</tr>
</table>
<table>
<tr>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-2" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-2)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle"><tspan x="100" y="90">accelerate</tspan><tspan x="100" y="130">science</tspan></text>
</svg>
</center>
</td>
<td width="50%">
<center>
<svg viewBox="0 0 200 200" width="55%">
<defs>
<linearGradient id="gradient-3" x1="0%" y1="0%" x2="100%" y2="0%">
<stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
<stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
</linearGradient> </defs>
<circle cx="100" cy="100" r="100" fill="url(#gradient-3)" />
<text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle">AutoAI</text>
</svg>
</center>
</td>
</tr>
</table>
</section>
<section id="section-76" class="slide level2">
<h2></h2>
<blockquote>
<p>One thing is I can live with is doubt, and uncertainty and not
knowing. I think it’s much more interesting to live with not knowing
than to have an answer that might be wrong.</p>
<p>Richard P. Feynmann in the <em>The Pleasure of Finding Things
Out</em> 1981.</p>
</blockquote>
</section>
<section id="section-77" class="slide level2">
<h2></h2>
<!-- AI Fallacy -->
<!-- SECTION The Structure of Scientific Revolutions -->
</section>
<section id="the-structure-of-scientific-revolutions"
class="slide level2">
<h2>The Structure of Scientific Revolutions</h2>
<div class="figure">
<div id="the-structure-of-scientific-revolutions-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/structure-of-scientific-revolutions.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions">The
Structure of Scientific Revolutions by Thomas S. Kuhn</a> suggests
scientific paradigms are recorded in books.
</aside>
</section>
<section id="section-78" class="slide level2">
<h2></h2>
<div class="figure">
<div id="blake-newton-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/blake-newton.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
William Blake’s <em>Newton</em>.
</aside>
</section>
<section id="section-79" class="slide level2">
<h2></h2>
<div class="figure">
<div id="lunette-rehoboam-abijah-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-lunette-rehoboam-abijah.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Lunette containing Rehoboam and Abijah.
</aside>
</section>
<section id="section-80" class="slide level2">
<h2></h2>
<div class="figure">
<div id="people-culture-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/people-culture.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
People communicate through artifacts and culture.
</aside>
</section>
<section id="section-81" class="slide level2">
<h2></h2>
<div class="figure">
<div id="elohim-creating-adam-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/blake-elohim-creating-adam.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
William Blake’s <em>Elohim Creating Adam</em>.
</aside>
</section>
<section id="section-82" class="slide level2">
<h2></h2>
<div class="figure">
<div id="michelangelo-fall-and-expulsion-from-garden-of-eden-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-fall-and-expulsion-from-the-garden-of-eden.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Photo of detail of the fall and expulsion from the Garden of Eden.
</aside>
</section>
<section id="section-83" class="slide level2">
<h2></h2>
<div class="figure">
<div id="bandwidth-vs-complexity-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/bandwidth-vs-complexity.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Bandwidth vs Complexity.
</aside>
<!-- Conversation LLM -->
<!--






## {}






<div class="figure">
<div class="figure-frame" id="monkey-business-figure">
<iframe width="600" height="450" src="https://www.youtube.com/embed/vJG698U2Mvo?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
</div>
<aside class="notes">Daniel Simon's famous illusion "monkey business". Focus on the movement of the ball distracts the viewer from seeing other aspects of the image.</aside>












-->
<!--include{_data-science/includes/data-selection-attention-bias.md}-->
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
<!-- Lecture 2 -->
</section>
<section id="section-84" class="slide level2">
<h2></h2>
<!--
Time scales, how when you expand or contract time signal becomes noise and noise becomes signal illustrate with Dirac delta and and stochastic processes in Fourier space, ito calculus. Latent force models.

Practical examples of what happens understochasticity:

0) Derive U = W + TS?? Go from microscopic to macroscopic. 

1) Kappenball --- world in between where interesting things happen,

2) Queue efficiency (M/M/1  1/(1-\rho))

3) Input to the system being in the form of bias and variance (or perhaps Brownian motion, wiener process)

(Latent force models being driven by this???? Latent force as high frequency information processing? Environment as slow?-->
</section>
<section id="section-85" class="slide level2">
<h2></h2>
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=1YQPAAAAQAAJ&amp;pg=PR17-IA2"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/1YQPAAAAQAAJ-PR17-IA2.png" /></a>
</div>
<aside class="notes">
This notion is known as <em>Laplace’s demon</em> or <em>Laplace’s
superman</em>.
</aside>
</section>
<section id="section-86" class="slide level2">
<h2></h2>
<div class="figure">
<div id="laplaces-demon-cropped-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/philosophicaless00lapliala_16_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
English translation of Laplace’s demon, taken from the Philosophical
Essay on probabilities <span class="citation"
data-cites="Laplace-essai14">Laplace (1814)</span> pg 3.
</aside>
<blockquote>
<p><em>Philosophical Essay on Probabilities</em> <span class="citation"
data-cites="Laplace-essai14">Laplace (1814)</span> pg 3</p>
</blockquote>
</section>
<section id="section-87" class="slide level2">
<h2></h2>
<center>
<span class="math display">\[
\text{model} + \text{data} \stackrel{\text{compute}}{\rightarrow}
\text{prediction}\]</span>
</center>
</section>
<section id="section-88" class="slide level2">
<h2></h2>
<blockquote>
<p>If we do discover a theory of everything … it would be the ultimate
triumph of human reason-for then we would truly know the mind of God</p>
<p>Stephen Hawking in <em>A Brief History of Time</em> 1988</p>
</blockquote>
</section>
<section id="section-89" class="slide level2">
<h2></h2>
</section>
<section id="section-90" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-rules-loneliness-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>loneliness</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
‘Death’ through loneliness in Conway’s game of life. If a cell is
surrounded by less than three cells, it ‘dies’ through loneliness.
</aside>
</section>
<section id="section-91" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>overcrowding</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
‘Death’ through overpopulation in Conway’s game of life. If a cell is
surrounded by more than three cells, it ‘dies’ through loneliness.
</aside>
</section>
<section id="section-92" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>birth</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Birth in Conway’s life. Any position surrounded by precisely three live
cells will give birth to a new cell at the next turn.
</aside>
</section>
<section id="section-93" class="slide level2">
<h2></h2>
<div class="figure">
<div id="glider-loafer-conway-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Glider (1969)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Glider.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
<em>Left</em> A Glider pattern discovered 1969 by Richard K. Guy.
<em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020). The glider is an oscillator that moves diagonally after
creation. From the simple rules of Life it’s not obvious that such an
object does exist, until you do the necessary computation.
</aside>
</section>
<section id="section-94" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gosper-glider-gun-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Gosperglidergun.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Gosper glider gun is a configuration that creates gliders. A new
glider is released after every 30 turns.
</aside>
</section>
<section id="section-95" class="slide level2">
<h2></h2>
</section>
<section id="section-96" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-loafer-spaceship-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Loafer (2013)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Loafer.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
<em>Left</em> A Loafer pattern discovered by Josh Ball in 2013.
<em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020).
</aside>
</section>
<section id="section-97" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-in-life-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-in-life.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Game of Life running in Life. The video is drawing out recursively
showing pixels that are being formed by filling cells with moving
spaceships. Each individual pixel in this game of life is made up of
<span class="math inline">\(2048 \times 2048\)</span> pixels called an
<a href="https://www.conwaylife.com/wiki/OTCA_metapixel">OTCA
metapixel</a>.
</aside>
</section>
<section id="section-98" class="slide level2">
<h2></h2>
<div class="figure">
<div id="intro-to-life-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/Kk2MH9O4pXY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
An introduction to the Game of Life by Alan Zucconi.
</aside>
</section>
<section id="section-99" class="slide level2">
<h2></h2>
<aside class="notes">
The phenomenon of emergent behaviour also applies to real world
simulations like climate and weather. E.g. Niall Robinson defining a
hurricane to search for hurricane’s im climate simulations.
</aside>
</section>
<section id="section-100" class="slide level2">
<h2></h2>
<div class="figure">
<div id="probability-relative-in-part-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/philosophicaless00lapliala_18_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
To Laplace, determinism is a strawman. Ignorance of mechanism and data
leads to uncertainty which should be dealt with through probability.
</aside>
<blockquote>
<p><em>Philosophical Essay on Probabilities</em> <span class="citation"
data-cites="Laplace-essai14">Laplace (1814)</span> pg 5</p>
</blockquote>
<aside class="notes">
I like to refer to this notion as “Laplace’s Gremlin”, because the lack
of knowledge is the “gremlin of uncertainty” that inhibits the
“deterministic demon”
</aside>
</section>
<section id="section-101" class="slide level2">
<h2></h2>
<div class="figure">
<div id="germlins-think-its-fun-to-hurt-you-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/gremlins-think-its-fun-to-hurt-you.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Gremlins are seen as the cause of a number of challenges in this World
War II poster.
</aside>
</section>
<section id="section-102" class="slide level2">
<h2></h2>
<div class="figure">
<div id="lenox-globe-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="http://www.myoldmaps.com/renaissance-maps-1490-1800/314-the-lenox-globe/314-lenox.pdf">The
Lenox globe</a>, which dates from early 16th century, one of the
earliest known globes.
</aside>
</section>
<section id="section-103" class="slide level2">
<h2></h2>
<div class="figure">
<div id="lenox-globe-by-b-f-da-costa-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe-by-b-f-da-costa.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Drawing of the Lenox Globe by the historian for the Magazine of American
History in September 1879.
</aside>
</section>
<section id="section-104" class="slide level2">
<h2></h2>
<div class="figure">
<div id="lenox-globe-hic-sunt-dracones-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe-hic-sunt-dracones.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Detail from the Lenox globe located in the region of China, “hic sunt
dracones”
</aside>
</section>
<section id="section-105" class="slide level2">
<h2></h2>
<div class="figure">
<div id="met-office-weather-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/1944-06-05_met-office-weather.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Forecast from UK Met Office on 5th June 1944. (detail from <a
href="https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day"
class="uri">https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day</a>)
</aside>
</section>
<section id="section-106" class="slide level2">
<h2></h2>
<div class="figure">
<div id="dwd-weather-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/1944-06-05_dwd-weather.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Forecast from Deutscher Wetterdienst on 5th June 1944. (detail from <a
href="https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day"
class="uri">https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day</a>).
Note the lack of measurements within the UK. THis is the direction that
weather was coming from so the locaiton of weather fronts (and
associated storms) was harder for the Deutscher Wetterdienst to predict
than the Met Office.
</aside>
<!-- thermodynamics -->
</section>
<section id="section-107" class="slide level2">
<h2></h2>
<div class="figure">
<div id="lap-engine-boulton-watt-figure" class="figure-frame">
<table>
<tr>
<td width="60%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/lap-engine.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Lap Engine (1788)</em>
</center>
</td>
<td width="40%">
<center>
total energy <br> = <br> available energy <br> + <br> temperature <br>
<span class="math inline">\(\times\)</span> <br>entropy
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
James Watt’s Lap Engine which incorporates many of his innovations to
the steam engine, making it more efficient.
</aside>
<!--THEORY of IGNORANCE-->
</section>
<section id="section-108" class="slide level2">
<h2></h2>
<div class="figure">
<div id="russell-wiener-russell-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Betrand Russell</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Albert Einstein</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Norbert Wiener</em>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Bertrand Russell (1872-1970), Albert Einstein (1879-1955), Norbert
Wiener, (1894-1964)
</aside>
<aside class="notes">
Wiener came to Cambridge in 1913. Russell showed him Einstein’s 1905
paper on Brownian motion (<span class="citation"
data-cites="Einstein-brownian05">Einstein (1905)</span>)
</aside>
</section>
<section id="section-109" class="slide level2">
<h2></h2>
<div class="figure">
<div id="brownian-motion-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Brownian motion of a large particle in a group of smaller particles. The
movement is known as a <em>Wiener process</em> after Norbert Wiener.
</aside>
</section>
<section id="section-110" class="slide level2">
<h2></h2>
<div class="figure">
<div id="maxwell-boltzmann-gibbs-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>James Clerk Maxwell</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Ludwig Boltzmann</em>
</center>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<em>Josiah Willard Gibbs</em>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906) Josiah
Willard Gibbs (1839-1903)
</aside>
</section>
<section id="section-111" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pooh-rabbit-hoosh-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/E.-H.-Shepard_Two-ink-drawings-from-The-House-at-Pooh-Corner-I_.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Rabbit and Pooh watch the result of Pooh’s hooshing idea to move Eeyore
towards the shore.
</aside>
</section>
<section id="section-112" class="slide level2">
<h2></h2>
<blockquote>
<p>When you are a Bear of Very Little Brain, and you Think of Things,
you find sometimes that a Thing which seemed very Thingish inside you is
quite different when it gets out into the open and has other people
looking at it.</p>
<p>A.A. Milne as Winnie-the-Pooh in <em>The House at Pooh Corner</em>,
1928</p>
</blockquote>
</section>
<section id="section-113" class="slide level2">
<h2></h2>
</section>
<section id="section-114" class="slide level2">
<h2></h2>
<div class="figure">
<div id="hydrodynamica-danielis-bernoulli-figure" class="figure-frame">
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=3yRVAAAAcAAJ&amp;pg=PP7"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/3yRVAAAAcAAJ-PP7.png" /></a>
</div>
</div>
</div>
<aside class="notes">
Daniel Bernoulli’s <em>Hydrodynamica</em> published in 1738. It was one
of the first works to use the idea of conservation of energy. It used
Newton’s laws to predict the behaviour of gases.
</aside>
</section>
<section id="section-115" class="slide level2">
<h2></h2>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=3yRVAAAAcAAJ&amp;pg=PA200"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/3yRVAAAAcAAJ-PA200.png" /></a>
</div>
</div>
</div>
<aside class="notes">
Daniel Bernoulli’s chapter on the kinetic theory of gases, for a review
on the context of this chapter see <span class="citation"
data-cites="Mikhailov:hydrodynamica05">Mikhailov (n.d.)</span>. For 1738
this is extraordinary thinking. The notion of kinetic theory of gases
wouldn’t become fully accepted in Physics until 1908 when a model of
Einstein’s was verified by Jean Baptiste Perrin.
</aside>
</section>
<section id="section-116" class="slide level2">
<h2></h2>
<div class="figure">
<div id="entropy-billiards-js-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="multiball-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left ">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="multiball-entropy">
</output>
</div>
<div id="multiball-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="multiball-newball" style="text-align:right">
New Ball
</button>
<button id="multiball-pause" style="text-align:right">
Pause
</button>
<button id="multiball-skip" style="text-align:right">
Skip 1000s
</button>
<button id="multiball-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/ballworld.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/multiball.js"></script>
</div>
</div>
<aside class="notes">
Bernoulli’s simple kinetic models of gases assume that the molecules of
air operate like billiard balls.
</aside>
</section>
<section id="section-117" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gaussian-histogram-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ml/gaussian-histogram.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
James Clerk Maxwell 1831-1879 Derived distribution of velocities of
particles in an ideal gas (elastic fluid).
</aside>
</section>
<section id="section-118" class="slide level2">
<h2></h2>
<div class="figure">
<div id="maxwell-boltzmann-gibbs-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906) Josiah
Willard Gibbs (1839-1903)
</aside>
</section>
<section id="section-119" class="slide level2">
<h2></h2>
<div class="figure">
<div id="boltzmann-warmetheorie-figure" class="figure-frame">
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=Vuk5AQAAMAAJ&amp;pg=PA373"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/Vuk5AQAAMAAJ-PA373.png" /></a>
</div>
</div>
</div>
<aside class="notes">
Boltzmann’s paper <span class="citation"
data-cites="Boltzmann-warmetheorie77">Boltzmann (n.d.)</span> which
introduced the relationship between entropy and probability. A
translation with notes is available in <span class="citation"
data-cites="Kim-translation15">Sharp and Matschinsky (2015)</span>.
</aside>
</section>
<section id="section-120" class="slide level2">
<h2></h2>
<div class="figure">
<div id="eddington-book-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/arthur-stanley-eddington.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_7.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Eddington’s book on the Nature of the Physical World <span
class="citation" data-cites="Eddington:nature29">(Eddington,
1929)</span>
</aside>
</section>
<section id="section-121" class="slide level2">
<h2></h2>
<div class="figure">
<div id="physical-world-chandra-figure" class="figure-frame">
<table>
<tr>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="49%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/ChandraNobel.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Chandrasekhar (1910-1995) derived the limit at which a star collapses in
on itself. Eddington’s confidence in the 2nd law may have been what
drove him to dismiss Chandrasekhar’s ideas, humiliating a young
scientist who would later receive a Nobel prize for the work.
</aside>
</section>
<section id="section-122" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deepest-humiliation-eddington-cropped-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/natureofphysical00eddi_100_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Eddington makes his feelings about the primacy of the second law clear.
This primacy is perhaps because the second law can be demonstrated
mathematically, building on the work of Maxwell, Gibbs and Boltzmann.
<span class="citation" data-cites="Eddington:nature29">Eddington
(1929)</span>
</aside>
</section>
<section id="section-123" class="slide level2">
<h2></h2>
<div class="figure">
<div id="albert-einstein-photo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Albert Einstein’s 1905 paper on Brownian motion introduced stochastic
differential equations which can be used to model the ‘curve of a simple
molecule of air’.
</aside>
</section>
<section id="section-124" class="slide level2">
<h2></h2>
<div class="figure">
<div id="russell-wiener-russell-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Betrand Russell</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="85%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Albert Einstein</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Norbert Wiener</em>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Bertrand Russell (1872-1970), Albert Einstein (1879-1955), Norbert
Wiener, (1894-1964)
</aside>
<aside class="notes">
Wiener came to Cambridge in 1913. Russell showed him Einstein’s 1905
paper on Brownian motion (<span class="citation"
data-cites="Einstein-brownian05">Einstein (1905)</span>)
</aside>
</section>
<section id="section-125" class="slide level2">
<h2></h2>
<div class="figure">
<div id="brownian-motion-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Brownian motion of a large particle in a group of smaller particles. The
movement is known as a <em>Wiener process</em> after Norbert Wiener.
</aside>
</section>
<section id="section-126" class="slide level2">
<h2></h2>
<div class="figure">
<div id="norbert-wiener-yellow-peril-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/wiener-yellow-peril.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Norbert Wiener (1894 - 1964). Founder of cybernetics and the information
era. He used Gibbs’s ideas to develop a “theory of ignorance” that he
deployed in early communication. On the right is Wiener’s wartime report
that used stochastic processes in forecasting with applications in radar
control (image from <span class="citation"
data-cites="Coales-yellow14">Coales and Kane (2014)</span>).
</aside>
</section>
<section id="section-127" class="slide level2">
<h2></h2>
<div class="figure">
<div id="kappen-ball-figure" class="figure-frame">
<div>
<div style="width:900px;text-align:center;display:inline">
<span style="float:left;">Score:
<output id="kappenball-score">
</output>
</span> <span style="float:right;">Energy:
<output id="kappenball-energy">
</output>
</span>
<div style="clear: both;">

</div>
</div>
<canvas id="kappenball-canvas" width="900" height="500" style="border:1px solid black;display:inline;text-align:center ">
</canvas>
<div>
<input type="range" min="0" max="100" value="0" class="slider" id="kappenball-stochasticity" style="width:900px;"/>
</div>
<div>
<button id="kappenball-newball" style="text-align:right">
New Ball
</button>
<button id="kappenball-pause" style="text-align:right">
Pause
</button>
</div>
<output id="kappenball-count">
</output>
<script src="https://inverseprobability.com/talks/scripts//ballworld/kappenball.js"></script>
</div>
</div>
</div>
<aside class="notes">
Kappen Ball
</aside>
</section>
<section id="section-128" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions
defined by our covarariance
</aside>
</section>
<section id="section-129" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in
total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-130" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in
Bayesian inference we discard all samples that are distant from the
data.
</aside>
</section>
<section id="section-131" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the
collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-132" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed
analytically using linear algebra.
</aside>
</section>
<section id="section-133" class="slide level2">
<h2></h2>
<div class="figure">
<div id="maxwells-demon-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//physics/maxwells-demon.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Maxwell’s demon opens and closes a door which allows fast particles to
pass from left to right and slow particles to pass from right to left.
This makes the left hand side colder than the right.
</aside>
</section>
<section id="section-134" class="slide level2">
<h2></h2>
<div class="figure">
<div id="maxwells-demon-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="maxwell-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="maxwell-entropy">
</output>
</div>
<div id="maxwell-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="maxwell-newball" style="text-align:right">
New Ball
</button>
<button id="maxwell-pause" style="text-align:right">
Pause
</button>
<button id="maxwell-skip" style="text-align:right">
Skip 1000s
</button>
<button id="maxwell-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://inverseprobability.com/talks/scripts//ballworld/maxwell.js"></script>
</div>
</div>
<aside class="notes">
Maxwell’s Demon. The demon decides balls are either cold (blue) or hot
(red) according to their velocity. Balls are allowed to pass the green
membrane from right to left only if they are cold, and from left to
right, only if they are hot.
</aside>
</section>
<section id="section-135" class="slide level2">
<h2></h2>
<ul>
<li>HAMs change how we share ambiguous information.</li>
<li>We need to think about how that effects our sharing of
proabilities.</li>
</ul>
<!-- lecture 3 -->
</section>
<section id="section-136" class="slide level2">
<h2></h2>
<center>
A chance for us to acknowledge our ignorance and to rediscover
interdisplinary science.
</center>
</section>
<section id="section-137" class="slide level2">
<h2></h2>
<p><span class="fragment fade-in"><small>Outline of the DeepFace
architecture. A front-end of a single convolution-pooling-convolution
filtering on the rectified input, followed by three locally-connected
layers and two fully-connected layers. Color illustrates feature maps
produced at each layer. The net includes more than 120 million
parameters, where more than 95% come from the local and fully
connected.</small></span></p>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.
</aside>
<div style="text-align:right">
<small>Source: DeepFace <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small>
</div>
</section>
<section id="section-138" class="slide level2">
<h2></h2>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Deep learning models are composition of simple functions. We can think
of a pinball machine as an analogy. Each layer of pins corresponds to
one of the layers of functions in the model. Input data is represented
by the location of the ball from left to right when it is dropped in
from the top. Output class comes from the position of the ball as it
leaves the pins at the bottom.
</aside>
</section>
<section id="section-139" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
At initialization, the pins, which represent the parameters of the
function, aren’t in the right place to bring the balls to the correct
decisions.
</aside>
</section>
<section id="section-140" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
After learning the pins are now in the right place to bring the balls to
the correct decisions.
</aside>
<!--Connect supply chain as a "challenge" tot he abstraction of Schroedinger's bridge. Link to Optimal Transport (matching without the "physics"). Maxwell's demon.-->
<!--Control ability paper with Mauricio and Simo??)-->
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
<!--include{_gp/includes/what-is-a-gp.md} -->
</section>
<section id="section-141" class="slide level2">
<h2></h2>
<ul>
<li><em>Deep Gaussian Processes and Variational Propagation of
Uncertainty</em> <span class="citation"
data-cites="Damianou:thesis2015">Damianou (2015)</span></li>
</ul>
</section>
<section id="section-142" class="slide level2">
<h2></h2>
</section>
<section id="section-143" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/mackay-baby.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<p>MacKay: NeurIPS Tutorial 1997 “Have we thrown out the baby with the
bathwater?” <span class="citation"
data-cites="MacKay:gpintroduction98">(Published as MacKay,
n.d.)</span></p>
</section>
<section id="section-144" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn1.svg" width="50%" style=" ">
</object>
</section>
<section id="section-145" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn2.svg" width="50%" style=" ">
</object>
</section>
<section id="section-146" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{
h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{
h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
</section>
<section id="section-147" class="slide level2">
<h2></h2>
<ul>
<li><p>Potential problem: if number of nodes in two adjacent layers is
big, corresponding <span class="math inline">\(\mathbf{W}\)</span> is
also very big and there is the potential to overfit.</p></li>
<li><p>Proposed solution: “dropout”.</p></li>
<li><p>Alternative solution: parameterize <span
class="math inline">\(\mathbf{W}\)</span> with its SVD. <span
class="math display">\[
\mathbf{W}= \mathbf{U}\boldsymbol{ \Lambda}\mathbf{V}^\top
\]</span> or <span class="math display">\[
\mathbf{W}= \mathbf{U}\mathbf{V}^\top
\]</span> where if <span class="math inline">\(\mathbf{W}\in
\Re^{k_1\times k_2}\)</span> then <span
class="math inline">\(\mathbf{U}\in \Re^{k_1\times q}\)</span> and <span
class="math inline">\(\mathbf{V}\in \Re^{k_2\times q}\)</span>, i.e. we
have a low rank matrix factorization for the weights.</p></li>
</ul>
</section>
<section id="section-148" class="slide level2">
<h2></h2>
<div class="figure">
<div id="low-rank-mapping-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//wisuvt.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pictorial representation of the low rank form of the matrix <span
class="math inline">\(\mathbf{W}\)</span>.
</aside>
</section>
<section id="section-149" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck1.svg" width="60%" style=" ">
</object>
</section>
<section id="section-150" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck2.svg" width="60%" style=" ">
</object>
</section>
<section id="section-151" class="slide level2">
<h2></h2>
<p>The network can now be written mathematically as <span
class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{U}_1 \mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \mathbf{ h}_{1}\\
  \mathbf{ h}_{2} &amp;= \phi\left(\mathbf{U}_2 \mathbf{ z}_{2}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \mathbf{ h}_{2}\\
  \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{U}_3 \mathbf{ z}_{3}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4^\top\mathbf{ h}_{3}.
\end{align}
\]</span></p>
</section>
<section id="section-152" class="slide level2">
<h2></h2>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \phi\left(\mathbf{U}_1
\mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \phi\left(\mathbf{U}_2
\mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4 ^\top \mathbf{ z}_{3}
\end{align}
\]</span></p>
</section>
<section id="section-153" class="slide level2">
<h2></h2>
<ul>
<li><p>Replace each neural network with a Gaussian process <span
class="math display">\[
\begin{align}
\mathbf{ z}_{1} &amp;= \mathbf{ f}_1\left(\mathbf{ x}\right)\\
\mathbf{ z}_{2} &amp;= \mathbf{ f}_2\left(\mathbf{ z}_{1}\right)\\
\mathbf{ z}_{3} &amp;= \mathbf{ f}_3\left(\mathbf{ z}_{2}\right)\\
\mathbf{ y}&amp;= \mathbf{ f}_4\left(\mathbf{ z}_{3}\right)
\end{align}
\]</span></p></li>
<li><p>Equivalent to prior over parameters, take width of each layer to
infinity.</p></li>
</ul>
<!-- SECTION Deep Learning -->
</section>
<section id="deep-learning" class="slide level2">
<h2>Deep Learning</h2>
<!-- No slide titles in this context -->
</section>
<section id="section-154" class="slide level2">
<h2></h2>
<ul>
<li><p>Composite <em>multivariate</em> function</p>
<p><span class="math display">\[
\mathbf{g}(\mathbf{ x})=\mathbf{ f}_5(\mathbf{ f}_4(\mathbf{
f}_3(\mathbf{ f}_2(\mathbf{ f}_1(\mathbf{ x}))))).
\]</span></p></li>
</ul>
</section>
<section id="section-155" class="slide level2">
<h2></h2>
<ul>
<li>Composite <em>multivariate</em> function <span
class="math display">\[
p(\mathbf{ y}|\mathbf{ x})= p(\mathbf{ y}|\mathbf{ f}_5)p(\mathbf{
f}_5|\mathbf{ f}_4)p(\mathbf{ f}_4|\mathbf{ f}_3)p(\mathbf{
f}_3|\mathbf{ f}_2)p(\mathbf{ f}_2|\mathbf{ f}_1)p(\mathbf{
f}_1|\mathbf{ x})
\]</span></li>
</ul>
<div class="figure">
<div id="deep-markov-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Probabilistically the deep Gaussian process can be represented as a
Markov chain. Indeed they can even be analyzed in this way <span
class="citation" data-cites="Dunlop:deep2017">(Dunlop et al.,
n.d.)</span>.
</aside>
</section>
<section id="section-156" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical.svg" width="7%" style=" ">
</object>
</div>
</div>
<aside class="notes">
More usually deep probabilistic models are written vertically rather
than horizontally as in the Markov chain.
</aside>
</section>
<section id="section-157" class="slide level2">
<h2></h2>
<ul>
<li><p>Gaussian processes give priors over functions.</p></li>
<li><p>Elegant properties:</p>
<ul>
<li>e.g. <em>Derivatives</em> of process are also Gaussian distributed
(if they exist).</li>
</ul></li>
<li><p>For particular covariance functions they are ‘universal
approximators’, i.e. all functions can have support under the
prior.</p></li>
<li><p>Gaussian derivatives might ring alarm bells.</p></li>
<li><p>E.g. a priori they don’t believe in function ‘jumps’.</p></li>
</ul>
</section>
<section id="section-158" class="slide level2">
<h2></h2>
<ul>
<li><p>From a process perspective: <em>process
composition</em>.</p></li>
<li><p>A (new?) way of constructing more complex <em>processes</em>
based on simpler components.</p></li>
</ul>
</section>
<section id="section-159" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical.svg" width style=" ">
</object>
</section>
<section id="section-160" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-markov-vertical-side-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-markov-vertical-side.svg" width="15%" style=" ">
</object>
</div>
</div>
<aside class="notes">
More generally we aren’t constrained by the Markov chain. We can design
structures that respect our belief about the underlying conditional
dependencies. Here we are adding a side note from the chain.
</aside>
</section>
<section id="section-161" class="slide level2">
<h2></h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear
mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="nonlinear-mapping-3d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-3d-plot.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A two dimensional grid mapped into three dimensions to form a two
dimensional manifold.
</aside>
</section>
<section id="section-162" class="slide level2">
<h2></h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear
mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="non-linear-mapping-2d-plot-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/nonlinear-mapping-2d-plot.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A one dimensional line mapped into two dimensions by two separate
independent functions. Each point can be mapped exactly through the
mappings.
</aside>
</section>
<section id="section-163" class="slide level2">
<h2></h2>
<ul>
<li><p>Propagate a probability distribution through a non-linear
mapping.</p></li>
<li><p>Normalisation of distribution becomes intractable.</p></li>
</ul>
<div class="figure">
<div id="gaussian-through-nonlinear-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//dimred/gaussian-through-nonlinear.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A Gaussian density over the input of a non linear function leads to a
very non Gaussian output. Here the output is multimodal.
</aside>
</section>
<section id="section-164" class="slide level2">
<h2></h2>
<ul>
<li>Standard variational bound has the form: <span
class="math display">\[
\mathcal{L}= \left\langle\log p(\mathbf{
y}|\mathbf{Z})\right\rangle_{q(\mathbf{Z})} + \text{KL}\left(
q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)
\]</span></li>
</ul>
</section>
<section id="section-165" class="slide level2">
<h2></h2>
<ul>
<li>Requires expectation of <span class="math inline">\(\log p(\mathbf{
y}|\mathbf{Z})\)</span> under <span
class="math inline">\(q(\mathbf{Z})\)</span>. <span
class="math display">\[
\begin{align}
\log p(\mathbf{ y}|\mathbf{Z}) = &amp; -\frac{1}{2}\mathbf{
y}^\top\left(\mathbf{K}_{\mathbf{ f}, \mathbf{
f}}+\sigma^2\mathbf{I}\right)^{-1}\mathbf{ y}\\ &amp; -\frac{1}{2}\log
\det{\mathbf{K}_{\mathbf{ f}, \mathbf{ f}}+\sigma^2 \mathbf{I}}
-\frac{n}{2}\log 2\pi
\end{align}
\]</span> <span class="math inline">\(\mathbf{K}_{\mathbf{ f}, \mathbf{
f}}\)</span> is dependent on <span
class="math inline">\(\mathbf{Z}\)</span> and it appears in the
inverse.</li>
</ul>
</section>
<section id="section-166" class="slide level2">
<h2></h2>
<ul>
<li>Consider collapsed variational bound, <span class="fragment"
data-fragment-index="1"><small><span class="math display">\[
  p(\mathbf{ y})\geq \prod_{i=1}^nc_i \int \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{
f}\right\rangle,\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{
u}
\]</span></small></span> <span class="fragment"
data-fragment-index="2"><small><span class="math display">\[
  p(\mathbf{ y}|\mathbf{Z})\geq \prod_{i=1}^nc_i \int
\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span> <span class="fragment"
data-fragment-index="3"><small><span class="math display">\[
    \int p(\mathbf{ y}|\mathbf{Z})p(\mathbf{Z}) \text{d}\mathbf{Z}\geq
\int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}p(\mathbf{ u}) \text{d}\mathbf{ u}
\]</span></small></span></li>
</ul>
</section>
<section id="section-167" class="slide level2">
<h2></h2>
<ul>
<li>Apply variational lower bound to the inner integral. <small><span
class="math display">\[
  \begin{align}
  \int \prod_{i=1}^nc_i \mathcal{N}\left(\mathbf{
y}|\left\langle\mathbf{ f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)
p(\mathbf{Z})\text{d}\mathbf{Z}\geq &amp;
\left\langle\sum_{i=1}^n\log  c_i\right\rangle_{q(\mathbf{Z})}\\ &amp;
+\left\langle\log\mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{ u},
\mathbf{Z})},\sigma^2\mathbf{I}\right)\right\rangle_{q(\mathbf{Z})}\\&amp;
+ \text{KL}\left( q(\mathbf{Z})\,\|\,p(\mathbf{Z}) \right)    
  \end{align}
\]</span></small></li>
<li>Which is analytically tractable for Gaussian <span
class="math inline">\(q(\mathbf{Z})\)</span> and some covariance
functions.</li>
</ul>
</section>
<section id="section-168" class="slide level2">
<h2></h2>
<ul>
<li>Need expectations under <span
class="math inline">\(q(\mathbf{Z})\)</span> of: <small><span
class="math display">\[
\log c_i = \frac{1}{2\sigma^2} \left[k_{i, i} - \mathbf{ k}_{i, \mathbf{
u}}^\top \mathbf{K}_{\mathbf{ u}, \mathbf{ u}}^{-1} \mathbf{ k}_{i,
\mathbf{ u}}\right]
\]</span></small> and <small><span class="math display">\[
\log \mathcal{N}\left(\mathbf{ y}|\left\langle\mathbf{
f}\right\rangle_{p(\mathbf{ f}|\mathbf{
u},\mathbf{Y})},\sigma^2\mathbf{I}\right) = -\frac{1}{2}\log
2\pi\sigma^2 - \frac{1}{2\sigma^2}\left(y_i - \mathbf{K}_{\mathbf{ f},
\mathbf{ u}}\mathbf{K}_{\mathbf{ u},\mathbf{ u}}^{-1}\mathbf{
u}\right)^2
\]</span></small></li>
</ul>
</section>
<section id="section-169" class="slide level2">
<h2></h2>
<ul>
<li>This requires the expectations <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{
u}}\right\rangle_{q(\mathbf{Z})}
\]</span> and <span class="math display">\[
\left\langle\mathbf{K}_{\mathbf{ f},\mathbf{ u}}\mathbf{K}_{\mathbf{
u},\mathbf{ u}}^{-1}\mathbf{K}_{\mathbf{ u},\mathbf{
f}}\right\rangle_{q(\mathbf{Z})}
\]</span> which can be computed analytically for some covariance
functions <span class="citation"
data-cites="Damianou:variational15">(Damianou et al., 2016)</span> or
through sampling <span class="citation"
data-cites="Damianou:thesis2015 Salimbeni:doubly2017">(Damianou, 2015;
Salimbeni and Deisenroth, 2017)</span>.</li>
</ul>
</section>
<section id="section-170" class="slide level2">
<h2></h2>
<ul>
<li>MAP approach <span class="citation"
data-cites="Lawrence:hgplvm07">(Lawrence and Moore, 2007)</span>.</li>
<li>Hamiltonian Monte Carlo approach <span class="citation"
data-cites="Havasi:deepgp18">(Havasi et al., 2018)</span>.</li>
<li>Expectation Propagation approach <span class="citation"
data-cites="Bui:deep16">(Bui et al., 2016)</span>.</li>
</ul>
</section>
<section id="section-171" class="slide level2">
<h2></h2>
<div class="figure">
<div id="neural-network-uncertainty-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepgp/neural-network-uncertainty.png" width="90%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Even the latest work on Bayesian neural networks has severe problems
handling uncertainty. In this example, <span class="citation"
data-cites="Izmailov:subspace19">(Izmailov et al., 2019)</span>, methods
even fail to interpolate through the data correctly or provide well
calibrated error bars in regions where data is observed.
</aside>
<div style="text-align:right">
<span class="citation" data-cites="Izmailov:subspace19">Izmailov et al.
(2019)</span>
</div>
</section>
<section id="section-172" class="slide level2">
<h2></h2>
<ul>
<li>Deep architectures allow abstraction of features <span
class="citation"
data-cites="Bengio:deep09 Hinton:fast06 Salakhutdinov:quantitative08">(Bengio,
2009; Hinton and Osindero, 2006; Salakhutdinov and Murray,
n.d.)</span></li>
<li>We use variational approach to stack GP models.</li>
</ul>
</section>
<section id="section-173" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'stack-pca-sample');
</script>
<p><small></small>
<input id="range-stack-pca-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-pca-sample')" oninput="setDivs('stack-pca-sample')">
<button onclick="plusDivs(-1, 'stack-pca-sample')">❮</button>
<button onclick="plusDivs(1, 'stack-pca-sample')">❯</button></p>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-0.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-1.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-2.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-3.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-pca-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-pca-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-174" class="slide level2">
<h2></h2>
<script>
showDivs(0, 'stack-gp-sample');
</script>
<p><small></small>
<input id="range-stack-gp-sample" type="range" min="0" max="4" value="0" onchange="setDivs('stack-gp-sample')" oninput="setDivs('stack-gp-sample')">
<button onclick="plusDivs(-1, 'stack-gp-sample')">❮</button>
<button onclick="plusDivs(1, 'stack-gp-sample')">❯</button></p>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-0.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-1.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-2.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-3.svg" width="10%" style=" ">
</object>
</div>
<div class="stack-gp-sample" style="text-align:center;">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//stack-gp-sample-4.svg" width="10%" style=" ">
</object>
</div>
</section>
<section id="section-175" class="slide level2">
<h2></h2>
<ul>
<li><p><em>Avoiding pathologies in very deep networks</em> <span
class="citation" data-cites="Duvenaud:pathologies14">Duvenaud et al.
(2014)</span> show that the derivative distribution of the process
becomes more <em>heavy tailed</em> as number of layers
increase.</p></li>
<li><p><em>How Deep Are Deep Gaussian Processes?</em> <span
class="citation" data-cites="Dunlop:deep2017">Dunlop et al.
(n.d.)</span> perform a theoretical analysis possible through
conditional Gaussian Markov property.</p></li>
</ul>
</section>
<section id="section-176" class="slide level2">
<h2></h2>
<div class="figure">
<div id="visualization-deep-gp-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/XhIvygQYFFQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Visualization of mapping of a two dimensional space through a deep
Gaussian process.
</aside>
</section>
<section id="section-177" class="slide level2">
<h2></h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="section-178" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="section-179" class="slide level2">
<h2></h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Alan Turing, in 1946 he was only 11 minutes slower than the winner of
the 1948 games. Would he have won a hypothetical games held in 1946?
Source:
<a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank">Alan
Turing Internet Scrapbook</a>.
</aside>
</section>
<section id="section-180" class="slide level2">
<h2></h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had
been held in 1946?</li>
</ul>
</section>
<section id="section-181" class="slide level2">
<h2></h2>
</section>
<section id="section-182" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/olympic-marathon-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Marathon data. The error bars are
too large, perhaps due to the outlier from 1904.
</aside>
</section>
<section id="section-183" class="slide level2">
<h2></h2>
<ul>
<li><p>Can a Deep Gaussian process help?</p></li>
<li><p>Deep GP is one GP feeding into another.</p></li>
</ul>
</section>
<section id="section-184" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep GP fit to the Olympic marathon data. Error bars now change as the
prediction evolves.
</aside>
</section>
<section id="section-185" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Point samples run through the deep Gaussian process show the
distribution of output locations.
</aside>
</section>
<section id="section-186" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-0.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from input to the latent layer is broadly, with some
flattening as time goes on. Variance is high across the input range.
</aside>
</section>
<section id="section-187" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-1.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from the latent layer to the output layer.
</aside>
</section>
<section id="section-188" class="slide level2">
<h2></h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-pinball.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A pinball plot shows the movement of the ‘ball’ as it passes through
each layer of the Gaussian processes. Mean directions of movement are
shown by lines. Shading gives one standard deviation of movement
position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. There is
some grouping of later points towards the right in the first layer,
which also injects a large amount of uncertainty. Due to flattening of
the curve in the second layer towards the right the uncertainty is
reduced in final output.
</aside>
</section>
<section id="section-189" class="slide level2">
<h2></h2>
<ul>
<li>Given given expression levels in the form of a time series from
<span class="citation" data-cites="DellaGatta:direct08">Della Gatta et
al. (2008)</span>.</li>
</ul>
</section>
<section id="section-190" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/della-gatta-gene.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gene expression levels over time for a gene from data provided by <span
class="citation" data-cites="DellaGatta:direct08">Della Gatta et al.
(2008)</span>. We would like to understand whether there is signal in
the data, or we are only observing noise.
</aside>
</section>
<section id="section-191" class="slide level2">
<h2></h2>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene
<span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and
Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-192" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Freddie Kalaitzis
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/freddie-kalaitzis.jpg" clip-path="url(#clip4)"/>
</svg>
</div>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The example is taken from the paper “A Simple Approach to Ranking
Differentially Expressed Gene Expression Time Courses through Gaussian
Process Regression.” <span class="citation"
data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.
</aside>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180"
class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-193" class="slide level2">
<h2></h2>
</section>
<section id="section-194" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale
parameter initialized to 50 minutes.
</aside>
</section>
<section id="section-195" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp2.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the time scale
parameter initialized to 2000 minutes.
</aside>
</section>
<section id="section-196" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/della-gatta-gene-gp3.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Result of the fit of the Gaussian process model with the noise
initialized low (standard deviation 0.1) and the time scale parameter
initialized to 20 minutes.
</aside>
</section>
<section id="section-197" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima000.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<!--
## {}



<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/multiple-optima001.svg" width="" style=" "></object>-->
</section>
<section id="section-198" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the Della Gatta gene expression data.
</aside>
</section>
<section id="section-199" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process samples fitted to the Della Gatta gene expression
data.
</aside>
</section>
<section id="section-200" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-0.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process mapping from input to latent layer for the della Gatta
gene expression data.
</aside>
</section>
<section id="section-201" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-layer-1.svg" width="50%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process mapping from latent to output layer for the della Gatta
gene expression data.
</aside>
</section>
<section id="section-202" class="slide level2">
<h2></h2>
<div class="figure">
<div id="della-gatta-gene-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/della-gatta-gene-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A pinball plot shows the movement of the ‘ball’ as it passes through
each layer of the Gaussian processes. Mean directions of movement are
shown by lines. Shading gives one standard deviation of movement
position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. Pinball
plot of the della Gatta gene expression data.
</aside>
</section>
<section id="section-203" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/step-function.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Simulation study of step function data artificially generated. Here
there is a small overlap between the two lines.
</aside>
</section>
<section id="section-204" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian
process models in Python. It is designed for teaching and modelling. We
welcome contributions which can be made through the GitHub repository <a
href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="section-205" class="slide level2">
<h2></h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use
GPs.</li>
<li>Available through GitHub <a
href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="section-206" class="slide level2">
<h2></h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the
algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="section-207" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/step-function-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the step function data. Note the large error
bars and the over-smoothing of the discontinuity. Error bars are shown
at two standard deviations.
</aside>
</section>
<section id="section-208" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the step function data.
</aside>
</section>
<section id="section-209" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process model for the step function fit.
</aside>
</section>
<section id="section-210" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-0.svg" width style=" ">
</object>
</section>
<section id="section-211" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-1.svg" width style=" ">
</object>
</section>
<section id="section-212" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-2.svg" width style=" ">
</object>
</section>
<section id="section-213" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-3.svg" width style=" ">
</object>
</section>
<section id="section-214" class="slide level2">
<h2></h2>
<div class="figure">
<div id="step-function-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pinball plot of the deep GP fitted to the step function data. Each layer
of the model pushes the ‘ball’ towards the left or right, saturating at
1 and 0. This causes the final density to be be peaked at 0 and 1.
Transitions occur driven by the uncertainty of the mapping in each
layer.
</aside>
</section>
<section id="section-215" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helment-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/motorcycle-helmet.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Motorcycle helmet data. The data consists of acceleration readings on a
motorcycle helmet undergoing a collision. The data exhibits
heteroschedastic (time varying) noise levles and non-stationarity.
</aside>
</section>
<section id="section-216" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/motorcycle-helmet-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="section-217" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the motorcycle helmet accelerometer data.
</aside>
</section>
<section id="section-218" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process as fitted to the motorcycle
helmet accelerometer data.
</aside>
</section>
<section id="section-219" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Mappings from the input to the latent layer for the motorcycle helmet
accelerometer data.
</aside>
</section>
<section id="section-220" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-layer-1.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Mappings from the latent layer to the output layer for the motorcycle
helmet accelerometer data.
</aside>
</section>
<section id="section-221" class="slide level2">
<h2></h2>
<div class="figure">
<div id="motorcycle-helmet-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/motorcycle-helmet-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pinball plot for the mapping from input to output layer for the
motorcycle helmet accelerometer data.
</aside>
</section>
<section id="section-222" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Ground truth movement for the position taken while recording the
multivariate time-course of wireless access point signal strengths.
</aside>
</section>
<section id="section-223" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-data-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/robot-wireless-dim-1.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Output dimension 1 from the robot wireless data. This plot shows signal
strength changing over time.
</aside>
</section>
<section id="section-224" class="slide level2">
<h2></h2>
</section>
<section id="section-225" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/robot-wireless-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Robot Wireless dimension 1.
</aside>
</section>
<section id="section-226" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-deep-gp-dim-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Fit of the deep Gaussian process to dimension 1 of the robot wireless
data.
</aside>
</section>
<section id="section-227" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-deep-gp-samples-dim-1-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-deep-gp-samples-dim-1.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process fit to dimension 1 of the robot
wireless data.
</aside>
</section>
<section id="section-228" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-ground-truth-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-ground-truth.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The ground truth movement of the WiFi reception apparatus in the Robot
WiFi experiment
</aside>
</section>
<section id="section-229" class="slide level2">
<h2></h2>
<div class="figure">
<div id="robot-wireless-latent-space-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/robot-wireless-latent-space.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Inferred two dimensional latent space from the model for the robot
wireless data.
</aside>
</section>
<section id="section-230" class="slide level2">
<h2></h2>
<ul>
<li>‘High five’ data.</li>
<li>From CMU Mocap Database <span class="citation"
data-cites="CMU-mocap03">(CMU Motion Capture Lab, 2003)</span>.</li>
<li>Model learns structure between two interacting subjects.</li>
</ul>
</section>
<section id="section-231" class="slide level2">
<h2></h2>
<div class="figure">
<div id="shared-latent-variable-model-graph-figure"
class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//shared.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Shared latent variable model structure. Here two related data sets are
brought together with a set of latent variables that are partially
shared and partially specific to one of the data sets.
</aside>
</section>
<section id="section-232" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-gp-high-five-figure" class="figure-frame">
<p><img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deep-gp-high-five2.png" width="80%" height="auto" align="" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle"></p>
</div>
</div>
<aside class="notes">
Latent spaces of the ‘high five’ data. The structure of the model is
automatically learnt. One of the latent spaces is coordinating how the
two figures walk together, the other latent spaces contain latent
variables that are specific to each of the figures separately.
</aside>
</section>
<section id="section-233" class="slide level2">
<h2></h2>
</section>
<section id="section-234" class="slide level2">
<h2></h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Zhenwen Dai
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/zhenwen-dai.jpg" clip-path="url(#clip5)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip6">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Andreas Damianou
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/andreas-damianou.png" clip-path="url(#clip6)"/>
</svg>
</div>
</section>
<section id="section-235" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-latent-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-latent.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Latent space for the deep Gaussian process learned through unsupervised
learning and fitted to a subset of the MNIST digits subsample.
</aside>
</section>
<section id="section-236" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-1-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-237" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-2-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-238" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-3-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-239" class="slide level2">
<h2></h2>
<div class="figure">
<div id="mnist-digits-subsample-hidden-1-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/mnist-digits-subsample-hidden-4-0.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Visualisation of the intermediate layer, plot of dimension 1 vs
dimension 0.
</aside>
</section>
<section id="section-240" class="slide level2">
<h2></h2>
<div class="figure">
<div id="digit-samples-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/digit-samples-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
These digits are produced by taking a tour of the two dimensional latent
space (as described by a Gaussian process sample) and mapping the tour
into the data space. We visualize the mean of the mapping in the images.
</aside>
</section>
<section id="section-241" class="slide level2">
<h2></h2>
<div class="figure">
<div id="deep-health-model-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deep-health.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The deep health model uses different layers of abstraction in the deep
Gaussian process to represent information about diagnostics and
treatment to model interelationships between a patients different data
modalities.
</aside>
</section>
<section id="section-242" class="slide level2">
<h2></h2>
<ul>
<li>HAMs change how we share ambiguous information.</li>
<li>We need to think about how that effects our sharing of
proabilities.</li>
</ul>
</section>
<section id="section-243" class="slide level2 scrollable">
<h2 class="scrollable"></h2>
<ul>
<li><p>book: <a
href="https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248">The
Atomic Human</a></p></li>
<li><p>twitter: <a
href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></p></li>
<li><p>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></p></li>
<li><p>blog posts:</p>
<p><a
href="http://inverseprobability.com/2016/11/19/lies-damned-lies-big-data">Lies,
Damned Lies and Big Data</a></p></li>
</ul>
</section>
<section id="section-244" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable"></h2>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Bengio:deep09" class="csl-entry" role="listitem">
Bengio, Y., 2009. <span class="nocase">Learning Deep Architectures for
AI</span>. Found. Trends Mach. Learn. 2, 1–127. <a
href="https://doi.org/10.1561/2200000006">https://doi.org/10.1561/2200000006</a>
</div>
<div id="ref-Boltzmann-warmetheorie77" class="csl-entry"
role="listitem">
Boltzmann, L., n.d. Über die <span>B</span>eziehung zwischen dem zweiten
<span>H</span>auptsatze der mechanischen <span>W</span>armetheorie und
der <span>W</span>ahrscheinlichkeitsrechnung, respective den
<span>S</span>ätzen über das wärmegleichgewicht. Sitzungberichte der
Kaiserlichen Akademie der Wissenschaften. Mathematisch-Naturwissen
Classe. Abt. II LXXVI, 373–435.
</div>
<div id="ref-Bui:deep16" class="csl-entry" role="listitem">
Bui, T., Hernandez-Lobato, D., Hernandez-Lobato, J., Li, Y., Turner, R.,
2016. <a href="http://proceedings.mlr.press/v48/bui16.html">Deep
<span>G</span>aussian processes for regression using approximate
expectation propagation</a>, in: Balcan, M.F., Weinberger, K.Q. (Eds.),
Proceedings of the 33rd International Conference on Machine Learning,
Proceedings of Machine Learning Research. PMLR, New York, New York, USA,
pp. 1472–1481.
</div>
<div id="ref-CMU-mocap03" class="csl-entry" role="listitem">
CMU Motion Capture Lab, 2003. The <span>CMU</span> mocap database.
</div>
<div id="ref-Coales-yellow14" class="csl-entry" role="listitem">
Coales, J.F., Kane, S.J., 2014. The <span>“yellow peril”</span> and
after. IEEE Control Systems Magazine 34, 65–69. <a
href="https://doi.org/10.1109/MCS.2013.2287387">https://doi.org/10.1109/MCS.2013.2287387</a>
</div>
<div id="ref-Damianou:thesis2015" class="csl-entry" role="listitem">
Damianou, A., 2015. Deep <span>G</span>aussian processes and variational
propagation of uncertainty (PhD thesis). University of Sheffield.
</div>
<div id="ref-Damianou:variational15" class="csl-entry" role="listitem">
Damianou, A., Titsias, M.K., Lawrence, N.D., 2016. Variational inference
for latent variables and uncertain inputs in <span>G</span>aussian
processes. Journal of Machine Learning Research 17.
</div>
<div id="ref-DellaGatta:direct08" class="csl-entry" role="listitem">
Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D.,
Missero, C., Bernardo, D. di, 2008. Direct targets of the TRP63
transcription factor revealed by a combination of gene expression
profiling and reverse engineering. Genome Research 18, 939–948. <a
href="https://doi.org/10.1101/gr.073601.107">https://doi.org/10.1101/gr.073601.107</a>
</div>
<div id="ref-Dunlop:deep2017" class="csl-entry" role="listitem">
Dunlop, M.M., Girolami, M.A., Stuart, A.M., Teckentrup, A.L., n.d. <a
href="http://jmlr.org/papers/v19/18-015.html">How deep are deep
<span>G</span>aussian processes?</a> Journal of Machine Learning
Research 19, 1–46.
</div>
<div id="ref-Duvenaud:pathologies14" class="csl-entry" role="listitem">
Duvenaud, D., Rippel, O., Adams, R., Ghahramani, Z., 2014. Avoiding
pathologies in very deep networks.
</div>
<div id="ref-Eddington:nature29" class="csl-entry" role="listitem">
Eddington, A.S., 1929. The nature of the physical world. Dent (London).
<a
href="https://doi.org/10.2307/2180099">https://doi.org/10.2307/2180099</a>
</div>
<div id="ref-Einstein-brownian05" class="csl-entry" role="listitem">
Einstein, A., 1905. Über die von der molekularkinetischen
<span>T</span>heorie der <span>W</span>ärme geforderte
<span>B</span>ewegung von in ruhenden <span>F</span>lüssigkeiten
suspendierten <span>T</span>eilchen. Annalen der Physik 322, 549–560. <a
href="https://doi.org/10.1002/andp.19053220806">https://doi.org/10.1002/andp.19053220806</a>
</div>
<div id="ref-Havasi:deepgp18" class="csl-entry" role="listitem">
Havasi, M., Hernández-Lobato, J.M., Murillo-Fuentes, J.J., 2018. <a
href="http://papers.nips.cc/paper/7979-inference-in-deep-gaussian-processes-using-stochastic-gradient-hamiltonian-monte-carlo.pdf">Inference
in deep <span>G</span>aussian processes using stochastic gradient
<span>H</span>amiltonian <span>M</span>onte <span>C</span>arlo</a>, in:
Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,
Garnett, R. (Eds.), Advances in Neural Information Processing Systems
31. Curran Associates, Inc., pp. 7506–7516.
</div>
<div id="ref-Heider-experimental44" class="csl-entry" role="listitem">
Heider, F., Simmel, M., 1944. An experimental study of apparent
behavior. The American Journal of Psychology 57, 243–259. <a
href="https://doi.org/10.2307/1416950">https://doi.org/10.2307/1416950</a>
</div>
<div id="ref-Hinton:fast06" class="csl-entry" role="listitem">
Hinton, G.E., Osindero, S., 2006. A fast learning algorithm for deep
belief nets. Neural Computation 18, 2006.
</div>
<div id="ref-Huang-inner22" class="csl-entry" role="listitem">
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng,
A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T.,
Brown, N., Luu, L., Levine, S., Hausman, K., ichter, brian, 2023. <a
href="https://proceedings.mlr.press/v205/huang23c.html">Inner monologue:
Embodied reasoning through planning with language models</a>, in: Liu,
K., Kulic, D., Ichnowski, J. (Eds.), Proceedings of the 6th Conference
on Robot Learning, Proceedings of Machine Learning Research. PMLR, pp.
1769–1782.
</div>
<div id="ref-Izmailov:subspace19" class="csl-entry" role="listitem">
Izmailov, P., Maddox, W.J., Kirichenko, P., Garipov, T., Vetrov, D.P.,
Wilson, A.G., 2019. <a href="http://arxiv.org/abs/1907.07504">Subspace
inference for bayesian deep learning</a>. CoRR abs/1907.07504.
</div>
<div id="ref-Kalaitzis:simple11" class="csl-entry" role="listitem">
Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking
differentially expressed gene expression time courses through
<span>Gaussian</span> process regression. BMC Bioinformatics 12. <a
href="https://doi.org/10.1186/1471-2105-12-180">https://doi.org/10.1186/1471-2105-12-180</a>
</div>
<div id="ref-Laplace-essai14" class="csl-entry" role="listitem">
Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.
Courcier, Paris.
</div>
<div id="ref-Lawrence:embodiment17" class="csl-entry" role="listitem">
Lawrence, N.D., 2017. <a href="https://arxiv.org/abs/1705.07996">Living
together: Mind and machine intelligence</a>. arXiv.
</div>
<div id="ref-Lawrence:hgplvm07" class="csl-entry" role="listitem">
Lawrence, N.D., Moore, A.J., 2007. Hierarchical <span>G</span>aussian
process latent variable models. pp. 481–488.
</div>
<div id="ref-MacKay:gpintroduction98" class="csl-entry" role="listitem">
MacKay, D.J.C., n.d. Introduction to <span>G</span>aussian processes.
pp. 133–166.
</div>
<div id="ref-Mackay-behind91" class="csl-entry" role="listitem">
MacKay, D.M., 1991. Behind the eye. Basil Blackwell.
</div>
<div id="ref-Mikhailov:hydrodynamica05" class="csl-entry"
role="listitem">
Mikhailov, G.K., n.d. Daniel bernoulli, hydrodynamica (1738).
</div>
<div id="ref-ONeill-trust02" class="csl-entry" role="listitem">
O’Neill, O., 2002. A question of trust. Cambridge University Press.
</div>
<div id="ref-Salakhutdinov:quantitative08" class="csl-entry"
role="listitem">
Salakhutdinov, R., Murray, I., n.d. On the quantitative analysis of deep
belief networks. pp. 872–879.
</div>
<div id="ref-Salimbeni:doubly2017" class="csl-entry" role="listitem">
Salimbeni, H., Deisenroth, M., 2017. <a
href="http://papers.nips.cc/paper/7045-doubly-stochastic-variational-inference-for-deep-gaussian-processes.pdf">Doubly
stochastic variational inference for deep <span>G</span>aussian
processes</a>, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H.,
Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in Neural
Information Processing Systems 30. Curran Associates, Inc., pp.
4591–4602.
</div>
<div id="ref-Kim-translation15" class="csl-entry" role="listitem">
Sharp, K., Matschinsky, F., 2015. Translation of <span>L</span>udwig
<span>B</span>oltzmann’s paper <span>“on the relationship between the
second fundamental theorem of the mechanical theory of heat and
probability calculations regarding the conditions for thermal
equilibrium.”</span> Entropy 17, 1971–2009. <a
href="https://doi.org/10.3390/e17041971">https://doi.org/10.3390/e17041971</a>
</div>
<div id="ref-Susskind-future15" class="csl-entry" role="listitem">
Susskind, R.E., Susskind, D., 2015. The future of the professions: How
technology will transform the work of human experts. Oxford University
Press.
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Admiralty-gunnery45" class="csl-entry" role="listitem">
The Admiralty, 1945. <a href="https://www.maritime.org/doc/br224/">The
gunnery pocket book, b.r. 224/45</a>.
</div>
<div id="ref-Thompson-juries89" class="csl-entry" role="listitem">
Thompson, W.C., 1989. <a href="http://www.jstor.org/stable/1191906">Are
juries competent to evaluate statistical evidence?</a> Law and
Contemporary Problems 52, 9–41.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
