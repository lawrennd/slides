<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2024-03-12">
  <title>The Atomic Human</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="https://inverseprobability.com/assets/css/talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2/css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="https://inverseprobability.com/assets/js/figure-animate.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">The Atomic Human</h1>
  <p class="subtitle" style="text-align:center">Understanding Ourselves
in the Age of AI</p>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil
D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2024-03-12</time></p>
  <p class="venue" style="text-align:center">St Andrews’ Distinguished
Lecture Series</p>
</section>

<section class="slide level2">

<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="section" class="slide level2">
<h2></h2>
<div class="figure">
<div id="ford-model-t-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/1925_Ford_Model_T_touring.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A 1925 Ford Model T built at Henry Ford’s Highland Park Plant in
Dearborn, Michigan. This example now resides in Australia, owned by the
founder of FordModelT.net. From <a
href="https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg"
class="uri">https://commons.wikimedia.org/wiki/File:1925_Ford_Model_T_touring.jpg</a>
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div class="figure">
<div id="diving-bell-and-butterfly-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-diving-bell-and-the-butterfly.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Diving Bell and the Buttefly is the autobiography of Jean Dominique
Bauby.
</aside>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<div class="figure">
<div id="diving-bell-letters-figure" class="figure-frame">
<div style="text-align:center;font-size:200%">
E S A R I N T U L <br> O M D P C F B V <br> H G J Q Z Y X K W
</div>
</div>
</div>
<aside class="notes">
The ordering of the letters that Bauby used for writing his
autobiography.
</aside>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="figure">
<div id="jean-dominique-bauby-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jean-Dominique_Bauby.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Jean Dominique Bauby was the Editor in Chief of the French Elle
Magazine, he suffered a stroke that destroyed his brainstem, leaving him
only capable of moving one eye. Jean Dominique became a victim of locked
in syndrome.
</aside>
</section>
<section id="section-4" class="slide level2">
<h2></h2>
<div class="figure">
<div id="bauby-shannon-figure" class="figure-frame">
<table>
<tr>
<td width>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jean-Dominique_Bauby.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ClaudeShannon_MFO3807.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Claude Shannon developed information theory which allows us to quantify
how much Bauby can communicate. This allows us to compare how locked in
he is to us.
</aside>
</section>
<section id="section-5" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-atomic-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/atomic-eye.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Atomic Eye, by slicing away aspects of the human that we used to
believe to be unique to us, but are now the preserve of the machine, we
learn something about what it means to be human.
</aside>
</section>
<section id="section-6" class="slide level2">
<h2></h2>
<div class="figure">
<div id="colossus-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//computing/Colossus.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A Colossus Mark 2 codebreaking computer being operated by Dorothy Du
Boisson (left) and Elsie Booker (right). Colossus was designed by Tommy
Flowers, but programmed and operated by groups of Wrens based at
Bletchley Park.
</aside>
</section>
<section id="section-7" class="slide level2">
<h2></h2>
<div class="figure">
<div id="embodiment-factors-table-figure" class="figure-frame">
<table>
<tr>
<td>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor.svg" width="15%" style=" ">
</object>
</td>
<td align="center">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//human.svg" width="60%" style=" ">
</object>
</td>
</tr>
<tr>
<td>
bits/min
</td>
<td align="center">
billions
</td>
<td align="center">
2,000
</td>
</tr>
<tr>
<td>
billion <br>calculations/s
</td>
<td align="center">
~100
</td>
<td align="center">
a billion
</td>
</tr>
<tr>
<td>
embodiment
</td>
<td align="center">
20 minutes
</td>
<td align="center">
5 billion years
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Embodiment factors are the ratio between our ability to compute and our
ability to communicate. Relative to the machine we are also locked in.
In the table we represent embodiment as the length of time it would take
to communicate one second’s worth of computation. For computers it is a
matter of minutes, but for a human, it is a matter of thousands of
millions of years. See also “Living Together: Mind and Machine
Intelligence” <span class="citation"
data-cites="Lawrence:embodiment17">Lawrence (2017)</span>
</aside>
</section>
<section id="section-8" class="slide level2">
<h2></h2>
</section>
<section id="section-9" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation000.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-10" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation001.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-11" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation002.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-12" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation003.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-13" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation004.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-14" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation005.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-15" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation006.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-16" class="slide level2">
<h2></h2>
<div class="anne-bob-conversation"
style="maxwidth:100vw; max-height:100vh">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-bob-conversation007.svg" width="70%" style=" ">
</object>
</div>
</section>
<section id="section-17" class="slide level2">
<h2></h2>
<div class="figure">
<div id="cappella-sistina-ceiling-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-sistine-chapel-ceiling.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The ceiling of the Sistine Chapel.
</aside>
</section>
<section id="section-18" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-creation-of-man-michelangelo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-the-creation-of-man.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Photo of Detail of Creation of Man from the Sistine chapel ceiling.
</aside>
</section>
<section id="section-19" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-creation-of-man-detail-god-michelangelo-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-the-creation-of-man-detail-god.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Photo detail of God.
</aside>
</section>
<section id="section-20" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information001.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
</section>
<section id="section-21" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information002.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
</section>
<section id="section-22" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-3-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information003.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The trinity of human, data, and computer, and highlights the modern
phenomenon. The communication channel between computer and data now has
an extremely high bandwidth. The channel between human and computer and
the channel between data and human is narrow. New direction of
information flow, information is reaching us mediated by the computer.
The focus on classical statistics reflected the importance of the direct
communication between human and data. The modern challenges of data
science emerge when that relationship is being mediated by the machine.
</aside>
</section>
<section id="section-23" class="slide level2">
<h2></h2>
<div class="figure">
<div id="classic-baby-shoes-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Classic_baby_shoes.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
For sale: baby shoes, never worn
</center>
</div>
</div>
<aside class="notes">
Consider the six-word novel, apocryphally credited to Ernest Hemingway,
“For sale: baby shoes, never worn”. To understand what that means to a
human, you need a great deal of additional context. Context that is not
directly accessible to a machine that has not got both the evolved and
contextual understanding of our own condition to realize both the
implication of the advert and what that implication means emotionally to
the previous owner.
</aside>
<aside class="notes">
Ernest Hemingway’s apocryphal six-word novel would mean nothing to a
computer.
</aside>
</section>
<section id="section-24" class="slide level2">
<h2></h2>
<div class="figure">
<div id="chicago-cuneiform-stone-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//cuneiform/chicago-cuneiform-stone.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Chicago Stone, side 2, recording sale of a number of fields, probably
from Isin, Early Dynastic Period, c. 2600 BC, black basalt
</aside>
</section>
<section id="section-25" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-future-of-professions-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/the-future-of-professions.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="https://www.amazon.co.uk/Future-Professions-Technology-Transform-Experts/dp/0198713398">The
Future of Professions</a> <span class="citation"
data-cites="Susskind-future15">(Susskind and Susskind, 2015)</span> is a
2015 book focussed on how the next wave of technology revolution is
going to effect the professions.
</aside>
</section>
<section id="section-26" class="slide level2">
<h2></h2>
<div class="figure">
<div id="coin-pusher-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//policy/Coin_pusher_2.jpg" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A coin pusher is a game where coins are dropped into th etop of the
machine, and they disrupt those on the existing steps. With any coin
drop, many coins move, but it is those on the edge, who are often only
indirectly effected, but also most traumatically effected by the change.
</aside>
</section>
<section id="section-27" class="slide level2">
<h2></h2>
<div class="figure">
<div id="ml-report-cover-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/ml-report-cover-page.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Royal Society report on Machine Learning was released on 25th April
2017
</aside>
</section>
<section id="section-28" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-cover-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-report-mori-poll-cover.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Royal Society comissioned <a
href="https://royalsociety.org/-/media/policy/projects/machine-learning/publications/public-views-of-machine-learning-ipsos-mori.pdf">public
research from Mori</a> as part of the machine learning review.
</aside>
</section>
<section id="section-29" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-1-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-1.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
One of the questions focussed on machine learning applications.
</aside>
</section>
<section id="section-30" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-2-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-2.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The public were broadly supportive of a range of application areas.
</aside>
</section>
<section id="section-31" class="slide level2">
<h2></h2>
<div class="figure">
<div id="rs-report-mori-poll-3-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/rs-mori-views-of-specific-ml-applications-3.png" width="" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
But they failed to see the point in AI’s that could produce poetry.
</aside>
</section>
<section id="section-32" class="slide level2">
<h2></h2>
<div class="figure">
<div id="chat-gpt-mercutio-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/chat-gpt-mercutio-2.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Despite the public’s desire to see challenges in health, finance,
transport, crime solved by machine learning and their skepticism around
poetry, one of the problems we have solved seems to be poetry.
</aside>
<blockquote>
<p><strong>User</strong>: Hi ChatGPT, Could you write soliloquy that
introduces what a Large Language Model is as spoken by Mercutio?</p>
<p><strong>Machine</strong>: Ah, friends, lend me your ears, I prithee,
For now I must utter a tale of wonder, Of a creation divine, called the
Large Language Model, An oracle, a sorcerer, a wordsmith, yea verily, a
master of the quill.</p>
<p>From the depths of this scholarly dominion, A creature of artifice
did rise; Twas wrought of ones and zeroes, woven as if by Puck’s own
hand, To mimic our thoughts, our voice, our very essence.</p>
<p>A thousand minds, nay, a millionfold, Conjoined in this digital
Athenaeum, Words and phrases, like soldiers in battle, Marching to form
the verses of our age.</p>
<p>This Large Language Model, a changeling so fair, Hath the power to
bring forth wisdom, To transmute our queries, our curiosities, Into
answers, clear as a midsummer’s day.</p>
<p>Yet, beware, my companions, for magic hath its limits, Not unlike the
mortal bounds of we who dwell in flesh, The Large Language Model hath
its strengths, But ’tis not an all-knowing oracle, a god it is not.</p>
<p>As Mercutio, your humble servant, doth stand before thee, I marvel at
the art and science that hath brought forth this wondrous creation, This
Large Language Model, a testament to man’s yearning, To comprehend the
world and make meaning of the great tapestry of existence.</p>
<p>In faith, dear friends, let us embrace the magic and beauty of this
digital bard, And mayhaps, by its grace, we shall glean some wisdom, As
we wander together, hand in hand, Through this boundless realm of
knowledge and wonder.</p>
<p>ChatGPT transcript, 14th April 2023.</p>
</blockquote>
</section>
<section id="section-33" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-blue-marble-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/the-earth-seen-from-apollo-17.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Blue Marble, a photo of Earth taken from Apollo 17.
</aside>
<aside class="notes">
The blue marble image of the earth taken from Apollo 17 which became a
symbol of the environmental movement. December 1972??
</aside>
</section>
<section id="section-34" class="slide level2">
<h2></h2>
<div class="figure">
<div id="eagle-from-columbia-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/2131px-Earth,_Moon_and_Lunar_Module,_AS11-44-6643.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Eagle photographed from Columbia on its return from the Lunar surface.
</aside>
</section>
<section id="section-35" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-little-red-bus-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/the-little-red-bus.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Little Red Bus, Amelia Earhart’s plane in Derry after landing.
</aside>
</section>
<section id="section-36" class="slide level2">
<h2></h2>
<div class="figure">
<div id="naca-lmal-42612-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/NACA-LMAL-42612.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
1945 photo of the NACA test pilots, from left Mel Gough, Herb Hoover,
Jack Reeder, Stefan Cavallo and Bill Gray (photo NASA, NACA LMAL 42612)
</aside>
<aside class="notes">
Bob Gilruth worked on the flying qualities of aircraft.
</aside>
</section>
<section id="section-37" class="slide level2">
<h2></h2>
</section>
<section id="section-38" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation000.svg" width="80%" style=" ">
</object>
</section>
<section id="section-39" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation001.svg" width="80%" style=" ">
</object>
</section>
<section id="section-40" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation002.svg" width="80%" style=" ">
</object>
</section>
<section id="section-41" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation003.svg" width="80%" style=" ">
</object>
</section>
<section id="section-42" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation004.svg" width="80%" style=" ">
</object>
</section>
<section id="section-43" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation005.svg" width="80%" style=" ">
</object>
</section>
<section id="section-44" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation006.svg" width="80%" style=" ">
</object>
</section>
<section id="section-45" class="slide level2">
<h2></h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//anne-computer-conversation007.svg" width="80%" style=" ">
</object>
<aside class="notes">
This can be disturbing to humans because we are used to a low bandwidth
communication rate.
</aside>
<!--include{_ai/includes/p-n-fairness.md}-->
</section>
<section id="section-46" class="slide level2">
<h2></h2>
<div class="figure">
<div id="a-question-of-trust-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/a-question-of-trust.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a href="https://www.bbc.co.uk/programmes/p00gpzfq">A Question of Trust
by Onora O’Neil</a> which examines the nature of trust and its role in
society.
</aside>
</section>
<section id="section-47" class="slide level2">
<h2></h2>
<blockquote>
<p>Again Univesities are to treat each applicant fairly on the basis of
ability and promise, but they are supposed also to admit a socially more
representative intake.</p>
<p>There’s no guarantee that the process meets the target.</p>
<p>Onora O’Neill <em>A Question of Trust: Called to Account</em> Reith
Lectures 2002 <span class="citation" data-cites="ONeill-trust02">O’Neill
(2002)</span>]</p>
</blockquote>
<!--undef{noSlideTitle}


## {}




<table><tr><td width="50%"><center>

<svg viewBox="0 0 200 200" width="55%">
  <defs>
    <linearGradient id="gradient-0" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
    </linearGradient>
  </defs>
  <circle cx="100" cy="100" r="100" fill="url(#gradient-0)" />
  <text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle">policy</text>
</svg></center></td><td width="50%"><center>

<svg viewBox="0 0 200 200" width="55%">
  <defs>
    <linearGradient id="gradient-1" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
    </linearGradient>
  </defs>
  <circle cx="100" cy="100" r="100" fill="url(#gradient-1)" />
  <text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle"><tspan x="100" y="90">data</tspan><tspan x="100" y="130">governance</tspan></text>
</svg></center></td></tr></table>
<table><tr><td width="50%"><center>

<svg viewBox="0 0 200 200" width="55%">
  <defs>
    <linearGradient id="gradient-2" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
    </linearGradient>
  </defs>
  <circle cx="100" cy="100" r="100" fill="url(#gradient-2)" />
  <text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle"><tspan x="100" y="90">accelerate</tspan><tspan x="100" y="130">science</tspan></text>
</svg></center></td><td width="50%"><center>

<svg viewBox="0 0 200 200" width="55%">
  <defs>
    <linearGradient id="gradient-3" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(80,80,80);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(163,193,173);stop-opacity:1" />
    </linearGradient>
  </defs>
  <circle cx="100" cy="100" r="100" fill="url(#gradient-3)" />
  <text fill="#ffffff" x="100" y="100" text-anchor="middle" alignment-baseline="middle">AutoAI</text>
</svg></center></td></tr></table>

















define{noSlideTitle}
-->
<!-- SECTION The Structure of Scientific Revolutions -->
</section>
<section id="the-structure-of-scientific-revolutions"
class="slide level2">
<h2>The Structure of Scientific Revolutions</h2>
<div class="figure">
<div id="the-structure-of-scientific-revolutions-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/structure-of-scientific-revolutions.png" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions">The
Structure of Scientific Revolutions by Thomas S. Kuhn</a> suggests
scientific paradigms are recorded in books.
</aside>
</section>
<section id="section-48" class="slide level2">
<h2></h2>
<div class="figure">
<div id="blake-newton-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/blake-newton.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
William Blake’s <em>Newton</em>.
</aside>
</section>
<section id="section-49" class="slide level2">
<h2></h2>
<div class="figure">
<div id="lunette-rehoboam-abijah-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-lunette-rehoboam-abijah.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Lunette containing Rehoboam and Abijah.
</aside>
</section>
<section id="section-50" class="slide level2">
<h2></h2>
<div class="figure">
<div id="elohim-creating-adam-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/blake-elohim-creating-adam.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
William Blake’s <em>Elohim Creating Adam</em>.
</aside>
</section>
<section id="section-51" class="slide level2">
<h2></h2>
<div class="figure">
<div id="michelangelo-fall-and-expulsion-from-garden-of-eden-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//art/michelangelo-fall-and-expulsion-from-the-garden-of-eden.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Photo of detail of the fall and expulsion from the Garden of Eden.
</aside>
</section>
<section id="section-52" class="slide level2">
<h2></h2>
<div class="figure">
<div id="people-culture-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/people-culture.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
People communicate through artifacts and culture.
</aside>
<!-- Lecture 2 -->
<!-- SECTION Lecture 2 -->
</section>
<section id="lecture-2" class="slide level2">
<h2>Lecture 2</h2>
</section>
<section id="section-53" class="slide level2">
<h2></h2>
<div class="figure">
<div id="bandwidth-vs-complexity-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/bandwidth-vs-complexity.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Bandwidth vs Complexity.
</aside>
<!-- AI Fallacy -->
</section>
<section id="section-54" class="slide level2">
<h2></h2>
<!-- AI Fallacy -->
<!-- SECTION The Great AI Fallacy -->
</section>
<section id="the-great-ai-fallacy" class="slide level2">
<h2>The Great AI Fallacy</h2>
<div class="figure">
<div id="jeeves-springtime-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Jeeves_in_the_Springtime_01.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
We seem to have fallen for a perspective on AI that suggests it will
adapt to our schedule, rather in the manner of a 1930s manservant.
</aside>
<aside class="notes">
Since the machine learning field was rebranded as AI, done public
understanding, but also more interconnection with other fields.
Struggled to find a consistent definition for AI. But the definitions I
think public uses have one thing in common. The idea that AI will be the
first generation of automation to adapt to us.
</aside>
</section>
<section id="intellectual-debt" class="slide level2">
<h2>Intellectual Debt</h2>
<div class="figure">
<div id="intellectual-debt-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/2020-02-12-intellectual-debt.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Jonathan Zittrain’s term to describe the challenges of explanation that
come with AI is Intellectual Debt.
</aside>
</section>
<section id="technical-debt" class="slide level2">
<h2>Technical Debt</h2>
<ul>
<li>Compare with <em>technical debt</em>.</li>
<li>Highlighted by <span class="citation"
data-cites="Sculley:debt15">Sculley et al. (2015)</span>.</li>
</ul>
</section>
<section id="separation-of-concerns" class="slide level2">
<h2>Separation of Concerns</h2>
</section>
<section id="intellectual-debt-1" class="slide level2">
<h2>Intellectual Debt</h2>
<ul>
<li><p>Technical debt is the inability to <em>maintain</em> your complex
software system.</p></li>
<li><p>Intellectual debt is the inability to <em>explain</em> your
software system.</p></li>
</ul>
</section>
<section id="the-horizon-scandal" class="slide level2">
<h2>The Horizon Scandal</h2>
<div class="figure">
<div id="horizon-scandal-figure" class="figure-frame">
<table>
<tr>
<td width="25%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//logos/fujitsu-logo.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="49%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//software/horizon-select-committee.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="25%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//logos/post-office-logo.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
The still is from <a
href="https://www.parliamentlive.tv/Event/Index/d05cb9e7-04d0-4d05-8a43-ddd74b1eecc0">the
2015 select committee</a>.
</aside>
</section>
<section id="the-mythical-man-month" class="slide level2">
<h2>The Mythical Man-month</h2>
<div class="figure">
<div id="intellectual-debt-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/Mythical_man-month_(book_cover).jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Mythical Man-month <span class="citation"
data-cites="Brooks:mythical75">(Brooks, n.d.)</span> is a 1975 book
focussed on the challenges of software project coordination.
</aside>
</section>
<section id="artificial-vs-natural-systems" class="slide level2">
<h2>Artificial vs Natural Systems</h2>
<ul>
<li>Consider natural intelligence, or natural <em>systems</em></li>
<li>Contrast between an artificial <em>system</em> and an natural
system.</li>
<li>The key difference between the two is that artificial systems are
<em>designed</em> whereas natural systems are <em>evolved</em>.</li>
</ul>
</section>
<section id="natural-systems-are-evolved" class="slide level2">
<h2>Natural Systems are Evolved</h2>
<blockquote>
<p>Survival of the fittest</p>
<p>?</p>
</blockquote>
</section>
<section id="natural-systems-are-evolved-1" class="slide level2">
<h2>Natural Systems are Evolved</h2>
<blockquote>
<p>Survival of the fittest</p>
<p><a href="https://en.wikipedia.org/wiki/Herbert_Spencer">Herbet
Spencer</a>, 1864</p>
</blockquote>
</section>
<section id="natural-systems-are-evolved-2" class="slide level2">
<h2>Natural Systems are Evolved</h2>
<blockquote>
<p>Non-survival of the non-fit</p>
</blockquote>
</section>
<section id="mistake-we-make" class="slide level2">
<h2>Mistake we Make</h2>
<ul>
<li>Equate fitness for objective function.</li>
<li>Assume static environment and known objective.</li>
</ul>
</section>
<section id="technical-consequence" class="slide level2">
<h2>Technical Consequence</h2>
<ul>
<li>Classical systems design assumes <em>decomposability</em>.</li>
<li>Data-driven systems interfere with decomponsability.</li>
</ul>
</section>
<section id="bits-and-atoms" class="slide level2">
<h2>Bits and Atoms</h2>
<ul>
<li>The gap between the game and reality.</li>
<li>The need for extrapolation over interpolation.</li>
</ul>
</section>
<section id="ride-allocation-prediction" class="slide level2">
<h2>Ride Allocation Prediction</h2>
<div class="figure">
<div id="ride-allocation-system-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/ride-allocation-prediction.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Some software components in a ride allocation system. Circled components
are hypothetical, rectangles represent actual data.
</aside>
</section>
<section id="data-oriented-architectures" class="slide level2">
<h2>Data Oriented Architectures</h2>
<ul>
<li>View data to a <em>first-class citizen</em>.</li>
<li>Prioritise decentralisation.</li>
<li>Openness</li>
</ul>
</section>
<section id="data-orientated-architectures" class="slide level2">
<h2>Data Orientated Architectures</h2>
<ul>
<li>Historically we’ve been <em>software first</em>
<ul>
<li>A necessary but not sufficient condition for <em>data
first</em></li>
</ul></li>
<li>Move from
<ol type="1">
<li>service oriented architectures</li>
<li><em>data oriented architectures</em></li>
</ol></li>
</ul>
</section>
<section id="data-oriented-principles" class="slide level2">
<h2>Data Oriented Principles</h2>
<div class="figure">
<div id="data-oriented-principles-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//software/data-oriented-principles.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
For an overview of data oriented principles see <span class="citation"
data-cites="Cabrera-realworld23">Cabrera et al. (2023)</span>.
</aside>
</section>
<section id="how-llms-are-different" class="slide level2">
<h2>How LLMs are Different</h2>
</section>
<section id="the-moniac" class="slide level2">
<h2>The MONIAC</h2>
<div class="figure">
<div id="the-moniac-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Phillips_and_MONIAC_LSE.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Bill Phillips and his MONIAC (completed in 1949). The machine is an
analogue computer designed to simulate the workings of the UK economy.
</aside>
</section>
<section id="donald-mackay" class="slide level2">
<h2>Donald MacKay</h2>
<div class="figure">
<div id="donald-maccrimmon-mackay-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//people/DonaldMacKay1952.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Donald M. MacKay (1922-1987), a physicist who was an early member of the
cybernetics community and member of the Ratio Club.
</aside>
</section>
<section id="fire-control-systems" class="slide level2">
<h2>Fire Control Systems</h2>
<div class="figure">
<div id="low-angle-fire-control-team-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/low-angle-fire-control-team.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The fire control computer set at the centre of a system of observation
and tracking <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
</aside>
</section>
<section id="section-55" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-measurement-of-inclination-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/the-measurement-of-inclination.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Measuring inclination between two ships <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
Sophisticated fire control computers allowed the ship to continue to
fire while under maneuvers.
</aside>
</section>
<section id="section-56" class="slide level2">
<h2></h2>
<div class="figure">
<div id="typical-modern-fire-control-table-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/typical-modern-fire-control-table.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
A second world war gun computer’s control table <span class="citation"
data-cites="Admiralty-gunnery45">(The Admiralty, 1945)</span>.
</aside>
</section>
<section id="section-57" class="slide level2">
<h2></h2>
<div class="figure">
<div id="us-navy-training-film-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/gwf5mAlI7Ug?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
U.S. Navy training film MN-6783a. Basic Mechanisms of Fire Control
Computers. Mechanical Computer Instructional Film 27794 (1953) for the
Mk 1A Fire Control Computer.
</aside>
</section>
<section id="behind-the-eye" class="slide level2">
<h2>Behind the Eye</h2>
<div class="figure">
<div id="behind-the-eye-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/behind-the-eye.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="https://www.amazon.co.uk/Behind-Eye-Gifford-Lectures-MACKAY/dp/0631173323">Behind
the Eye</a> <span class="citation" data-cites="Mackay-behind91">(MacKay,
1991)</span> summarises MacKay’s Gifford Lectures, where MacKay uses the
operation of the eye as a window on the operation of the brain.
</aside>
</section>
<section id="section-58" class="slide level2">
<h2></h2>
<blockquote>
<p>Later in the 1940’s, when I was doing my Ph.D. work, there was much
talk of the brain as a computer and of the early digital computers that
were just making the headlines as “electronic brains.” As an analogue
computer man I felt strongly convinced that the brain, whatever it was,
was not a digital computer. I didn’t think it was an analogue computer
either in the conventional sense.</p>
</blockquote>
</section>
<section id="human-analogue-machine" class="slide level2">
<h2>Human Analogue Machine</h2>
<div class="figure">
<div id="human-analogue-machine-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/human-analogue-machine.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The human analogue machine creates a feature space which is analagous to
that we use to reason, one way of doing this is to have a machine
attempt to compress all human generated text in an auto-regressive
manner.
</aside>
</section>
<section id="human-analogue-machine-1" class="slide level2">
<h2>Human Analogue Machine</h2>
<div class="figure">
<div id="human-analogue-machine-2-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/processor-ham.svg" width="40%" style=" ">
</object>
</div>
</div>
<aside class="notes">
</aside>
<ul>
<li><p>A human-analogue machine is a machine that has created a feature
space that is analagous to the “feature space” our brain uses to
reason.</p></li>
<li><p>The latest generation of LLMs are exhibiting this charateristic,
giving them ability to converse.</p></li>
</ul>
</section>
<section id="heider-and-simmel-1944" class="slide level2">
<h2>Heider and Simmel (1944)</h2>
<div class="figure">
<div id="heider-simmel-shapes-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/8FIEZXMUM2I?start=7" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Fritz Heider and Marianne Simmel’s video of shapes from <span
class="citation"
data-cites="Heider-experimental44">(<strong>Heider-experimental44?</strong>)</span>.
</aside>
</section>
<section id="counterfeit-people" class="slide level2">
<h2>Counterfeit People</h2>
<ul>
<li>Perils of this include <em>counterfeit people</em>.</li>
<li>Daniel Dennett has described the challenges these bring in <a
href="https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/">an
article in The Atlantic</a>.</li>
</ul>
</section>
<section id="psychological-representation-of-the-machine"
class="slide level2">
<h2>Psychological Representation of the Machine</h2>
<ul>
<li><p>But if correctly done, the machine can be appropriately
“psychologically represented”</p></li>
<li><p>This might allow us to deal with the challenge of
<em>intellectual debt</em> where we create machines we cannot
explain.</p></li>
</ul>
</section>
<section id="section-59" class="slide level2">
<h2></h2>
<div class="figure">
<div id="anne-llm-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-llm-conversation.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.
</aside>
</section>
<section id="in-practice" class="slide level2">
<h2>In practice …</h2>
<ul>
<li><p>LLMs are already being used for robot planning <span
class="citation" data-cites="Huang-inner22">Huang et al.
(2023)</span></p></li>
<li><p>Ambiguities are reduced when the machine has had large scale
access to human cultural understanding.</p></li>
</ul>
</section>
<section id="inner-monologue" class="slide level2">
<h2>Inner Monologue</h2>
<div class="figure">
<div id="ai-for-data-analytics-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/0sJjdxn5kcI?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
The Inner Monologue paper suggests using LLMs for robotic planning <span
class="citation" data-cites="Huang-inner22">(Huang et al., 2023)</span>.
</aside>
</section>
<section id="ham" class="slide level2">
<h2>HAM</h2>
<div class="figure">
<div id="new-flow-of-information-4-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information004.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The trinity of human, data, and computer, and highlights the modern
phenomenon. The communication channel between computer and data now has
an extremely high bandwidth. The channel between human and computer and
the channel between data and human is narrow. New direction of
information flow, information is reaching us mediated by the computer.
The focus on classical statistics reflected the importance of the direct
communication between human and data. The modern challenges of data
science emerge when that relationship is being mediated by the machine.
</aside>
</section>
<section id="section-60" class="slide level2">
<h2></h2>
<div class="figure">
<div id="new-flow-of-information-ham-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//data-science/new-flow-of-information-ham.svg" width="70%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The HAM now sits between us and the traditional digital computer.
</aside>
</section>
<section id="networked-interactions" class="slide level2">
<h2>Networked Interactions</h2>
<div class="figure">
<div id="human-computers-interacting-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-computers-interacting.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Humans and computers interacting should be a major focus of our research
and engineering efforts.
</aside>
<!-- Fritz Heider -->
<!-- Conversation LLM -->
</section>
<section id="section-61" class="slide level2">
<h2></h2>
<div class="figure">
<div id="human-culture-interacting-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//ai/human-culture-interacting.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Humans use culture, facts and ‘artefacts’ to communicate.
</aside>
</section>
<section id="known-biases" class="slide level2">
<h2>Known Biases</h2>
</section>
<section id="complexity-in-action" class="slide level2">
<h2>Complexity in Action</h2>
<div class="figure">
<div id="monkey-business-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/vJG698U2Mvo?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Daniel Simon’s famous illusion “monkey business”. Focus on the movement
of the ball distracts the viewer from seeing other aspects of the image.
</aside>
</section>
<section id="a-hypothesis-as-a-liability" class="slide level2">
<h2>A Hypothesis as a Liability</h2>
<blockquote>
<p>“ ‘When someone seeks,’ said Siddhartha, ‘then it easily happens that
his eyes see only the thing that he seeks, and he is able to find
nothing, to take in nothing. […] Seeking means: having a goal. But
finding means: being free, being open, having no goal.’ ”</p>
<p>Hermann Hesse</p>
</blockquote>
</section>
<section id="the-scientific-process" class="slide level2">
<h2>The Scientific Process</h2>
<div class="figure">
<div id="licsb-popper-quote-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//data-science/licsb-popper-quote.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Quote from <span class="citation"
data-cites="Lawrence:licsbintro10">Lawrence (2010)</span> highlighting
the importance of interaction between data and hypothesis.
</aside>
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
</section>
<section id="conclusions" class="slide level2">
<h2>Conclusions</h2>
<ul>
<li>Intellectual debt means we don’t understand digital computer.</li>
<li>Interaction between focus and context part of how we deal with this
challenge.</li>
<li>LLMs could make these challenges harder but if well deployed could
help.</li>
</ul>
<!-- Lecture 3 -->
<!-- SECTION Lecture 3 -->
</section>
<section id="lecture-3" class="slide level2">
<h2>Lecture 3</h2>
</section>
<section id="richard-feynmann-on-doubt" class="slide level2">
<h2>Richard Feynmann on Doubt</h2>
<blockquote>
<p>One thing is I can live with is doubt, and uncertainty and not
knowing. I think it’s much more interesting to live with not knowing
than to have an answer that might be wrong.</p>
<p>Richard P. Feynmann in the <em>The Pleasure of Finding Things
Out</em> 1981.</p>
</blockquote>
<!-- Conversation LLM -->
</section>
<section id="section-62" class="slide level2">
<h2></h2>
<div class="figure">
<div id="anne-probability-conversation-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ai/anne-probability-conversation.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
The focus so far has been on reducing uncertainty to a few
representative values and sharing numbers with human beings. We forget
that most people can be confused by basic probabilities for example the
prosecutor’s fallacy.
</aside>
</section>
<section id="in-practice-1" class="slide level2">
<h2>In practice …</h2>
<ul>
<li><p>There is a lot of evidence that probabilities aren’t
interpretable.</p></li>
<li><p>See e.g. <span class="citation"
data-cites="Thompson-juries89">Thompson (1989)</span></p></li>
</ul>
</section>
<section id="section-63" class="slide level2">
<h2></h2>
<!--
Time scales, how when you expand or contract time signal becomes noise and noise becomes signal illustrate with Dirac delta and and stochastic processes in Fourier space, ito calculus. Latent force models.

Practical examples of what happens understochasticity:

0) Derive U = W + TS?? Go from microscopic to macroscopic. 

1) Kappenball --- world in between where interesting things happen,

2) Queue efficiency (M/M/1  1/(1-\rho))

3) Input to the system being in the form of bias and variance (or perhaps Brownian motion, wiener process)

(Latent force models being driven by this???? Latent force as high frequency information processing? Environment as slow?-->
<!-- Mathematical Statistics -->
</section>
<section id="section-64" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>??</p>
</blockquote>
</section>
<section id="section-65" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
</section>
<section id="section-66" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and statistics</p>
<p>Arthur Balfour 1848-1930</p>
</blockquote>
</section>
<section id="section-67" class="slide level2">
<h2></h2>
<blockquote>
<p>There are three types of lies: lies, damned lies and ‘big data’</p>
<p>Neil Lawrence 1972-?</p>
</blockquote>
</section>
<section id="mathematical-statistics" class="slide level2">
<h2><em>Mathematical</em> Statistics</h2>
<div class="figure">
<div id="portrait-of-karl-pearson-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Portrait_of_Karl_Pearson.jpg" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Karl Pearson (1857-1936), one of the founders of Mathematical
Statistics.
</aside>
</section>
<section id="mathematical-data-science" class="slide level2">
<h2>‘Mathematical Data Science’</h2>
<div class="figure">
<div id="question-mark-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Question_mark.png" width="30%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
‘Founded’ by ? (?-?)
</aside>
</section>
<section id="laplaces-demon" class="slide level2">
<h2>Laplace’s Demon</h2>
<div class="centered" style="">
<a
href="https://play.google.com/books/reader?id=1YQPAAAAQAAJ&amp;pg=PR17-IA2"><img
data-src="https://inverseprobability.com/talks/../slides/diagrams//books/1YQPAAAAQAAJ-PR17-IA2.png" /></a>
</div>
<aside class="notes">
This notion is known as <em>Laplace’s demon</em> or <em>Laplace’s
superman</em>.
</aside>
</section>
<section id="laplaces-demon-1" class="slide level2">
<h2>Laplace’s Demon</h2>
<div class="figure">
<div id="laplaces-demon-cropped-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/philosophicaless00lapliala_16_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
English translation of Laplace’s demon, taken from the Philosophical
Essay on probabilities <span class="citation"
data-cites="Laplace-essai14">Laplace (1814)</span> pg 3.
</aside>
<blockquote>
<p><em>Philosophical Essay on Probabilities</em> <span class="citation"
data-cites="Laplace-essai14">Laplace (1814)</span> pg 3</p>
</blockquote>
</section>
<section id="machine-learning" class="slide level2">
<h2>Machine Learning</h2>
<center>
<span class="math display">\[
\text{model} + \text{data} \stackrel{\text{compute}}{\rightarrow}
\text{prediction}\]</span>
</center>
</section>
<section id="theory-of-everything" class="slide level2">
<h2>Theory of Everything</h2>
<blockquote>
<p>If we do discover a theory of everything … it would be the ultimate
triumph of human reason-for then we would truly know the mind of God</p>
<p>Stephen Hawking in <em>A Brief History of Time</em> 1988</p>
</blockquote>
</section>
<section id="game-of-life" class="slide level2">
<h2>Game of Life</h2>
</section>
<section id="loneliness" class="slide level2">
<h2>Loneliness</h2>
<div class="figure">
<div id="life-rules-loneliness-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>loneliness</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-1-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
‘Death’ through loneliness in Conway’s game of life. If a cell is
surrounded by less than three cells, it ‘dies’ through loneliness.
</aside>
</section>
<section id="crowding" class="slide level2">
<h2>Crowding</h2>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>overcrowding</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-2-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
‘Death’ through overpopulation in Conway’s game of life. If a cell is
surrounded by more than three cells, it ‘dies’ through loneliness.
</aside>
</section>
<section id="birth" class="slide level2">
<h2>Birth</h2>
<div class="figure">
<div id="life-rules-crowding-figure" class="figure-frame">
<table>
<tr>
<td width="70%">
<table>
<tr>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-0.svg" width="100%" style=" ">
</object>
</center>
</td>
<td width="39%">
<center>
<em>birth</em>
</center>
<center>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//util/right-arrow.svg" width="60%" style=" ">
</object>
</center>
</td>
<td width="30%">
<center>
<object class data="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-rules-3-1.svg" width="100%" style=" ">
</object>
</center>
</td>
</tr>
</table>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Birth in Conway’s life. Any position surrounded by precisely three live
cells will give birth to a new cell at the next turn.
</aside>
</section>
<section id="glider" class="slide level2">
<h2>Glider</h2>
<div class="figure">
<div id="glider-loafer-conway-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Glider (1969)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Glider.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
<em>Left</em> A Glider pattern discovered 1969 by Richard K. Guy.
<em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020). The glider is an oscillator that moves diagonally after
creation. From the simple rules of Life it’s not obvious that such an
object does exist, until you do the necessary computation.
</aside>
</section>
<section id="section-68" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gosper-glider-gun-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Gosperglidergun.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Gosper glider gun is a configuration that creates gliders. A new
glider is released after every 30 turns.
</aside>
</section>
<section id="loafer" class="slide level2">
<h2>Loafer</h2>
</section>
<section id="section-69" class="slide level2">
<h2></h2>
<div class="figure">
<div id="the-loafer-spaceship-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<center>
<em>Loafer (2013)</em>
</center>
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/Loafer.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//maths/John-Conway.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
<em>Left</em> A Loafer pattern discovered by Josh Ball in 2013.
<em>Right</em>. John Horton Conway, creator of <em>Life</em>
(1937-2020).
</aside>
</section>
<section id="section-70" class="slide level2">
<h2></h2>
<div class="figure">
<div id="life-in-life-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//simulation/life-in-life.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The Game of Life running in Life. The video is drawing out recursively
showing pixels that are being formed by filling cells with moving
spaceships. Each individual pixel in this game of life is made up of
<span class="math inline">\(2048 \times 2048\)</span> pixels called an
<a href="https://www.conwaylife.com/wiki/OTCA_metapixel">OTCA
metapixel</a>.
</aside>
</section>
<section id="section-71" class="slide level2">
<h2></h2>
<div class="figure">
<div id="intro-to-life-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/Kk2MH9O4pXY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
An introduction to the Game of Life by Alan Zucconi.
</aside>
</section>
<section id="emergent-behaviour" class="slide level2">
<h2>Emergent Behaviour</h2>
<aside class="notes">
The phenomenon of emergent behaviour also applies to real world
simulations like climate and weather. E.g. Niall Robinson defining a
hurricane to search for hurricane’s im climate simulations.
</aside>
</section>
<section id="laplaces-gremlin" class="slide level2">
<h2>Laplace’s Gremlin</h2>
<div class="figure">
<div id="probability-relative-in-part-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/philosophicaless00lapliala_18_cropped.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
To Laplace, determinism is a strawman. Ignorance of mechanism and data
leads to uncertainty which should be dealt with through probability.
</aside>
<blockquote>
<p><em>Philosophical Essay on Probabilities</em> <span class="citation"
data-cites="Laplace-essai14">Laplace (1814)</span> pg 5</p>
</blockquote>
<aside class="notes">
I like to refer to this notion as “Laplace’s Gremlin”, because the lack
of knowledge is the “gremlin of uncertainty” that inhibits the
“deterministic demon”
</aside>
</section>
<section id="section-72" class="slide level2">
<h2></h2>
<div class="figure">
<div id="germlins-think-its-fun-to-hurt-you-figure"
class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/gremlins-think-its-fun-to-hurt-you.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Gremlins are seen as the cause of a number of challenges in this World
War II poster.
</aside>
</section>
<section id="abstraction-and-emergent-properties" class="slide level2">
<h2>Abstraction and Emergent Properties</h2>
<div class="figure">
<div id="simulation-scales-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//physics/simulation-scales.svg" width="90%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A scale of different simulations we might be interested in when
modelling the physical world. The scale is <span
class="math inline">\(\log_{10}\)</span> meters. The scale reflects
something about the level of granularity where we might choose to know
“all positions of all items of which nature is composed”.
</aside>
</section>
<section id="abstraction" class="slide level2">
<h2>Abstraction</h2>
<ul>
<li>We often abstract smaller scales away e.g. in statistical
mechanics.</li>
<li>When we’re abstracting finer length scales we can introduce
uncertainties.
<ul>
<li>E.g. Maxwell-Boltzmann distribution for ideal Gas.</li>
</ul></li>
</ul>
</section>
<section id="emergence" class="slide level2">
<h2>Emergence</h2>
<ul>
<li>But fine scale local-laws also lead to emergent properties.</li>
<li>Some of these properties exist at large scale.</li>
<li>In particular, when there are complex interactions between
molecules.</li>
</ul>
<aside class="notes">
Ideal gas is a model where interaction between molecules is relatively
simple. If this interaction is more complex (e.g. through quantum
interactions of their bonded electrons, more structure exists.
</aside>
</section>
<section id="molecular-dynamics-simulations" class="slide level2">
<h2>Molecular Dynamics Simulations</h2>
<div class="figure">
<div id="v-atp-ase-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//sysbio/rotary_proton_sv_pump_anim_final.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The V-ATPase enzyme pumps proteins across membranes. This molecular
dynamics simulation was published in <em>Science Advances</em> <span
class="citation" data-cites="Roh-cryo-em20">(Roh et al., 2020)</span>.
The scale is roughly <span class="math inline">\(10^{-8} m\)</span>.
</aside>
<aside class="notes">
Those complex interactions can also be modelled, but we can no longer
summarize with the Maxwell-Boltzmann distribution. We need molecular
dynamics simulations which can be combined with imaging using electron
microscopes operating at extremely cold temperatures.
</aside>
</section>
<section id="quantum-mechanics" class="slide level2">
<h2>Quantum Mechanics</h2>
<div class="figure">
<div id="many-electron-schroedinger-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/many-electron-schroedinger.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The many-electron Schrödinger equation is important in understanding how
Chemical bonds are formed.
</aside>
</section>
<section id="lenox-globe" class="slide level2">
<h2>Lenox Globe</h2>
<div class="figure">
<div id="lenox-globe-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
<a
href="http://www.myoldmaps.com/renaissance-maps-1490-1800/314-the-lenox-globe/314-lenox.pdf">The
Lenox globe</a>, which dates from early 16th century, one of the
earliest known globes.
</aside>
</section>
<section id="drawing-of-lenox-globe" class="slide level2">
<h2>Drawing of Lenox Globe</h2>
<div class="figure">
<div id="lenox-globe-by-b-f-da-costa-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe-by-b-f-da-costa.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Drawing of the Lenox Globe by the historian for the Magazine of American
History in September 1879.
</aside>
</section>
<section id="hic-sunt-dracones" class="slide level2">
<h2><em>Hic Sunt Dracones</em></h2>
<div class="figure">
<div id="lenox-globe-hic-sunt-dracones-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/lenox-globe-hic-sunt-dracones.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Detail from the Lenox globe located in the region of China, “hic sunt
dracones”
</aside>
</section>
<section id="weather" class="slide level2">
<h2>Weather</h2>
<div class="figure">
<div id="met-office-weather-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/1944-06-05_met-office-weather.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Forecast from UK Met Office on 5th June 1944. (detail from <a
href="https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day"
class="uri">https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day</a>)
</aside>
</section>
<section id="weather-1" class="slide level2">
<h2>Weather</h2>
<div class="figure">
<div id="dwd-weather-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/1944-06-05_dwd-weather.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Forecast from Deutscher Wetterdienst on 5th June 1944. (detail from <a
href="https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day"
class="uri">https://www.metoffice.gov.uk/research/library-and-archive/archive-hidden-treasures/d-day</a>).
Note the lack of measurements within the UK. THis is the direction that
weather was coming from so the locaiton of weather fronts (and
associated storms) was harder for the Deutscher Wetterdienst to predict
than the Met Office.
</aside>
<!-- thermodynamics -->
</section>
<section id="boulton-and-watts-lap-engine" class="slide level2">
<h2>Boulton and Watt’s Lap Engine</h2>
<div class="figure">
<div id="lap-engine-boulton-watt-figure" class="figure-frame">
<table>
<tr>
<td width="60%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/lap-engine.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Lap Engine (1788)</em>
</center>
</td>
<td width="40%">
<center>
total energy <br> = <br> available energy <br> + <br> temperature <br>
<span class="math inline">\(\times\)</span> <br>entropy
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
James Watt’s Lap Engine which incorporates many of his innovations to
the steam engine, making it more efficient.
</aside>
<!--THEORY of IGNORANCE-->
</section>
<section id="brownian-motion-and-wiener" class="slide level2">
<h2>Brownian Motion and Wiener</h2>
<div class="figure">
<div id="albert-einstein-photo-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Albert Einstein’s 1905 paper on Brownian motion introduced stochastic
differential equations which can be used to model the ‘curve of a simple
molecule of air’.
</aside>
</section>
<section id="section-73" class="slide level2">
<h2></h2>
<div class="figure">
<div id="russell-wiener-russell-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//philosophy/Bertrand_Russell_1957.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Betrand Russell</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Albert_Einstein_photo_1921.jpg" width="85%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Albert Einstein</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Norbert Wiener</em>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Bertrand Russell (1872-1970), Albert Einstein (1879-1955), Norbert
Wiener, (1894-1964)
</aside>
<aside class="notes">
Wiener came to Cambridge in 1913. Russell showed him Einstein’s 1905
paper on Brownian motion (<span class="citation"
data-cites="Einstein-brownian05">Einstein (1905)</span>)
</aside>
</section>
<section id="brownian-motion" class="slide level2">
<h2>Brownian Motion</h2>
<div class="figure">
<div id="brownian-motion-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/brownian-motion.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Brownian motion of a large particle in a group of smaller particles. The
movement is known as a <em>Wiener process</em> after Norbert Wiener.
</aside>
</section>
<section id="stochasticity-and-control" class="slide level2">
<h2>Stochasticity and Control</h2>
<div class="figure">
<div id="norbert-wiener-yellow-peril-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/Norbert_wiener.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="45%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//books/wiener-yellow-peril.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Norbert Wiener (1894 - 1964). Founder of cybernetics and the information
era. He used Gibbs’s ideas to develop a “theory of ignorance” that he
deployed in early communication. On the right is Wiener’s wartime report
that used stochastic processes in forecasting with applications in radar
control (image from <span class="citation"
data-cites="Coales-yellow14">Coales and Kane (2014)</span>).
</aside>
</section>
<section id="statistical-physics" class="slide level2">
<h2>Statistical Physics</h2>
<div class="figure">
<div id="maxwell-boltzmann-gibbs-figure" class="figure-frame">
<table>
<tr>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/james-clerk-maxwell.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>James Clerk Maxwell</em>
</center>
</td>
<td width="30%">
<center>
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/boltzmann2.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</center>
<center>
<em>Ludwig Boltzmann</em>
</center>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//physics/j-w-gibbs.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<center>
<em>Josiah Willard Gibbs</em>
</center>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
James Clerk Maxwell (1831-1879), Ludwig Boltzmann (1844-1906) Josiah
Willard Gibbs (1839-1903)
</aside>
</section>
<section id="entropy-billiards" class="slide level2">
<h2>Entropy Billiards</h2>
<div class="figure">
<div id="entropy-billiards-js-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="multiball-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left ">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="multiball-entropy">
</output>
</div>
<div id="multiball-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="multiball-newball" style="text-align:right">
New Ball
</button>
<button id="multiball-pause" style="text-align:right">
Pause
</button>
<button id="multiball-skip" style="text-align:right">
Skip 1000s
</button>
<button id="multiball-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/ballworld.js"></script>
<script src="https://inverseprobability.com/talks/scripts//ballworld/multiball.js"></script>
</div>
</div>
<aside class="notes">
Bernoulli’s simple kinetic models of gases assume that the molecules of
air operate like billiard balls.
</aside>
</section>
<section id="section-74" class="slide level2">
<h2></h2>
<div class="figure">
<div id="gaussian-histogram-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//ml/gaussian-histogram.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
James Clerk Maxwell 1831-1879 Derived distribution of velocities of
particles in an ideal gas (elastic fluid).
</aside>
</section>
<section id="maxwells-demon" class="slide level2">
<h2>Maxwell’s Demon</h2>
<div class="figure">
<div id="maxwells-demon-figure" class="figure-frame">
<object class data="https://inverseprobability.com/talks/../slides/diagrams//physics/maxwells-demon.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Maxwell’s demon opens and closes a door which allows fast particles to
pass from left to right and slow particles to pass from right to left.
This makes the left hand side colder than the right.
</aside>
</section>
<section id="section-75" class="slide level2">
<h2></h2>
<div class="figure">
<div id="maxwells-demon-figure" class="figure-frame">
<div>
<div style="width:68%;float:left">
<canvas id="maxwell-canvas" width="700" height="500" style="border:1px solid black;display:inline;text-align:left">
</canvas>
</div>
<div style="width:28%;float:right;margin:auto">
<div style="float:right;width:100%;margin:auto">
Entropy:
<output id="maxwell-entropy">
</output>
</div>
<div id="maxwell-histogram-canvas"
style="width:300px;height:250px;display:inline-block;text-align:right;margin:auto">

</div>
</div>
</div>
<div>
<button id="maxwell-newball" style="text-align:right">
New Ball
</button>
<button id="maxwell-pause" style="text-align:right">
Pause
</button>
<button id="maxwell-skip" style="text-align:right">
Skip 1000s
</button>
<button id="maxwell-histogram" style="text-align:right">
Histogram
</button>
</div>
<script src="https://inverseprobability.com/talks/scripts//ballworld/maxwell.js"></script>
</div>
</div>
<aside class="notes">
Maxwell’s Demon. The demon decides balls are either cold (blue) or hot
(red) according to their velocity. Balls are allowed to pass the green
membrane from right to left only if they are cold, and from left to
right, only if they are hot.
</aside>
</section>
<section id="section-76" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pooh-rabbit-hoosh-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ml/E.-H.-Shepard_Two-ink-drawings-from-The-House-at-Pooh-Corner-I_.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Rabbit and Pooh watch the result of Pooh’s hooshing idea to move Eeyore
towards the shore.
</aside>
</section>
<section id="section-77" class="slide level2">
<h2></h2>
<blockquote>
<p>When you are a Bear of Very Little Brain, and you Think of Things,
you find sometimes that a Thing which seemed very Thingish inside you is
quite different when it gets out into the open and has other people
looking at it.</p>
<p>A.A. Milne as Winnie-the-Pooh in <em>The House at Pooh Corner</em>,
1928</p>
</blockquote>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level2">
<h2>What is Machine Learning?</h2>
</section>
<section id="what-is-machine-learning-1" class="slide level2">
<h2>What is Machine Learning?</h2>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively
acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience
(other data! transfer learning etc), or beliefs about the regularities
of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a
categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a
href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine
Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-2" class="slide level2">
<h2>What is Machine Learning?</h2>
<p><span class="math display">\[\text{data} + \text{model}
\stackrel{\text{compute}}{\rightarrow} \text{prediction}\]</span></p>
<ul>
<li class="fragment">To combine data with a model need:</li>
<li class="fragment"><strong>a prediction function</strong> <span
class="math inline">\(f(\cdot)\)</span> includes our beliefs about the
regularities of the universe</li>
<li class="fragment"><strong>an objective function</strong> <span
class="math inline">\(E(\cdot)\)</span> defines the cost of
misprediction.</li>
</ul>
</section>
<section id="kappenball" class="slide level2">
<h2>Kappenball</h2>
<div class="figure">
<div id="kappen-ball-figure" class="figure-frame">
<div>
<div style="width:800px;text-align:center;display:inline">
<span style="float:left;">Score:
<output id="kappenball-score">
</output>
</span> <span style="float:right;">Energy:
<output id="kappenball-energy">
</output>
</span>
<div style="clear: both;">

</div>
</div>
<canvas id="kappenball-canvas" width="800" height="450" style="border:1px solid black;display:inline;text-align:center ">
</canvas>
<div>
<input type="range" min="0" max="100" value="0" class="slider" id="kappenball-stochasticity" style="width:800px;"/>
</div>
<div>
<button id="kappenball-newball" style="text-align:right">
New Ball
</button>
<button id="kappenball-pause" style="text-align:right">
Pause
</button>
</div>
<output id="kappenball-count">
</output>
<script src="https://inverseprobability.com/talks/scripts//ballworld/kappenball.js"></script>
</div>
</div>
</div>
<aside class="notes">
Kappen Ball
</aside>
</section>
<section id="prime-air" class="slide level2">
<h2>Prime Air</h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip4">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Gur Kimchi
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/gur-kimchi.png" clip-path="url(#clip4)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip5">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Paul Viola
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/paul-viola.png" clip-path="url(#clip5)"/>
</svg>
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip6">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
David Moro
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="https://inverseprobability.com/talks/../slides/diagrams//people/david-moro.png" clip-path="url(#clip6)"/>
</svg>
</div>
<div class="figure">
<div id="amazon-drone-flight-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/3HJtmx5f1Fc?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
An actual ‘Santa’s sleigh’. Amazon’s prototype delivery drone. Machine
learning algorithms are used across various systems including sensing
(computer vision for detection of wires, people, dogs etc) and piloting.
The technology is necessarily a combination of old and new ideas. The
transition from vertical to horizontal flight is vital for efficiency
and uses sophisticated machine learning to achieve.
</aside>
</section>
<section id="section-78" class="slide level2">
<h2></h2>
<div class="figure">
<div id="jeff-wilke-remars-figure" class="figure-frame">
<iframe width="600" height="450" src="https://www.youtube.com/embed/wa8DU-Sui8Q?start=3767" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
</div>
<aside class="notes">
Jeff Wilke (CEO Amazon Consumer) announcing the new drone at the Amazon
2019 re:MARS event alongside the scale of the Amazon supply chain.
</aside>
</section>
<section id="section-79" class="slide level2">
<h2></h2>
<div class="figure">
<div id="amazon-prime-air-remars-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//ai/amazon-prime-air-remars-june-2019.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Picture of the drone from Amazon Re-MARS event in 2019.
</aside>
</section>
<section id="section-80" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Here we’re showing 20 samples taken from the prior over functions
defined by our covarariance
</aside>
</section>
<section id="section-81" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
We can sample many such functions, in this slide there are now 1000 in
total. This is a sample from our prior over functions.
</aside>
</section>
<section id="section-82" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Now we observe data. Here there are three data points. Conceptually in
Bayesian inference we discard all samples that are distant from the
data.
</aside>
</section>
<section id="section-83" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
Throwing away such samples we are left with our posterior. This is the
collection of samples from the prior that are consistent with the data.
</aside>
</section>
<section id="section-84" class="slide level2">
<h2></h2>
<div class="centered" style="">
<img class="negate" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<aside class="notes">
The elegance of the Gaussian process is that this result can be computed
analytically using linear algebra.
</aside>
</section>
<section id="deepface" class="slide level2">
<h2>DeepFace</h2>
<p><span class="fragment fade-in"><small>Outline of the DeepFace
architecture. A front-end of a single convolution-pooling-convolution
filtering on the rectified input, followed by three locally-connected
layers and two fully-connected layers. Color illustrates feature maps
produced at each layer. The net includes more than 120 million
parameters, where more than 95% come from the local and fully
connected.</small></span></p>
<div class="figure">
<div id="deep-face-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//deepface_neg.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
The DeepFace architecture <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span>,
visualized through colors to represent the functional mappings at each
layer. There are 120 million parameters in the model.
</aside>
<div style="text-align:right">
<small>Source: DeepFace <span class="citation"
data-cites="Taigman:deepface14">(Taigman et al., 2014)</span></small>
</div>
</section>
<section id="deep-learning-as-pinball" class="slide level2">
<h2>Deep Learning as Pinball</h2>
<div class="figure">
<div id="early-pinball-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//576px-Early_Pinball.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
Deep learning models are composition of simple functions. We can think
of a pinball machine as an analogy. Each layer of pins corresponds to
one of the layers of functions in the model. Input data is represented
by the location of the ball from left to right when it is dropped in
from the top. Output class comes from the position of the ball as it
leaves the pins at the bottom.
</aside>
</section>
<section id="section-85" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-initialization-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball001.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
At initialization, the pins, which represent the parameters of the
function, aren’t in the right place to bring the balls to the correct
decisions.
</aside>
</section>
<section id="section-86" class="slide level2">
<h2></h2>
<div class="figure">
<div id="pinball-trained-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//pinball002.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
After learning the pins are now in the right place to bring the balls to
the correct decisions.
</aside>
<!--Connect supply chain as a "challenge" tot he abstraction of Schroedinger's bridge. Link to Optimal Transport (matching without the "physics"). Maxwell's demon.-->
<!--Control ability paper with Mauricio and Simo??)-->
<!-- Interfaces AI for Science -->
<!--include{_ai/includes/interfaces-ai-for-science.md}-->
<!--include{_gp/includes/what-is-a-gp.md} -->
</section>
<section id="deep-neural-network" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn1.svg" width="50%" style=" ">
</object>
</section>
<section id="deep-neural-network-1" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn2.svg" width="50%" style=" ">
</object>
</section>
<section id="mathematically" class="slide level2">
<h2>Mathematically</h2>
<p><span class="math display">\[
\begin{align*}
    \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{W}_1 \mathbf{ x}\right)\\
    \mathbf{ h}_{2} &amp;=  \phi\left(\mathbf{W}_2\mathbf{
h}_{1}\right)\\
    \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{W}_3 \mathbf{
h}_{2}\right)\\
    f&amp;= \mathbf{ w}_4 ^\top\mathbf{ h}_{3}
\end{align*}
\]</span></p>
</section>
<section id="bottleneck-layers-in-deep-neural-networks"
class="slide level2">
<h2>Bottleneck Layers in Deep Neural Networks</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck1.svg" width="60%" style=" ">
</object>
</section>
<section id="deep-neural-network-2" class="slide level2">
<h2>Deep Neural Network</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/deep-nn-bottleneck2.svg" width="60%" style=" ">
</object>
</section>
<section id="mathematically-1" class="slide level2">
<h2>Mathematically</h2>
<p>The network can now be written mathematically as <span
class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ h}_{1} &amp;= \phi\left(\mathbf{U}_1 \mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \mathbf{ h}_{1}\\
  \mathbf{ h}_{2} &amp;= \phi\left(\mathbf{U}_2 \mathbf{ z}_{2}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \mathbf{ h}_{2}\\
  \mathbf{ h}_{3} &amp;= \phi\left(\mathbf{U}_3 \mathbf{ z}_{3}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4^\top\mathbf{ h}_{3}.
\end{align}
\]</span></p>
</section>
<section id="a-cascade-of-neural-networks" class="slide level2">
<h2>A Cascade of Neural Networks</h2>
<p><span class="math display">\[
\begin{align}
  \mathbf{ z}_{1} &amp;= \mathbf{V}^\top_1 \mathbf{ x}\\
  \mathbf{ z}_{2} &amp;= \mathbf{V}^\top_2 \phi\left(\mathbf{U}_1
\mathbf{ z}_{1}\right)\\
  \mathbf{ z}_{3} &amp;= \mathbf{V}^\top_3 \phi\left(\mathbf{U}_2
\mathbf{ z}_{2}\right)\\
  \mathbf{ y}&amp;= \mathbf{ w}_4 ^\top \mathbf{ z}_{3}
\end{align}
\]</span></p>
</section>
<section id="cascade-of-gaussian-processes" class="slide level2">
<h2>Cascade of Gaussian Processes</h2>
<ul>
<li><p>Replace each neural network with a Gaussian process <span
class="math display">\[
\begin{align}
\mathbf{ z}_{1} &amp;= \mathbf{ f}_1\left(\mathbf{ x}\right)\\
\mathbf{ z}_{2} &amp;= \mathbf{ f}_2\left(\mathbf{ z}_{1}\right)\\
\mathbf{ z}_{3} &amp;= \mathbf{ f}_3\left(\mathbf{ z}_{2}\right)\\
\mathbf{ y}&amp;= \mathbf{ f}_4\left(\mathbf{ z}_{3}\right)
\end{align}
\]</span></p></li>
<li><p>Equivalent to prior over parameters, take width of each layer to
infinity.</p></li>
</ul>
</section>
<section id="gpy-a-gaussian-process-framework-in-python"
class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<aside class="notes">
GPy is a BSD licensed software code base for implementing Gaussian
process models in Python. It is designed for teaching and modelling. We
welcome contributions which can be made through the GitHub repository <a
href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a>
</aside>
<center>
<a href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1"
class="slide level2">
<h2>GPy: A Gaussian Process Framework in Python</h2>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use
GPs.</li>
<li>Available through GitHub <a
href="https://github.com/SheffieldML/GPy"
class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level2">
<h2>Features</h2>
<ul>
<li>Probabilistic-style programming (specify the model, not the
algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="olympic-marathon-data" class="slide level2">
<h2>Olympic Marathon Data</h2>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardized distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organized leading to very slow
times.</li>
</ul>
</td>
<td width="30%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ"
class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level2">
<h2>Olympic Marathon Data</h2>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/olympic-marathon.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Olympic marathon pace times since 1896.
</aside>
</section>
<section id="alan-turing" class="slide level2">
<h2>Alan Turing</h2>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="https://inverseprobability.com/talks/../slides/diagrams//turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<aside class="notes">
Alan Turing, in 1946 he was only 11 minutes slower than the winner of
the 1948 games. Would he have won a hypothetical games held in 1946?
Source:
<a href="http://www.turing.org.uk/scrapbook/run.html" target="_blank">Alan
Turing Internet Scrapbook</a>.
</aside>
</section>
<section id="probability-winning-olympics" class="slide level2">
<h2>Probability Winning Olympics?</h2>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had
been held in 1946?</li>
</ul>
</section>
<section id="gaussian-process-fit" class="slide level2">
<h2>Gaussian Process Fit</h2>
</section>
<section id="olympic-marathon-data-gp" class="slide level2">
<h2>Olympic Marathon Data GP</h2>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/olympic-marathon-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the Olympic Marathon data. The error bars are
too large, perhaps due to the outlier from 1904.
</aside>
</section>
<section id="deep-gp-fit" class="slide level2">
<h2>Deep GP Fit</h2>
<ul>
<li><p>Can a Deep Gaussian process help?</p></li>
<li><p>Deep GP is one GP feeding into another.</p></li>
</ul>
</section>
<section id="olympic-marathon-data-deep-gp" class="slide level2">
<h2>Olympic Marathon Data Deep GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp.svg" width="100%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep GP fit to the Olympic marathon data. Error bars now change as the
prediction evolves.
</aside>
</section>
<section id="olympic-marathon-data-deep-gp-1" class="slide level2">
<h2>Olympic Marathon Data Deep GP</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-samples.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
Point samples run through the deep Gaussian process show the
distribution of output locations.
</aside>
</section>
<section id="olympic-marathon-data-latent-1" class="slide level2">
<h2>Olympic Marathon Data Latent 1</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-0-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-0.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from input to the latent layer is broadly, with some
flattening as time goes on. Variance is high across the input range.
</aside>
</section>
<section id="olympic-marathon-data-latent-2" class="slide level2">
<h2>Olympic Marathon Data Latent 2</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-layer-1-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-layer-1.svg" width style=" ">
</object>
</div>
</div>
<aside class="notes">
The mapping from the latent layer to the output layer.
</aside>
</section>
<section id="olympic-marathon-pinball-plot" class="slide level2">
<h2>Olympic Marathon Pinball Plot</h2>
<div class="figure">
<div id="olympic-marathon-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/olympic-marathon-deep-gp-pinball.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
A pinball plot shows the movement of the ‘ball’ as it passes through
each layer of the Gaussian processes. Mean directions of movement are
shown by lines. Shading gives one standard deviation of movement
position. At each layer, the uncertainty is reset. The overal
uncertainty is the cumulative uncertainty from all the layers. There is
some grouping of later points towards the right in the first layer,
which also injects a large amount of uncertainty. Due to flattening of
the curve in the second layer towards the right the uncertainty is
reduced in final output.
</aside>
</section>
<section id="step-function-data" class="slide level2">
<h2>Step Function Data</h2>
<div class="figure">
<div id="step-function-data-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//datasets/step-function.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Simulation study of step function data artificially generated. Here
there is a small overlap between the two lines.
</aside>
</section>
<section id="step-function-data-gp" class="slide level2">
<h2>Step Function Data GP</h2>
<div class="figure">
<div id="step-function-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//gp/step-function-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Gaussian process fit to the step function data. Note the large error
bars and the over-smoothing of the discontinuity. Error bars are shown
at two standard deviations.
</aside>
</section>
<section id="step-function-data-deep-gp" class="slide level2">
<h2>Step Function Data Deep GP</h2>
<div class="figure">
<div id="step-function-deep-gp-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Deep Gaussian process fit to the step function data.
</aside>
</section>
<section id="step-function-data-deep-gp-1" class="slide level2">
<h2>Step Function Data Deep GP</h2>
<div class="figure">
<div id="step-function-deep-gp-samples-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-samples.svg" width="80%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Samples from the deep Gaussian process model for the step function fit.
</aside>
</section>
<section id="step-function-data-latent-1" class="slide level2">
<h2>Step Function Data Latent 1</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-0.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-2" class="slide level2">
<h2>Step Function Data Latent 2</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-1.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-3" class="slide level2">
<h2>Step Function Data Latent 3</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-2.svg" width style=" ">
</object>
</section>
<section id="step-function-data-latent-4" class="slide level2">
<h2>Step Function Data Latent 4</h2>
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-layer-3.svg" width style=" ">
</object>
</section>
<section id="step-function-pinball-plot" class="slide level2">
<h2>Step Function Pinball Plot</h2>
<div class="figure">
<div id="step-function-deep-gp-pinball-figure" class="figure-frame">
<object class="svgplot " data="https://inverseprobability.com/talks/../slides/diagrams//deepgp/step-function-deep-gp-pinball.svg" width="60%" style=" ">
</object>
</div>
</div>
<aside class="notes">
Pinball plot of the deep GP fitted to the step function data. Each layer
of the model pushes the ‘ball’ towards the left or right, saturating at
1 and 0. This causes the final density to be be peaked at 0 and 1.
Transitions occur driven by the uncertainty of the mapping in each
layer.
</aside>
</section>
<section id="the-future" class="slide level2">
<h2>The Future</h2>
<ul>
<li>Uncertainty important …</li>
<li>But also interaction (intervention, causality)</li>
</ul>
</section>
<section id="conclusions-1" class="slide level2">
<h2>Conclusions</h2>
<ul>
<li>HAMs change how we share ambiguous information.</li>
<li>We need to think about how that effects our sharing of
probabilities.</li>
<li>May allow the machine to be used more widely as a tool.</li>
</ul>
</section>
<section id="thanks" class="slide level2 scrollable">
<h2 class="scrollable">Thanks!</h2>
<ul>
<li><p>book: <a
href="https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248">The
Atomic Human</a></p></li>
<li><p>twitter: <a
href="https://twitter.com/lawrennd">@lawrennd</a></p></li>
<li><p>podcast: <a href="http://thetalkingmachines.com">The Talking
Machines</a></p></li>
<li><p>newspaper: <a
href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile
Page</a></p></li>
<li><p>blog posts:</p>
<p><a
href="http://inverseprobability.com/2018/02/06/natural-and-artificial-intelligence">Natural
and Artificial Intelligence</a></p>
<p><a
href="http://inverseprobability.com/2016/11/19/lies-damned-lies-big-data">Lies,
Damned Lies and Big Data</a></p>
<p><a
href="http://inverseprobability.com/2017/07/17/what-is-machine-learning">What
is Machine Learning?</a></p></li>
</ul>
</section>
<section id="references" class="slide level2 unnumbered scrollable">
<h2 class="unnumbered scrollable">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
role="list">
<div id="ref-Brooks:mythical75" class="csl-entry" role="listitem">
Brooks, F., n.d. The mythical man-month. Addison-Wesley.
</div>
<div id="ref-Cabrera-realworld23" class="csl-entry" role="listitem">
Cabrera, C., Paleyes, A., Thodoroff, P., Lawrence, N.D., 2023. <a
href="https://arxiv.org/abs/2302.04810">Real-world machine learning
systems: A survey from a data-oriented architecture perspective</a>.
</div>
<div id="ref-Coales-yellow14" class="csl-entry" role="listitem">
Coales, J.F., Kane, S.J., 2014. The <span>“yellow peril”</span> and
after. IEEE Control Systems Magazine 34, 65–69. <a
href="https://doi.org/10.1109/MCS.2013.2287387">https://doi.org/10.1109/MCS.2013.2287387</a>
</div>
<div id="ref-Einstein-brownian05" class="csl-entry" role="listitem">
Einstein, A., 1905. Über die von der molekularkinetischen
<span>T</span>heorie der <span>W</span>ärme geforderte
<span>B</span>ewegung von in ruhenden <span>F</span>lüssigkeiten
suspendierten <span>T</span>eilchen. Annalen der Physik 322, 549–560. <a
href="https://doi.org/10.1002/andp.19053220806">https://doi.org/10.1002/andp.19053220806</a>
</div>
<div id="ref-Huang-inner22" class="csl-entry" role="listitem">
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng,
A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T.,
Brown, N., Luu, L., Levine, S., Hausman, K., ichter, brian, 2023. <a
href="https://proceedings.mlr.press/v205/huang23c.html">Inner monologue:
Embodied reasoning through planning with language models</a>, in: Liu,
K., Kulic, D., Ichnowski, J. (Eds.), Proceedings of the 6th Conference
on Robot Learning, Proceedings of Machine Learning Research. PMLR, pp.
1769–1782.
</div>
<div id="ref-Laplace-essai14" class="csl-entry" role="listitem">
Laplace, P.S., 1814. Essai philosophique sur les probabilités, 2nd ed.
Courcier, Paris.
</div>
<div id="ref-Lawrence:embodiment17" class="csl-entry" role="listitem">
Lawrence, N.D., 2017. <a href="https://arxiv.org/abs/1705.07996">Living
together: Mind and machine intelligence</a>. arXiv.
</div>
<div id="ref-Lawrence:licsbintro10" class="csl-entry" role="listitem">
Lawrence, N.D., 2010. Introduction to learning and inference in
computational systems biology.
</div>
<div id="ref-Mackay-behind91" class="csl-entry" role="listitem">
MacKay, D.M., 1991. Behind the eye. Basil Blackwell.
</div>
<div id="ref-ONeill-trust02" class="csl-entry" role="listitem">
O’Neill, O., 2002. A question of trust. Cambridge University Press.
</div>
<div id="ref-Roh-cryo-em20" class="csl-entry" role="listitem">
Roh, S.-H., Shekhar, M., Pintilie, G., Chipot, C., Wilkens, S.,
Singharoy, A., Chiu, W., 2020. <span>Cryo-EM</span> and <span>MD</span>
infer water-mediated proton transport and autoinhibition mechanisms of
<span>Vo</span> complex. Science Advances 6. <a
href="https://doi.org/10.1126/sciadv.abb9605">https://doi.org/10.1126/sciadv.abb9605</a>
</div>
<div id="ref-Sculley:debt15" class="csl-entry" role="listitem">
Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner,
D., Chaudhary, V., Young, M., Crespo, J.-F., Dennison, D., 2015. <a
href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden
technical debt in machine learning systems</a>, in: Cortes, C.,
Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R. (Eds.), Advances in
Neural Information Processing Systems 28. Curran Associates, Inc., pp.
2503–2511.
</div>
<div id="ref-Susskind-future15" class="csl-entry" role="listitem">
Susskind, R.E., Susskind, D., 2015. The future of the professions: How
technology will transform the work of human experts. Oxford University
Press.
</div>
<div id="ref-Taigman:deepface14" class="csl-entry" role="listitem">
Taigman, Y., Yang, M., Ranzato, M., Wolf, L., 2014.
<span>DeepFace</span>: Closing the gap to human-level performance in
face verification, in: Proceedings of the <span>IEEE</span> Computer
Society Conference on Computer Vision and Pattern Recognition. <a
href="https://doi.org/10.1109/CVPR.2014.220">https://doi.org/10.1109/CVPR.2014.220</a>
</div>
<div id="ref-Admiralty-gunnery45" class="csl-entry" role="listitem">
The Admiralty, 1945. <a href="https://www.maritime.org/doc/br224/">The
gunnery pocket book, b.r. 224/45</a>.
</div>
<div id="ref-Thompson-juries89" class="csl-entry" role="listitem">
Thompson, W.C., 1989. <a href="http://www.jstor.org/stable/1191906">Are
juries competent to evaluate statistical evidence?</a> Law and
Contemporary Problems 52, 9–41.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2/lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/zoom-js/zoom.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/math/math.js', async: true },
          { src: 'https://unpkg.com/reveal.js@3.9.2/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
