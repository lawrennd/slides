<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2015-11-03">
  <title>Bayesian Regression</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="figure-animate.js"></script>
</head>
<body>
\[\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Bayesian Regression</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2015-11-03</time></p>
  <p class="venue" style="text-align:center">University of Sheffield</p>
</section>

<section class="slide level3">

<!-- Front matter -->
<!---->
<!--Back matter-->
<p>.</p>
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="overdetermined-system" class="slide level3">
<h3>Overdetermined System</h3>
<ul>
<li>With two unknowns and two observations: <span class="math display">\[
  \begin{aligned}
  \dataScalar_1 = &amp; m\inputScalar_1 + c\\
  \dataScalar_2 = &amp; m\inputScalar_2 + c
  \end{aligned}
  \]</span></li>
<li>Additional observation leads to <em>overdetermined</em> system. <span class="math display">\[
  \dataScalar_3 =  m\inputScalar_3 + c
  \]</span></li>
<li>This problem is solved through a noise model <span class="math inline">\(\noiseScalar \sim \gaussianSamp{0}{\dataStd^2}\)</span> <span class="math display">\[
  \begin{aligned}
  \dataScalar_1 = m\inputScalar_1 + c + \noiseScalar_1\\
      \dataScalar_2 = m\inputScalar_2 + c + \noiseScalar_2\\
      \dataScalar_3 = m\inputScalar_3 + c + \noiseScalar_3
  \end{aligned}
  \]</span></li>
</ul>
<!-- A system of two simultaneous equations with two unknowns. -->
<!-- How do we deal with three simultaneous equations with only two unknowns? -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   \dataScalar_1 = & m\inputScalar_1 + c\\ -->
<!--   \dataScalar_2 = & m\inputScalar_2 + c -->
<!-- \end{aligned} -->
<!-- $$  -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   \dataScalar_1-\dataScalar_2 = & m(\inputScalar_1 - \inputScalar_2) -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--  \frac{\dataScalar_1-\dataScalar_2}{\inputScalar_1 - \inputScalar_2} = & m -->
<!-- \end{aligned} -->
<!-- $$  -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   m & =\frac{\dataScalar_2-\dataScalar_1}{\inputScalar_2 - \inputScalar_1}\\ -->
<!--   c & = \dataScalar_1 - m \inputScalar_1 -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!--   \dataScalar_1 = & m\inputScalar_1 + c\\ -->
<!--   \dataScalar_2 = & m\inputScalar_2 + c\\ -->
<!--   \dataScalar_3 = & m\inputScalar_3 + c -->
<!-- \end{aligned} -->
<!-- $$ -->
<!--  -->
<!-- SECTION Underdetermined System -->
</section>
<section id="underdetermined-system" class="slide level3">
<h3>Underdetermined System</h3>
<ul>
<li>What about two unknowns and <em>one</em> observation? <span class="math display">\[\dataScalar_1 =  m\inputScalar_1 + c\]</span></li>
</ul>
<div class="fragment">
<p>Can compute <span class="math inline">\(m\)</span> given <span class="math inline">\(c\)</span>. <span class="math display">\[m = \frac{\dataScalar_1 - c}{\inputScalar}\]</span></p>
</div>
</section>
<section id="underdetermined-system-1" class="slide level3">
<h3>Underdetermined System</h3>
<script>
showDivs(0, 'under_determined_system');
</script>
<small></small> <input id="range-under_determined_system" type="range" min="0" max="9" value="0" onchange="setDivs('under_determined_system')" oninput="setDivs('under_determined_system')">
<button onclick="plusDivs(-1, 'under_determined_system')">
❮
</button>
<button onclick="plusDivs(1, 'under_determined_system')">
❯
</button>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system000.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system001.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system002.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system003.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system004.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system005.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system006.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system007.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system008.svg" style="vertical-align:middle;">
</object>
</div>
<div class="under_determined_system" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/under_determined_system009.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="the-bayesian-controversy-philosophical-underpinnings" class="slide level3">
<h3>The Bayesian Controversy: Philosophical Underpinnings</h3>
<p>A segment from the lecture in 2012 on philsophical underpinnings.</p>
<iframe width="1024" height="768" src="https://www.youtube.com/embed/AvlnFnvFw_0?start=1215" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</section>
<section id="noise-models" class="slide level3">
<h3>Noise Models</h3>
<ul>
<li>We aren’t modeling entire system.</li>
<li>Noise model gives mismatch between model and data.</li>
<li>Gaussian model justified by appeal to central limit theorem.</li>
<li>Other models also possible (Student-<span class="math inline">\(t\)</span> for heavy tails).</li>
<li>Maximum likelihood with Gaussian noise leads to <em>least squares</em>.</li>
</ul>
</section>
<section id="different-types-of-uncertainty" class="slide level3">
<h3>Different Types of Uncertainty</h3>
<ul>
<li>The first type of uncertainty we are assuming is <em>aleatoric</em> uncertainty.</li>
<li>The second type of uncertainty we are assuming is <em>epistemic</em> uncertainty.</li>
</ul>
</section>
<section id="aleatoric-uncertainty" class="slide level3">
<h3>Aleatoric Uncertainty</h3>
<ul>
<li>This is uncertainty we couldn’t know even if we wanted to. e.g. the result of a football match before it’s played.</li>
<li>Where a sheet of paper might land on the floor.</li>
</ul>
</section>
<section id="epistemic-uncertainty" class="slide level3">
<h3>Epistemic Uncertainty</h3>
<ul>
<li>This is uncertainty we could in principal know the answer too. We just haven’t observed enough yet, e.g. the result of a football match <em>after</em> it’s played.</li>
<li>What colour socks your lecturer is wearing.</li>
</ul>
</section>
<section id="reading" class="slide level3">
<h3>Reading</h3>
<ul>
<li><span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> Section 1.2.3 (pg 21–24).</li>
<li><span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> Section 1.2.6 (start from just past eq 1.64 pg 30-32).</li>
<li>Bayesian Inference</li>
<li><span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span> use an example of a coin toss for introducing Bayesian inference Chapter 3, Sections 3.1-3.4 (pg 95-117). Although you also need the beta density which we haven’t yet discussed. This is also the example that <span class="citation" data-cites="Laplace:memoire74">Laplace (1774)</span> used.</li>
<li><span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> Section 1.2.3 (pg 21–24).</li>
<li><span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> Section 1.2.6 (start from just past eq 1.64 pg 30-32).</li>
</ul>
</section>
<section id="prior-distribution" class="slide level3">
<h3>Prior Distribution</h3>
<ul>
<li>Bayesian inference requires a prior on the parameters.</li>
<li>The prior represents your belief <em>before</em> you see the data of the likely value of the parameters.</li>
<li>For linear regression, consider a Gaussian prior on the intercept:</li>
</ul>
<p><span class="math display">\[c \sim \gaussianSamp{0}{\alpha_1}\]</span></p>
</section>
<section id="posterior-distribution" class="slide level3">
<h3>Posterior Distribution</h3>
<ul>
<li>Posterior distribution is found by combining the prior with the likelihood.</li>
<li>Posterior distribution is your belief <em>after</em> you see the data of the likely value of the parameters.</li>
<li>The posterior is found through <strong>Bayes’ Rule</strong> <span class="math display">\[
  p(c|\dataScalar) = \frac{p(\dataScalar|c)p(c)}{p(\dataScalar)}
  \]</span></li>
</ul>
<p><span class="math display">\[
  \text{posterior} = \frac{\text{likelihood}\times \text{prior}}{\text{marginal likelihood}}.
  \]</span></p>
</section>
<section id="bayes-update" class="slide level3">
<h3>Bayes Update</h3>
<script>
showDivs(1, 'dem_gaussian');
</script>
<small></small> <input id="range-dem_gaussian" type="range" min="1" max="3" value="1" onchange="setDivs('dem_gaussian')" oninput="setDivs('dem_gaussian')">
<button onclick="plusDivs(-1, 'dem_gaussian')">
❮
</button>
<button onclick="plusDivs(1, 'dem_gaussian')">
❯
</button>
<div class="dem_gaussian" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/dem_gaussian001.svg" style="vertical-align:middle;">
</object>
</div>
<div class="dem_gaussian" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/dem_gaussian002.svg" style="vertical-align:middle;">
</object>
</div>
<div class="dem_gaussian" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/dem_gaussian003.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="stages-to-derivation-of-the-posterior" class="slide level3">
<h3>Stages to Derivation of the Posterior</h3>
<ul>
<li>Multiply likelihood by prior</li>
<li>they are “exponentiated quadratics”, the answer is always also an exponentiated quadratic because <span class="math inline">\(\exp(a^2)\exp(b^2) = \exp(a^2 + b^2)\)</span>.</li>
<li>Complete the square to get the resulting density in the form of a Gaussian.</li>
<li>Recognise the mean and (co)variance of the Gaussian. This is the estimate of the posterior.</li>
</ul>
</section>
<section id="main-trick" class="slide level3">
<h3>Main Trick</h3>
<p><span class="math display">\[p(c) = \frac{1}{\sqrt{2\pi\alpha_1}} \exp\left(-\frac{1}{2\alpha_1}c^2\right)\]</span> <span class="math display">\[p(\dataVector|\inputVector, c, m, \dataStd^2) = \frac{1}{\left(2\pi\dataStd^2\right)^{\frac{\numData}{2}}} \exp\left(-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i - m\inputScalar_i - c)^2\right)\]</span></p>
</section>
<section id="section" class="slide level3">
<h3></h3>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) = \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{p(\dataVector|\inputVector, m, \dataStd^2)}\]</span></p>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) =  \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{\int p(\dataVector|\inputVector, c, m, \dataStd^2)p(c) \text{d} c}\]</span></p>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) \propto  p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)\]</span></p>
<p><span class="math display">\[\begin{aligned}
    \log p(c | \dataVector, \inputVector, m, \dataStd^2) =&amp;-\frac{1}{2\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-c - m\inputScalar_i)^2-\frac{1}{2\alpha_1} c^2 + \text{const}\\
     = &amp;-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)^2 -\left(\frac{\numData}{2\dataStd^2} + \frac{1}{2\alpha_1}\right)c^2\\
    &amp; + c\frac{\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)}{\dataStd^2},
  \end{aligned}\]</span></p>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<p>complete the square of the quadratic form to obtain <span class="math display">\[\log p(c | \dataVector, \inputVector, m, \dataStd^2) = -\frac{1}{2\tau^2}(c - \mu)^2 +\text{const},\]</span> where <span class="math inline">\(\tau^2 = \left(\numData\dataStd^{-2} +\alpha_1^{-1}\right)^{-1}\)</span> and <span class="math inline">\(\mu = \frac{\tau^2}{\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)\)</span>.</p>
</section>
<section id="main-trick-1" class="slide level3">
<h3>Main Trick</h3>
<p><span class="math display">\[
p(c) = \frac{1}{\sqrt{2\pi\alpha_1}} \exp\left(-\frac{1}{2\alpha_1}c^2\right)
\]</span> <span class="math display">\[
p(\dataVector|\inputVector, c, m, \dataStd^2) = \frac{1}{\left(2\pi\dataStd^2\right)^{\frac{\numData}{2}}} \exp\left(-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i - m\inputScalar_i - c)^2\right)
\]</span> <span class="math display">\[
p(c| \dataVector, \inputVector, m, \dataStd^2) = \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{p(\dataVector|\inputVector, m, \dataStd^2)}
\]</span> <span class="math display">\[
p(c| \dataVector, \inputVector, m, \dataStd^2) =  \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{\int p(\dataVector|\inputVector, c, m, \dataStd^2)p(c) \text{d} c}
\]</span> <span class="math display">\[
p(c| \dataVector, \inputVector, m, \dataStd^2) \propto  p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)
\]</span> <span class="math display">\[
\begin{aligned}
\log p(c | \dataVector, \inputVector, m, \dataStd^2) =&amp;-\frac{1}{2\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-c - m\inputScalar_i)^2-\frac{1}{2\alpha_1} c^2 + \text{const}\\
     = &amp;-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)^2 -\left(\frac{\numData}{2\dataStd^2} + \frac{1}{2\alpha_1}\right)c^2\\
     &amp; + c\frac{\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)}{\dataStd^2}, 
\end{aligned}
\]</span> complete the square of the quadratic form to obtain <span class="math display">\[
\log p(c | \dataVector, \inputVector, m, \dataStd^2) = -\frac{1}{2\tau^2}(c - \mu)^2 +\text{const},
\]</span> where <span class="math inline">\(\tau^2 = \left(n\dataStd^{-2} +\alpha_1^{-1}\right)^{-1}\)</span> and <span class="math inline">\(\mu = \frac{\tau^2}{\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)\)</span>.</p>
</section>
<section id="the-joint-density" class="slide level3">
<h3>The Joint Density</h3>
<ul>
<li>Really want to know the <em>joint</em> posterior density over the parameters <span class="math inline">\(c\)</span> <em>and</em> <span class="math inline">\(m\)</span>.</li>
<li>Could now integrate out over <span class="math inline">\(m\)</span>, but it’s easier to consider the multivariate case.</li>
</ul>
</section>
<section id="two-dimensional-gaussian" class="slide level3">
<h3>Two Dimensional Gaussian</h3>
<ul>
<li>Consider height, <span class="math inline">\(h/m\)</span> and weight, <span class="math inline">\(w/kg\)</span>.</li>
<li>Could sample height from a distribution: <span class="math display">\[
  p(h) \sim \gaussianSamp{1.7}{0.0225}.
  \]</span></li>
<li>And similarly weight: <span class="math display">\[
  p(w) \sim \gaussianSamp{75}{36}.
  \]</span></li>
</ul>
</section>
<section id="height-and-weight-models" class="slide level3">
<h3>Height and Weight Models</h3>
<div class="figure">
<div id="height-weight-gaussian-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/height_weight_gaussian.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="independence-assumption" class="slide level3">
<h3>Independence Assumption</h3>
<ul>
<li>We assume height and weight are independent.</li>
</ul>
<p><span class="math display">\[
  p(w, h) = p(w)p(h).
  \]</span></p>
</section>
<section id="sampling-two-dimensional-variables" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>
<script>
showDivs(0, 'independent_height_weight');
</script>
<small></small> <input id="range-independent_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('independent_height_weight')" oninput="setDivs('independent_height_weight')">
<button onclick="plusDivs(-1, 'independent_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'independent_height_weight')">
❯
</button>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight000.svg" style="vertical-align:middle;">
</object>
</div>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight001.svg" style="vertical-align:middle;">
</object>
</div>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight002.svg" style="vertical-align:middle;">
</object>
</div>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight003.svg" style="vertical-align:middle;">
</object>
</div>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight004.svg" style="vertical-align:middle;">
</object>
</div>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight005.svg" style="vertical-align:middle;">
</object>
</div>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight006.svg" style="vertical-align:middle;">
</object>
</div>
<div class="independent_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/independent_height_weight007.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="body-mass-index" class="slide level3">
<h3>Body Mass Index</h3>
<ul>
<li>In reality they are dependent (body mass index) <span class="math inline">\(= \frac{w}{h^2}\)</span>.</li>
<li>To deal with this dependence we introduce <em>correlated</em> multivariate Gaussians.</li>
</ul>
</section>
<section id="sampling-two-dimensional-variables-1" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>
<script>
showDivs(0, 'correlated_height_weight');
</script>
<small></small> <input id="range-correlated_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('correlated_height_weight')" oninput="setDivs('correlated_height_weight')">
<button onclick="plusDivs(-1, 'correlated_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'correlated_height_weight')">
❯
</button>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight000.svg" style="vertical-align:middle;">
</object>
</div>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight001.svg" style="vertical-align:middle;">
</object>
</div>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight002.svg" style="vertical-align:middle;">
</object>
</div>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight003.svg" style="vertical-align:middle;">
</object>
</div>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight004.svg" style="vertical-align:middle;">
</object>
</div>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight005.svg" style="vertical-align:middle;">
</object>
</div>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight006.svg" style="vertical-align:middle;">
</object>
</div>
<div class="correlated_height_weight" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/correlated_height_weight007.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="independent-gaussians" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = p(w)p(h)
\]</span></p>
</section>
<section id="independent-gaussians-1" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi \dataStd_1^2}\sqrt{2\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\frac{(w-\meanScalar_1)^2}{\dataStd_1^2} + \frac{(h-\meanScalar_2)^2}{\dataStd_2^2}\right)\right)
\]</span></p>
</section>
<section id="independent-gaussians-2" class="slide level3">
<h3>Independent Gaussians</h3>
<p><small> <span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi\dataStd_1^22\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)^\top\begin{bmatrix}\dataStd_1^2&amp; 0\\0&amp;\dataStd_2^2\end{bmatrix}^{-1}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)\right)
\]</span> </small></p>
</section>
<section id="independent-gaussians-3" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi \mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-1" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)^\top\mathbf{D}^{-1}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-2" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\rotationMatrix\mathbf{D}^{-1}\rotationMatrix^\top(\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix^{-1} = \rotationMatrix \mathbf{D}^{-1} \rotationMatrix^\top
\]</span></p>
</section>
<section id="correlated-gaussian-3" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\covarianceMatrix}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\covarianceMatrix^{-1} (\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix = \rotationMatrix \mathbf{D} \rotationMatrix^\top
\]</span></p>
</section>
<section id="the-prior-density" class="slide level3">
<h3>The Prior Density</h3>
<p>Let’s assume that the prior density is given by a zero mean Gaussian, which is independent across each of the parameters, <span class="math display">\[
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha \eye}
\]</span> In other words, we are assuming, for the prior, that each element of the parameters vector, <span class="math inline">\(\mappingScalar_i\)</span>, was drawn from a Gaussian density as follows <span class="math display">\[
\mappingScalar_i \sim \gaussianSamp{0}{\alpha}
\]</span> Let’s start by assigning the parameter of the prior distribution, which is the variance of the prior distribution, <span class="math inline">\(\alpha\)</span>.</p>
</section>
<section id="reading-1" class="slide level3">
<h3>Reading</h3>
<ul>
<li><p>Section 2.3 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> up to top of pg 85 (multivariate Gaussians).</p></li>
<li><p>Section 3.3 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> up to 159 (pg 152–159).</p></li>
</ul>
</section>
<section id="revisit-olympics-data" class="slide level3">
<h3>Revisit Olympics Data</h3>
<ul>
<li><p>Use Bayesian approach on olympics data with polynomials.</p></li>
<li><p>Choose a prior <span class="math inline">\(\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha \eye}\)</span> with <span class="math inline">\(\alpha = 1\)</span>.</p></li>
<li><p>Choose noise variance <span class="math inline">\(\dataStd^2 = 0.01\)</span></p></li>
</ul>
</section>
<section id="sampling-the-prior" class="slide level3">
<h3>Sampling the Prior</h3>
<ul>
<li>Always useful to perform a ‘sanity check’ and sample from the prior before observing the data.</li>
<li>Since <span class="math inline">\(\dataVector = \basisMatrix \mappingVector + \noiseVector\)</span> just need to sample <span class="math display">\[
  \mappingVector \sim \gaussianSamp{0}{\alpha\eye}
  \]</span> <span class="math display">\[
  \noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2}
  \]</span> with <span class="math inline">\(\alpha=1\)</span> and <span class="math inline">\(\dataStd^2 = 0.01\)</span>.</li>
</ul>
</section>
<section id="computing-the-posterior" class="slide level3">
<h3>Computing the Posterior</h3>
<p><span class="math display">\[
p(\mappingVector | \dataVector, \inputVector, \dataStd^2) = \gaussianDist{\mappingVector}{\meanVector_\mappingScalar}{\covarianceMatrix_\mappingScalar}
\]</span> with <span class="math display">\[
\covarianceMatrix_\mappingScalar = \left(\dataStd^{-2}\basisMatrix^\top \basisMatrix + \alpha^{-1}\eye\right)^{-1}
\]</span> and <span class="math display">\[
\meanVector_\mappingScalar = \covarianceMatrix_\mappingScalar \dataStd^{-2}\basisMatrix^\top \dataVector
\]</span></p>
</section>
<section id="olympic-data-with-bayesian-polynomials" class="slide level3">
<h3>Olympic Data with Bayesian Polynomials</h3>
<script>
showDivs(1, 'olympic_BLM_polynomial_number');
</script>
<small></small> <input id="range-olympic_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_BLM_polynomial_number')" oninput="setDivs('olympic_BLM_polynomial_number')">
<button onclick="plusDivs(-1, 'olympic_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_BLM_polynomial_number')">
❯
</button>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number001.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number002.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number003.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number004.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number005.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number006.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number007.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number008.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number009.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number010.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number011.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number012.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number013.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number014.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number015.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number016.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number017.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number018.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number019.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number020.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number021.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number022.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number023.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number024.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number025.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_BLM_polynomial_number026.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="hold-out-validation" class="slide level3">
<h3>Hold Out Validation</h3>
<script>
showDivs(1, 'olympic_val_BLM_polynomial_number');
</script>
<small></small> <input id="range-olympic_val_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_val_BLM_polynomial_number')" oninput="setDivs('olympic_val_BLM_polynomial_number')">
<button onclick="plusDivs(-1, 'olympic_val_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_val_BLM_polynomial_number')">
❯
</button>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number001.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number002.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number003.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number004.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number005.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number006.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number007.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number008.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number009.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number010.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number011.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number012.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number013.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number014.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number015.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number016.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number017.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number018.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number019.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number020.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number021.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number022.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number023.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number024.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number025.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_val_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number026.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="fold-cross-validation" class="slide level3">
<h3>5-fold Cross Validation</h3>
<script>
showDivs(1, 'olympic_5cv05_BLM_polynomial_number');
</script>
<small></small> <input id="range-olympic_5cv05_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_5cv05_BLM_polynomial_number')" oninput="setDivs('olympic_5cv05_BLM_polynomial_number')">
<button onclick="plusDivs(-1, 'olympic_5cv05_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_5cv05_BLM_polynomial_number')">
❯
</button>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number001.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number002.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number003.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number004.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number005.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number006.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number007.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number008.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number009.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number010.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number011.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number012.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number013.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number014.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number015.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number016.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number017.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number018.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number019.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number020.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number021.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number022.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number023.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number024.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number025.svg" style="vertical-align:middle;">
</object>
</div>
<div class="olympic_5cv05_BLM_polynomial_number" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number026.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="model-fit" class="slide level3">
<h3>Model Fit</h3>
<ul>
<li>Marginal likelihood doesn’t always increase as model order increases.</li>
<li>Bayesian model always has 2 parameters, regardless of how many basis functions (and here we didn’t even fit them).</li>
<li>Maximum likelihood model over fits through increasing number of parameters.</li>
<li>Revisit maximum likelihood solution with validation set.</li>
</ul>
</section>
<section id="regularized-mean" class="slide level3">
<h3>Regularized Mean</h3>
<ul>
<li>Validation fit here based on mean solution for <span class="math inline">\(\mappingVector\)</span> only.</li>
<li>For Bayesian solution <span class="math display">\[
  \meanVector_w = \left[\dataStd^{-2}\basisMatrix^\top\basisMatrix + \alpha^{-1}\eye\right]^{-1} \dataStd^{-2} \basisMatrix^\top \dataVector
  \]</span> instead of <span class="math display">\[
  \mappingVector^* = \left[\basisMatrix^\top\basisMatrix\right]^{-1} \basisMatrix^\top \dataVector
  \]</span></li>
<li>Two are equivalent when <span class="math inline">\(\alpha \rightarrow \infty\)</span>.</li>
<li>Equivalent to a prior for <span class="math inline">\(\mappingVector\)</span> with infinite variance.</li>
<li>In other cases <span class="math inline">\(\alpha \eye\)</span> <em>regularizes</em> the system (keeps parameters smaller).</li>
</ul>
</section>
<section id="sampling-from-the-posterior" class="slide level3">
<h3>Sampling from the Posterior</h3>
<ul>
<li>Now check samples by extracting <span class="math inline">\(\mappingVector\)</span> from the <em>posterior</em>.</li>
<li>Now for <span class="math inline">\(\dataVector = \basisMatrix \mappingVector + \noiseVector\)</span> need <span class="math display">\[
  \mappingVector \sim \gaussianSamp{\meanVector_w}{\covarianceMatrix_w}
  \]</span> with <span class="math inline">\(\covarianceMatrix_w = \left[\dataStd^{-2}\basisMatrix^\top \basisMatrix + \alpha^{-1}\eye\right]^{-1}\)</span> and <span class="math inline">\(\meanVector_w =\covarianceMatrix_w \dataStd^{-2} \basisMatrix^\top \dataVector\)</span> <span class="math display">\[
  \noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}
  \]</span> with <span class="math inline">\(\alpha=1\)</span> and <span class="math inline">\(\dataStd^2 = 0.01\)</span>.</li>
</ul>
</section>
<section id="marginal-likelihood" class="slide level3">
<h3>Marginal Likelihood</h3>
<ul>
<li><p>The marginal likelihood can also be computed, it has the form: <span class="math display">\[
  p(\dataVector|\inputMatrix, \dataStd^2, \alpha) = \frac{1}{(2\pi)^\frac{n}{2}\left|\kernelMatrix\right|^\frac{1}{2}} \exp\left(-\frac{1}{2} \dataVector^\top \kernelMatrix^{-1} \dataVector\right)
  \]</span> where <span class="math inline">\(\kernelMatrix = \alpha \basisMatrix\basisMatrix^\top + \dataStd^2 \eye\)</span>.</p></li>
<li><p>So it is a zero mean <span class="math inline">\(\numData\)</span>-dimensional Gaussian with covariance matrix <span class="math inline">\(\kernelMatrix\)</span>.</p></li>
</ul>
</section>
<section id="computing-the-expected-output" class="slide level3">
<h3>Computing the Expected Output</h3>
<ul>
<li>Given the posterior for the parameters, how can we compute the expected output at a given location?</li>
<li>Output of model at location <span class="math inline">\(\inputVector_i\)</span> is given by <span class="math display">\[
  \mappingFunction(\inputVector_i; \mappingVector) = \basisVector_i^\top \mappingVector
  \]</span></li>
<li>We want the expected output under the posterior density, <span class="math inline">\(p(\mappingVector|\dataVector, \inputMatrix, \dataStd^2, \alpha)\)</span>.</li>
<li>Mean of mapping function will be given by <span class="math display">\[
  \begin{aligned} \left\langle f(\inputVector_i; \mappingVector)\right\rangle_{p(\mappingVector|\dataVector, \inputMatrix, \dataStd^2, \alpha)} &amp;= \basisVector_i^\top \left\langle\mappingVector\right\rangle_{p(\mappingVector|\dataVector, \inputMatrix, \dataStd^2, \alpha)} \\  &amp; = \basisVector_i^\top \meanVector_w \end{aligned}
  \]</span></li>
</ul>
</section>
<section id="variance-of-expected-output" class="slide level3">
<h3>Variance of Expected Output</h3>
<ul>
<li>Variance of model at location <span class="math inline">\(\inputVector_i\)</span> is given by <span class="math display">\[
  \begin{aligned}\text{var}(\mappingFunction(\inputVector_i; \mappingVector)) &amp;= \left\langle(\mappingFunction(\inputVector_i; \mappingVector))^2\right\rangle - \left\langle \mappingFunction(\inputVector_i; \mappingVector)\right\rangle^2 \\&amp;= \basisVector_i^\top\left\langle\mappingVector\mappingVector^\top\right\rangle \basisVector_i - \basisVector_i^\top \left\langle\mappingVector\right\rangle\left\langle\mappingVector\right\rangle^\top \basisVector_i \\&amp;= \basisVector_i^\top \covarianceMatrix_i\basisVector_i
  \end{aligned}
  \]</span> where all these expectations are taken under the posterior density, <span class="math inline">\(p(\mappingVector|\dataVector, \inputMatrix, \dataStd^2, \alpha)\)</span>.</li>
</ul>
</section>
<section id="reading-2" class="slide level3">
<h3>Reading</h3>
<ul>
<li><p>Section 3.7–3.8 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span> (pg 122–133).</p></li>
<li><p>Section 3.4 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> (pg 161–165).</p></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bishop:book06">
<p>Bishop, C.M., 2006. Pattern recognition and machine learning. springer.</p>
</div>
<div id="ref-Laplace:memoire74">
<p>Laplace, P.S., 1774. Mémoire sur la probabilité des causes par les évènemens, in: Mémoires de Mathèmatique et de Physique, Presentés à lAcadémie Royale Des Sciences, Par Divers Savans, &amp; Lù Dans Ses Assemblées 6. pp. 621–656.</p>
</div>
<div id="ref-Rogers:book11">
<p>Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
