<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2015-12-15">
  <title>Special Topics: Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: {
         extensions: ["color.js"]
      }
    });
  </script>
  <script src="figure-animate.js"></script>
</head>
<body>
\[\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
\]
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Special Topics: Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2015-12-15</time></p>
  <p class="venue" style="text-align:center">University of Sheffield</p>
</section>

<section class="slide level3">

<!-- Front matter -->
<!---->
<!--Back matter-->
<p>.</p>
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
</section>
<section id="review" class="slide level3">
<h3>Review</h3>
<ul>
<li>Last week: Logistic Regression and Generalised Linear Models</li>
<li>Introduced link functions and different transformations.</li>
<li>Showed examples in classification and mentioned possibilities for disease rate models.</li>
<li>This week:
<ul>
<li>Gaussian Processes: non parametric Bayesian modelling }</li>
</ul></li>
</ul>
</section>
<section id="generalized-linear-models" class="slide level3">
<h3>Generalized Linear Models</h3>
<p>Logistic regression is part of a wider class of models known as <em>generalized linear models</em>. In these models we determine that some characteristic of the model is speicified by a function that is liniear in the parameters. So we might suggest that <span class="math display">\[
\log \frac{p(\inputVector)}{1-p(\inputVector)} = \mappingFunction(\inputVector; \mappingVector)
\]</span> where <span class="math inline">\(\mappingFunction(\inputVector; \mappingVector)\)</span> is a linear-in-the-parameters function (here the parameters are <span class="math inline">\(\mappingVector\)</span>, which is generally non-linear in the inputs. So far we have considered basis function models of the form <span class="math display">\[
\mappingFunction(\inputVector) =
\mappingVector^\top \basisVector(\inputVector).
\]</span></p>
</section>
<section id="gaussian-processes" class="slide level3">
<h3>Gaussian Processes</h3>
</section>
<section id="prediction-of-mappingfunction_2-from-mappingfunction_1" class="slide level3">
<h3>Prediction of <span class="math inline">\(\mappingFunction_{2}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h3>
<script>
showDivs(9, 'two_point_sample2');
</script>
<small></small> <input id="range-two_point_sample2" type="range" min="9" max="12" value="9" onchange="setDivs('two_point_sample2')" oninput="setDivs('two_point_sample2')">
<button onclick="plusDivs(-1, 'two_point_sample2')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample2')">
❯
</button>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample009.svg" style="vertical-align:middle;">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample010.svg" style="vertical-align:middle;">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample011.svg" style="vertical-align:middle;">
</object>
</div>
<div class="two_point_sample2" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample012.svg" style="vertical-align:middle;">
</object>
</div>
{
<div class="figure">
<div id="two-point-sample-one-two-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample012.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="prediction-of-mappingfunction_2-from-mappingfunction_1-1" class="slide level3">
<h3>Prediction of <span class="math inline">\(\mappingFunction_{2}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h3>
<p><small> * The single contour of the Gaussian density represents the <font color="white">joint distribution, <span class="math inline">\(p(\mappingFunction_1, \mappingFunction_2)\)</span></font></p>
<div class="fragment">
<ul>
<li>We observe that <font color="white"><span class="math inline">\(\mappingFunction_1=?\)</span></font></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Conditional density: <font color="white"><span class="math inline">\(p(\mappingFunction_2|\mappingFunction_1=?)\)</span></font> </small></li>
</ul>
</div>
</section>
<section id="prediction-with-correlated-gaussians" class="slide level3">
<h3>Prediction with Correlated Gaussians</h3>
<ul>
<li><p>Prediction of <span class="math inline">\(\mappingFunction_2\)</span> from <span class="math inline">\(\mappingFunction_1\)</span> requires <em>conditional density</em>.</p></li>
<li><p>Conditional density is <em>also</em> Gaussian. <span class="math display">\[
  p(\mappingFunction_2|\mappingFunction_1) = {\mathcal{N}\left(\mappingFunction_2|\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1,\kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}\right)}
  \]</span> where covariance of joint density is given by <span class="math display">\[
  \kernelMatrix= \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}\end{bmatrix}
  \]</span></p></li>
</ul>
</section>
<section id="prediction-of-mappingfunction_8-from-mappingfunction_1" class="slide level3">
<h3>Prediction of <span class="math inline">\(\mappingFunction_{8}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h3>
<script>
showDivs(13, 'two_point_sample3');
</script>
<small></small> <input id="range-two_point_sample3" type="range" min="13" max="17" value="13" onchange="setDivs('two_point_sample3')" oninput="setDivs('two_point_sample3')">
<button onclick="plusDivs(-1, 'two_point_sample3')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample3')">
❯
</button>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample013.svg" style="vertical-align:middle;">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample014.svg" style="vertical-align:middle;">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample015.svg" style="vertical-align:middle;">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample016.svg" style="vertical-align:middle;">
</object>
</div>
<div class="two_point_sample3" style="text-align:center;">
<object class="svgplot " data="../slides/diagrams/gp/two_point_sample017.svg" style="vertical-align:middle;">
</object>
</div>
</section>
<section id="details" class="slide level3">
<h3>Details</h3>
<ul>
<li>The single contour of the Gaussian density represents the <font color="white">joint distribution, <span class="math inline">\(p(\mappingFunction_1, \mappingFunction_8)\)</span></font></li>
</ul>
<div class="fragment">
<ul>
<li>We observe a value for <font color="white"><span class="math inline">\(\mappingFunction_1=-?\)</span></font></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Conditional density: <font color="white"><span class="math inline">\(p(\mappingFunction_5|\mappingFunction_1=?)\)</span></font>.</li>
</ul>
</div>
</section>
<section id="prediction-with-correlated-gaussians-1" class="slide level3">
<h3>Prediction with Correlated Gaussians</h3>
<ul>
<li><p>Prediction of <span class="math inline">\(\mappingFunctionVector_*\)</span> from <span class="math inline">\(\mappingFunctionVector\)</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <span class="math display">\[
  p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|\kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector,\kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}\right)}
  \]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span class="math display">\[
  \kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
  \]</span></p></li>
</ul>
</section>
<section id="prediction-with-correlated-gaussians-2" class="slide level3">
<h3>Prediction with Correlated Gaussians</h3>
<ul>
<li><p>Prediction of <span class="math inline">\(\mappingFunctionVector_*\)</span> from <span class="math inline">\(\mappingFunctionVector\)</span> requires multivariate <em>conditional density</em>.</p></li>
<li><p>Multivariate conditional density is <em>also</em> Gaussian. <large> <span class="math display">\[
  p(\mappingFunctionVector_*|\mappingFunctionVector) = {\mathcal{N}\left(\mappingFunctionVector_*|{\boldsymbol{{\mu}}},\boldsymbol{\Sigma}\right)}
  \]</span> <span class="math display">\[
  {\boldsymbol{{\mu}}}= \kernelMatrix_{*,\mappingFunctionVector}\kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\mappingFunctionVector
  \]</span> <span class="math display">\[\boldsymbol{\Sigma} = \kernelMatrix_{*,*}-\kernelMatrix_{*,\mappingFunctionVector} \kernelMatrix_{\mappingFunctionVector,\mappingFunctionVector}^{-1}\kernelMatrix_{\mappingFunctionVector,*}
  \]</span> </large></p></li>
<li><p>Here covariance of joint density is given by <span class="math display">\[
  \kernelMatrix= \begin{bmatrix} \kernelMatrix_{\mappingFunctionVector, \mappingFunctionVector} &amp; \kernelMatrix_{*, \mappingFunctionVector}\\ \kernelMatrix_{\mappingFunctionVector, *} &amp; \kernelMatrix_{*, *}\end{bmatrix}
  \]</span></p></li>
</ul>
</section>
<section id="marginal-likelihood" class="slide level3">
<h3>Marginal Likelihood</h3>
</section>
<section id="sampling-from-the-prior" class="slide level3">
<h3>Sampling from the Prior</h3>
</section>
<section id="weight-space-view" class="slide level3">
<h3>Weight Space View</h3>
</section>
<section id="function-space-view" class="slide level3">
<h3>Function Space View</h3>
<p>Now we can use the <code>np.random.multivariate_normal</code> command for sampling from a multivariate normal with covariance given by <span class="math inline">\(\kernelMatrix\)</span> and zero mean,</p>
<p>where the effect of our noise term is to roughen the sampled functions, we can also increase the variance of the noise to see a different effect,</p>

</section>
<section id="non-degenerate-gaussian-processes" class="slide level3">
<h3>Non-degenerate Gaussian Processes</h3>
<ul>
<li>This process is <em>degenerate</em>.</li>
<li>Covariance function is of rank at most <span class="math inline">\(\numHidden\)</span>.</li>
<li>As <span class="math inline">\(\numData \rightarrow \infty\)</span>, covariance matrix is not full rank.</li>
<li>Leading to <span class="math inline">\(\det{\kernelMatrix} = 0\)</span></li>
</ul>
</section>
<section id="infinite-networks" class="slide level3">
<h3>Infinite Networks</h3>
<ul>
<li>In ML Radford Neal <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span> asked “what would happen if you took <span class="math inline">\(\numHidden \rightarrow \infty\)</span>?”</li>
</ul>
<div class="figure">
<div id="neal-infinite-priors-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/neal-infinite-priors.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="roughly-speaking" class="slide level3">
<h3>Roughly Speaking</h3>
<ul>
<li>Instead of</li>
</ul>
<p><span class="math display">\[
  \begin{align*}
  \kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) &amp; = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)\\
  &amp; = \alpha \sum_k \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}_k, \inputVector_j\right)
  \end{align*}
  \]</span></p>
<ul>
<li>Sample infinitely many from a prior density, <span class="math inline">\(p(\mappingVector^{(1)})\)</span>,</li>
</ul>
<p><span class="math display">\[
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \int \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right) \activationScalar\left(\mappingVector^{(1)}, \inputVector_j\right) p(\mappingVector^{(1)}) \text{d}\mappingVector^{(1)}
\]</span></p>
<ul>
<li>Also applies for non-Gaussian <span class="math inline">\(p(\mappingVector^{(1)})\)</span> because of the <em>central limit theorem</em>.</li>
</ul>
</section>
<section id="simple-probabilistic-program" class="slide level3">
<h3>Simple Probabilistic Program</h3>
<ul>
<li><p>If <span class="math display">\[
  \begin{align*} 
  \mappingVector^{(1)} &amp; \sim p(\cdot)\\ \phi_i &amp; = \activationScalar\left(\mappingVector^{(1)}, \inputVector_i\right), 
  \end{align*}
  \]</span> has finite variance.</p></li>
<li><p>Then taking number of hidden units to infinity, is also a Gaussian process.</p></li>
</ul>
</section>
<section id="further-reading" class="slide level3">
<h3>Further Reading</h3>
<ul>
<li><p>Chapter 2 of Neal’s thesis <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>Rest of Neal’s thesis. <span class="citation" data-cites="Neal:bayesian94">(Neal, 1994)</span></p></li>
<li><p>David MacKay’s PhD thesis <span class="citation" data-cites="MacKay:bayesian92">(MacKay, 1992)</span></p></li>
</ul>
</section>
<section id="gaussian-process" class="slide level3">
<h3>Gaussian Process</h3>
<p>Now we can image the resulting covariance,</p>
<p>To visualise the covariance between the points we can use the <code>imshow</code> function in matplotlib.</p>
<p>Finally, we can sample functions from the marginal likelihood.</p>

</section>
<section id="section" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample001.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample002.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
{
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="gaussian-process-1" class="slide level3">
<h3>Gaussian Process</h3>
<p>The Gaussian process perspective takes the marginal likelihood of the data to be a joint Gaussian density with a covariance given by <span class="math inline">\(\kernelMatrix\)</span>. So the model likelihood is of the form, <span class="math display">\[
p(\dataVector|\inputMatrix) =
\frac{1}{(2\pi)^{\frac{\numData}{2}}|\kernelMatrix|^{\frac{1}{2}}}
\exp\left(-\frac{1}{2}\dataVector^\top \left(\kernelMatrix+\dataStd^2
\eye\right)^{-1}\dataVector\right)
\]</span> where the input data, <span class="math inline">\(\inputMatrix\)</span>, influences the density through the covariance matrix, <span class="math inline">\(\kernelMatrix\)</span> whose elements are computed through the covariance function, <span class="math inline">\(\kernelScalar(\inputVector, \inputVector^\prime)\)</span>.</p>
<p>This means that the negative log likelihood (the objective function) is given by, <span class="math display">\[
E(\boldsymbol{\theta}) = \frac{1}{2} \log |\kernelMatrix|
+ \frac{1}{2} \dataVector^\top \left(\kernelMatrix +
\dataStd^2\eye\right)^{-1}\dataVector
\]</span> where the <em>parameters</em> of the model are also embedded in the covariance function, they include the parameters of the kernel (such as lengthscale and variance), and the noise variance, <span class="math inline">\(\dataStd^2\)</span>. Let’s create a class in python for storing these variables.</p>
</section>
<section id="making-predictions" class="slide level3">
<h3>Making Predictions</h3>
<p>We now have a probability density that represents functions. How do we make predictions with this density? The density is known as a process because it is <em>consistent</em>. By consistency, here, we mean that the model makes predictions for <span class="math inline">\(\mappingFunctionVector\)</span> that are unaffected by future values of <span class="math inline">\(\mappingFunctionVector^*\)</span> that are currently unobserved (such as test points). If we think of <span class="math inline">\(\mappingFunctionVector^*\)</span> as test points, we can still write down a joint probability density over the training observations, <span class="math inline">\(\mappingFunctionVector\)</span> and the test observations, <span class="math inline">\(\mappingFunctionVector^*\)</span>. This joint probability density will be Gaussian, with a covariance matrix given by our covariance function, <span class="math inline">\(\kernelScalar(\inputVector_i, \inputVector_j)\)</span>. <span class="math display">\[
\begin{bmatrix}\mappingFunctionVector \\ \mappingFunctionVector^*\end{bmatrix} \sim \gaussianSamp{\zerosVector}{\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\
\kernelMatrix_\ast^\top &amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}}
\]</span> where here <span class="math inline">\(\kernelMatrix\)</span> is the covariance computed between all the training points, <span class="math inline">\(\kernelMatrix_\ast\)</span> is the covariance matrix computed between the training points and the test points and <span class="math inline">\(\kernelMatrix_{\ast,\ast}\)</span> is the covariance matrix computed betwen all the tests points and themselves. To be clear, let’s compute these now for our example, using <code>x</code> and <code>y</code> for the training data (although <code>y</code> doesn’t enter the covariance) and <code>x_pred</code> as the test locations.</p>
<p>Now we use this structure to visualise the covariance between test data and training data. This structure is how information is passed between trest and training data. Unlike the maximum likelihood formalisms we’ve been considering so far, the structure expresses <em>correlation</em> between our different data points. However, just like the <a href="./week9.ipynb">naive Bayes approach</a> we now have a <em>joint density</em> between some variables of interest. In particular we have the joint density over <span class="math inline">\(p(\mappingFunctionVector, \mappingFunctionVector^*)\)</span>. The joint density is <em>Gaussian</em> and <em>zero mean</em>. It is specified entirely by the <em>covariance matrix</em>, <span class="math inline">\(\kernelMatrix\)</span>. That covariance matrix is, in turn, defined by a covariance function. Now we will visualise the form of that covariance in the form of the matrix, <span class="math display">\[
\begin{bmatrix} \kernelMatrix &amp; \kernelMatrix_\ast \\ \kernelMatrix_\ast^\top
&amp; \kernelMatrix_{\ast,\ast}\end{bmatrix}
\]</span></p>
<p>There are four blocks to this color plot. The upper left block is the covariance of the training data with itself, <span class="math inline">\(\kernelMatrix\)</span>. We see some structure here due to the missing data from the first and second world wars. Alongside this covariance (to the right and below) we see the cross covariance between the training and the test data (<span class="math inline">\(\kernelMatrix_*\)</span> and <span class="math inline">\(\kernelMatrix_*^\top\)</span>). This is giving us the covariation between our training and our test data. Finally the lower right block The banded structure we now observe is because some of the training points are near to some of the test points. This is how we obtain ‘communication’ between our training data and our test data. If there is no structure in <span class="math inline">\(\kernelMatrix_*\)</span> then our belief about the test data simply matches our prior.</p>
</section>
<section id="conditional-density" class="slide level3">
<h3>Conditional Density</h3>
<p>Just as in naive Bayes, we first defined the joint density (although there it was over both the labels and the inputs, <span class="math inline">\(p(\dataVector, \inputMatrix)\)</span> and now we need to define <em>conditional</em> distributions that answer particular questions of interest. In particular we might be interested in finding out the values of the function for the prediction function at the test data given those at the training data, <span class="math inline">\(p(\mappingFunctionVector_*|\mappingFunctionVector)\)</span>. Or if we include noise in the training observations then we are interested in the conditional density for the prediction function at the test locations given the training observations, <span class="math inline">\(p(\mappingFunctionVector^*|\dataVector)\)</span>.</p>
<p>As ever all the various questions we could ask about this density can be answered using the <em>sum rule</em> and the <em>product rule</em>. For the multivariate normal density the mathematics involved is that of <em>linear algebra</em>, with a particular emphasis on the <em>partitioned inverse</em> or <a href="http://en.wikipedia.org/wiki/Invertible_matrix#Blockwise_inversion"><em>block matrix inverse</em></a>, but they are beyond the scope of this course, so you don’t need to worry about remembering them or rederiving them. We are simply writing them here because it is this <em>conditional</em> density that is necessary for making predictions.</p>
<p>The conditional density is also a multivariate normal, <span class="math display">\[
\mappingFunctionVector^* | \mappingFunctionVector \sim \gaussianSamp{\meanVector_\mappingFunction}{\mathbf{C}_\mappingFunction}
\]</span> with a mean given by <span class="math display">\[
\meanVector_\mappingFunction = \kernelMatrix_*^\top \left[\kernelMatrix + \dataStd^2
\eye\right]^{-1} \dataVector
\]</span> and a covariance given by <span class="math display">\[
\mathbf{C}_\mappingFunction
= \kernelMatrix_{*,*} - \kernelMatrix_*^\top \left[\kernelMatrix + \dataStd^2
\eye\right]^{-1} \kernelMatrix_\ast.
\]</span> Let’s compute what those posterior predictions are for the olympic marathon data.</p>
<p>where for convenience we’ve defined</p>
<p><span class="math display">\[\mathbf{A} = \left[\kernelMatrix +
\dataStd^2\eye\right]^{-1}\kernelMatrix_*.\]</span></p>
<p>We can visualize the covariance of the <em>conditional</em>,</p>
<p>and we can plot the mean of the conditional</p>
<p>as well as the associated error bars. These are given (similarly to the Bayesian parametric model from the last lab) by the standard deviations of the marginal posterior densities. The marginal posterior variances are given by the diagonal elements of the posterior covariance,</p>
<p>They can be added to the underlying mean function to give the error bars,</p>
<p>This gives us a prediction from the Gaussian process. Remember machine learning is <span class="math display">\[
\text{data} + \text{model} \rightarrow \text{prediction}.
\]</span> Here our data is from the olympics, and our model is a Gaussian process with two parameters. The assumptions about the world are encoded entirely into our Gaussian process covariance. The GP covariance assumes that the function is highly smooth, and that correlation falls off with distance (scaled according to the length scale, <span class="math inline">\(\ell\)</span>). The model sustains the uncertainty about the function, this means we see an increase in the size of the error bars during periods like the 1st and 2nd World Wars when no olympic marathon was held.</p>

</section>
<section id="the-importance-of-the-covariance-function" class="slide level3">
<h3>The Importance of the Covariance Function</h3>
<p>The covariance function encapsulates our assumptions about the data. The equations for the distribution of the prediction function, given the training observations, are highly sensitive to the covariation between the test locations and the training locations as expressed by the matrix <span class="math inline">\(\kernelMatrix_*\)</span>. We defined a matrix <span class="math inline">\(\mathbf{A}\)</span> which allowed us to express our conditional mean in the form, <span class="math display">\[
\meanVector_\mappingFunction = \mathbf{A}^\top \dataVector,
\]</span> where <span class="math inline">\(\dataVector\)</span> were our <em>training observations</em>. In other words our mean predictions are always a linear weighted combination of our <em>training data</em>. The weights are given by computing the covariation between the training and the test data (<span class="math inline">\(\kernelMatrix_*\)</span>) and scaling it by the inverse covariance of the training data observations, <span class="math inline">\(\left[\kernelMatrix + \dataStd^2 \eye\right]^{-1}\)</span>. This inverse is the main computational object that needs to be resolved for a Gaussian process. It has a computational burden which is <span class="math inline">\(O(\numData^3)\)</span> and a storage burden which is <span class="math inline">\(O(\numData^2)\)</span>. This makes working with Gaussian processes computationally intensive for the situation where <span class="math inline">\(\numData&gt;10,000\)</span>.</p>
<iframe width height src="https://www.youtube.com/embed/ewJ3AxKclOg?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</section>
<section id="improving-the-numerics" class="slide level3">
<h3>Improving the Numerics</h3>
<p>In practice we shouldn’t be using matrix inverse directly to solve the GP system. One more stable way is to compute the <em>Cholesky decomposition</em> of the kernel matrix. The log determinant of the covariance can also be derived from the Cholesky decomposition.</p>
</section>
<section id="capacity-control" class="slide level3">
<h3>Capacity Control</h3>
</section>
<section id="gradients-of-the-likelihood" class="slide level3">
<h3>Gradients of the Likelihood</h3>
</section>
<section id="overall-process-scale" class="slide level3">
<h3>Overall Process Scale</h3>
<p>In general we won’t be able to find parameters of the covariance function through fixed point equations, we will need to do gradient based optimization.</p>
</section>
<section id="capacity-control-and-data-fit" class="slide level3">
<h3>Capacity Control and Data Fit</h3>
<p>The objective function can be decomposed into two terms, a capacity control term, and a data fit term. The capacity control term is the log determinant of the covariance. The data fit term is the matrix inner product between the data and the inverse covariance.</p>
</section>
<section id="learning-covariance-parameters" class="slide level3">
<h3>Learning Covariance Parameters</h3>
<p>Can we determine covariance parameters from the data?</p>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<p><span class="math display">\[\gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\det{\kernelMatrix}^{\frac{1}{2}}}}{\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}\]</span></p>
</section>
<section id="section-6" class="slide level3">
<h3></h3>
<p><span class="math display">\[\begin{aligned}
    \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=\frac{1}{(2\pi)^\frac{\numData}{2}{\color{white} \det{\kernelMatrix}^{\frac{1}{2}}}}{\color{white}\exp\left(-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}\right)}
\end{aligned}
\]</span></p>
</section>
<section id="section-7" class="slide level3">
<h3></h3>
<p><span class="math display">\[
\begin{aligned}
    \log \gaussianDist{\dataVector}{\mathbf{0}}{\kernelMatrix}=&amp;{\color{white}-\frac{1}{2}\log\det{\kernelMatrix}}{\color{white}-\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}} \\ &amp;-\frac{\numData}{2}\log2\pi
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\errorFunction(\parameterVector) = {\color{white} \frac{1}{2}\log\det{\kernelMatrix}} + {\color{white} \frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}
\]</span></p>
</section>
<section id="section-8" class="slide level3">
<h3></h3>
<p>The parameters are <em>inside</em> the covariance function (matrix). <span class="math display">\[\kernelScalar_{i, j} = \kernelScalar(\inputVals_i, \inputVals_j; \parameterVector)\]</span></p>
</section>
<section id="eigendecomposition-of-covariance" class="slide level3">
<h3>Eigendecomposition of Covariance</h3>
<p><span><span class="math display">\[\kernelMatrix = \rotationMatrix \eigenvalueMatrix^2 \rotationMatrix^\top\]</span></span></p>
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/gp/gp-optimize-eigen.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<span class="math inline">\(\eigenvalueMatrix\)</span> represents distance on axes. <span class="math inline">\(\rotationMatrix\)</span> gives rotation.
</td>
</tr>
</table>
</section>
<section id="eigendecomposition-of-covariance-1" class="slide level3">
<h3>Eigendecomposition of Covariance</h3>
<ul>
<li><span class="math inline">\(\eigenvalueMatrix\)</span> is <em>diagonal</em>, <span class="math inline">\(\rotationMatrix^\top\rotationMatrix = \eye\)</span>.</li>
<li>Useful representation since <span class="math inline">\(\det{\kernelMatrix} = \det{\eigenvalueMatrix^2} = \det{\eigenvalueMatrix}^2\)</span>.</li>
</ul>
</section>
<section id="capacity-control-colorwhite-log-detkernelmatrix" class="slide level3">
<h3>Capacity control: <span class="math inline">\({\color{white} \log \det{\kernelMatrix}}\)</span></h3>
<!--
\only<1>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant1}}\only<2>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant2}}\only<3>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant3}}\only<4>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant4}}\only<5>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant5}}\only<6>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant6}}\only<7>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant7}}\only<8>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant8}}\only<9>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant9}}\only<10>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseDeterminant10}}-->
</section>
<section id="data-fit-colorwhite-fracdatavectortopkernelmatrix-1datavector2" class="slide level3">
<h3>Data Fit: <span class="math inline">\({\color{white} \frac{\dataVector^\top\kernelMatrix^{-1}\dataVector}{2}}\)</span></h3>
<!--

\only<1>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic1}}\only<2>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic2}}\only<3>{\inputdiagram{../../../gp/tex/diagrams/gpOptimiseQuadratic3}}-->
</section>
<section id="errorfunctionparametervector-colorwhitefrac12logdetkernelmatrixcolorwhitefracdatavectortopkernelmatrix-1datavector2" class="slide level3">
<h3><span class="math display">\[\errorFunction(\parameterVector) = {\color{white}\frac{1}{2}\log\det{\kernelMatrix}}+{\color{white}\frac{\dataVector^{\top}\kernelMatrix^{-1}\dataVector}{2}}\]</span></h3>
<script>
showDivs(0, 'gp-optimise');
</script>
<small></small> <input id="range-gp-optimise" type="range" min="0" max="10" value="0" onchange="setDivs('gp-optimise')" oninput="setDivs('gp-optimise')">
<button onclick="plusDivs(-1, 'gp-optimise')">
❮
</button>
<button onclick="plusDivs(1, 'gp-optimise')">
❯
</button>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise000.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise001.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise002.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise003.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise004.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise005.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise006.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise007.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise008.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise009.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise010.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise011.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise012.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise013.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise014.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise015.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise016.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise017.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise018.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise019.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
<div class="gp-optimise" style="text-align:center;">
<table>
<tr>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise020.svg" style="vertical-align:middle;">
</object>
</td>
<td width="50%">
<object class="svgplot " data="../slides/diagrams/gp/gp-optimise021.svg" style="vertical-align:middle;">
</object>
</td>
</tr>
</table>
</div>
</section>
<section id="exponentiated-quadratic-covariance" class="slide level3">
<h3>Exponentiated Quadratic Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector-\inputVector^\prime}^2}{2\lengthScale^2}\right)\]</span>
</center>
<div class="figure">
<div id="eq-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class="100%" align data="../slides/diagrams/kern/eq_covariance.svg" style="vertical-align:middle">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/eq_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
</section>
<section id="olympic-marathon-data" class="slide level3">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td width="70%">
<ul>
<li>Gold medal times for Olympic Marathon since 1896.</li>
<li>Marathons before 1924 didn’t have a standardised distance.</li>
<li>Present results using pace per km.</li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/Stephen_Kiprotich.jpg" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3">
<h3>Olympic Marathon Data</h3>
<div class="figure">
<div id="olympic-marathon-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/olympic-marathon.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="alan-turing" class="slide level3">
<h3>Alan Turing</h3>
<div class="figure">
<div id="turing-run-times-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/turing-times.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/turing-run.jpg" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
</section>
<section id="probability-winning-olympics" class="slide level3">
<h3>Probability Winning Olympics?</h3>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
</section>
<section id="olympic-marathon-data-gp" class="slide level3">
<h3>Olympic Marathon Data GP</h3>
<div class="figure">
<div id="olympic-marathon-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/olympic-marathon-gp.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="della-gatta-gene-data" class="slide level3">
<h3>Della Gatta Gene Data</h3>
<ul>
<li>Given given expression levels in the form of a time series from <span class="citation" data-cites="DellaGatta:direct08">Della Gatta et al. (2008)</span>.</li>
</ul>
</section>
<section id="della-gatta-gene-data-1" class="slide level3">
<h3>Della Gatta Gene Data</h3>
<div class="figure">
<div id="della-gatta-gene-data-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/datasets/della-gatta-gene.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="gene-expression-example" class="slide level3">
<h3>Gene Expression Example</h3>
<ul>
<li>Want to detect if a gene is expressed or not, fit a GP to each gene <span class="citation" data-cites="Kalaitzis:simple11">Kalaitzis and Lawrence (2011)</span>.</li>
</ul>
</section>
<section id="section-9" class="slide level3">
<h3></h3>
<div class="figure">
<div id="a-simple-approach-to-ranking-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/1471-2105-12-180_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<center>
<a href="http://www.biomedcentral.com/1471-2105/12/180" class="uri">http://www.biomedcentral.com/1471-2105/12/180</a>
</center>
</section>
<section id="section-10" class="slide level3">
<h3></h3>
</section>
<section id="tp53-gene-data-gp" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<div class="figure">
<div id="della-gatta-gene-gp-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="tp53-gene-data-gp-1" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<div class="figure">
<div id="della-gatta-gene-gp2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp2.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="tp53-gene-data-gp-2" class="slide level3">
<h3>TP53 Gene Data GP</h3>
<div class="figure">
<div id="della-gatta-gene-gp3-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/della-gatta-gene-gp3.svg" style="vertical-align:middle;">
</object>
</div>
</div>
</section>
<section id="multiple-optima" class="slide level3">
<h3>Multiple Optima</h3>
<div class="figure">
<div id="gp-multiple-optima000-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/multiple-optima000.svg" style="vertical-align:middle;">
</object>
</div>
</div>
<!--
### Multiple Optima 



<object class="svgplot "  data="../slides/diagrams/gp/multiple-optima001.svg"  style="vertical-align:middle;"></object>-->
</section>
<section id="example-prediction-of-malaria-incidence-in-uganda" class="slide level3">
<h3>Example: Prediction of Malaria Incidence in Uganda</h3>
<p><span style="text-align:right"><img class="" src="../slides/diagrams/people/2013_03_28_180606.JPG" width="1.5cm" align="" style="background:none; border:none; box-shadow:none; position:absolute; clip:rect(2662px,1780px,1110px,600px);vertical-align:middle"></span></p>
<ul>
<li>Work with Ricardo Andrade Pacheco, John Quinn and Martin Mubaganzi (Makerere University, Uganda)</li>
<li>See <a href="http://air.ug/research.html">AI-DEV Group</a>.</li>
</ul>
</section>
<section id="malaria-prediction-in-uganda" class="slide level3">
<h3>Malaria Prediction in Uganda</h3>
<div class="figure">
<div id="uganda-districts-2006-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/uganda-districts-2006.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<p><span style="text-align:right"><span class="citation" data-cites="Andrade:consistent14 Mubangizi:malaria14">(Andrade-Pacheco et al., 2014; Mubangizi et al., 2014)</span></span></p>
</section>
<section id="kapchorwa-district" class="slide level3">
<h3>Kapchorwa District</h3>
<div class="figure">
<div id="kapchorwa-district-in-uganda-figure" class="figure-frame">
<object class="50%" align data="../slides/diagrams/health/Kapchorwa_District_in_Uganda.svg" style="vertical-align:middle">
</object>
</div>
</div>
</section>
<section id="tororo-district" class="slide level3">
<h3>Tororo District</h3>
<div class="figure">
<div id="tororo-district-in-uganda-figure" class="figure-frame">
<object class="50%" align data="../slides/diagrams/health/Tororo_District_in_Uganda.svg" style="vertical-align:middle">
</object>
</div>
</div>
</section>
<section id="malaria-prediction-in-nagongera-sentinel-site" class="slide level3">
<h3>Malaria Prediction in Nagongera (Sentinel Site)</h3>
<div class="figure">
<div id="sentinel-nagongera-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/health/sentinel_nagongera.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="mubende-district" class="slide level3">
<h3>Mubende District</h3>
<div class="figure">
<div id="mubende-district-in-uganda-figure" class="figure-frame">
<object class="50%" align data="../slides/diagrams/health/Mubende_District_in_Uganda.svg" style="vertical-align:middle">
</object>
</div>
</div>
</section>
<section id="malaria-prediction-in-uganda-1" class="slide level3">
<h3>Malaria Prediction in Uganda</h3>
<div class="figure">
<div id="malaria-prediction-mubende-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/mubende.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="gp-school-at-makerere" class="slide level3">
<h3>GP School at Makerere</h3>
<div class="figure">
<div id="-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/gpss/1157497_513423392066576_1845599035_n.jpg" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="early-warning-systems" class="slide level3">
<h3>Early Warning Systems</h3>
</section>
<section id="kabarole-district" class="slide level3">
<h3>Kabarole District</h3>
<div class="figure">
<div id="kabarole-district-in-uganda-figure" class="figure-frame">
<object class="50%" align data="../slides/diagrams/health/Kabarole_District_in_Uganda.svg" style="vertical-align:middle">
</object>
</div>
</div>
<div class="figure">
<div id="kabarole-disease-over-time-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/kabarole.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="early-warning-systems-1" class="slide level3">
<h3>Early Warning Systems</h3>
<div class="figure">
<div id="Kabarole early warning system over time.-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/health/monitor.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="additive-covariance" class="slide level3">
<h3>Additive Covariance</h3>
<center>
<span class="math display">\[\kernelScalar_f(\inputVector, \inputVector^\prime) = \kernelScalar_g(\inputVector, \inputVector^\prime) + \kernelScalar_h(\inputVector, \inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="add-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class="100%" align data="../slides/diagrams/kern/add_covariance.svg" style="vertical-align:middle">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/add_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
</section>
<section id="section-11" class="slide level3">
<h3></h3>
<div class="figure">
<div id="bialik-friday-the-13th-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bialik-fridaythe13th-1.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
</section>
<section id="gelman-book" class="slide level3">
<h3>Gelman Book</h3>
<div class="figure">
<div id="bayesian-data-analysis-figure" class="figure-frame">
<table>
<tr>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover_1.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
<td width="50%">
<div class="centered" style="">
<img class="" src="../slides/diagrams/ml/bda_cover.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
<p><span style="text-align:right"><span class="citation" data-cites="Gelman:bayesian13">Gelman et al. (2013)</span></span></p>
</section>
<section id="basis-function-covariance" class="slide level3">
<h3>Basis Function Covariance</h3>
<center>
<span class="math display">\[\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)\]</span>
</center>
<div class="figure">
<div id="basis-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class="100%" align data="../slides/diagrams/kern/basis_covariance.svg" style="vertical-align:middle">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
</section>
<section id="brownian-covariance" class="slide level3">
<h3>Brownian Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(t, t^\prime)=\alpha \min(t, t^\prime)\]</span>
</center>
<div class="figure">
<div id="brownian-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class="100%" align data="../slides/diagrams/kern/brownian_covariance.svg" style="vertical-align:middle">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
</section>
<section id="mlp-covariance" class="slide level3">
<h3>MLP Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)\]</span>
</center>
<div class="figure">
<div id="mlp-covariance-plot-figure" class="figure-frame">
<table>
<tr>
<td width="45%">
<object class="100%" align data="../slides/diagrams/kern/mlp_covariance.svg" style="vertical-align:middle">
</object>
</td>
<td width="45%">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</td>
</tr>
</table>
</div>
</div>
</section>
<section id="gpss-gaussian-process-summer-school" class="slide level3">
<h3>GPSS: Gaussian Process Summer School</h3>
<table>
<tr>
<td width="60%">
<ul>
<li><a href="http://gpss.cc" class="uri">http://gpss.cc</a></li>
<li>Next one is in Sheffield in <em>September 2019</em>.</li>
<li>Many lectures from past meetings available online</li>
</ul>
</td>
<td width="40%">
<div style="width:1.5cm;text-align:center">

</div>
</td>
</tr>
</table>
</section>
<section id="gpy-a-gaussian-process-framework-in-python" class="slide level3">
<h3>GPy: A Gaussian Process Framework in Python</h3>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
</div>
<center>
<a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a>
</center>
</section>
<section id="gpy-a-gaussian-process-framework-in-python-1" class="slide level3">
<h3>GPy: A Gaussian Process Framework in Python</h3>
<ul>
<li>BSD Licensed software base.</li>
<li>Wide availability of libraries, ‘modern’ scripting language.</li>
<li>Allows us to set projects to undergraduates in Comp Sci that use GPs.</li>
<li>Available through GitHub <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></li>
<li>Reproducible Research with Jupyter Notebook.</li>
</ul>
</section>
<section id="features" class="slide level3">
<h3>Features</h3>
<ul>
<li>Probabilistic-style programming (specify the model, not the algorithm).</li>
<li>Non-Gaussian likelihoods.</li>
<li>Multivariate outputs.</li>
<li>Dimensionality reduction.</li>
<li>Approximations for large data sets.</li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Andrade:consistent14">
<p>Andrade-Pacheco, R., Mubangizi, M., Quinn, J., Lawrence, N.D., 2014. Consistent mapping of government malaria records across a changing territory delimitation. Malaria Journal 13. <a href="https://doi.org/10.1186/1475-2875-13-S1-P5" class="uri">https://doi.org/10.1186/1475-2875-13-S1-P5</a></p>
</div>
<div id="ref-DellaGatta:direct08">
<p>Della Gatta, G., Bansal, M., Ambesi-Impiombato, A., Antonini, D., Missero, C., Bernardo, D. di, 2008. Direct targets of the trp63 transcription factor revealed by a combination of gene expression profiling and reverse engineering. Genome Research 18, 939–948. <a href="https://doi.org/10.1101/gr.073601.107" class="uri">https://doi.org/10.1101/gr.073601.107</a></p>
</div>
<div id="ref-Gelman:bayesian13">
<p>Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B., 2013. Bayesian data analysis, 3rd ed. Chapman; Hall.</p>
</div>
<div id="ref-Kalaitzis:simple11">
<p>Kalaitzis, A.A., Lawrence, N.D., 2011. A simple approach to ranking differentially expressed gene expression time courses through Gaussian process regression. BMC Bioinformatics 12. <a href="https://doi.org/10.1186/1471-2105-12-180" class="uri">https://doi.org/10.1186/1471-2105-12-180</a></p>
</div>
<div id="ref-MacKay:bayesian92">
<p>MacKay, D.J.C., 1992. Bayesian methods for adaptive models (PhD thesis). California Institute of Technology.</p>
</div>
<div id="ref-Mubangizi:malaria14">
<p>Mubangizi, M., Andrade-Pacheco, R., Smith, M.T., Quinn, J., Lawrence, N.D., 2014. Malaria surveillance with multiple data sources using Gaussian process models, in: 1st International Conference on the Use of Mobile Ict in Africa.</p>
</div>
<div id="ref-Neal:bayesian94">
<p>Neal, R.M., 1994. Bayesian learning for neural networks (PhD thesis). Dept. of Computer Science, University of Toronto.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
